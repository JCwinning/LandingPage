{
  "hash": "b73596c10991300ed2d8783df2fcb5e7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Retrieval-Augmented Generation(RAG) in R & Python\"\nauthor: \"Tony D\"\ndate: \"2025-11-02\"\ncategories: [AI, API, tutorial]\nimage: \"images.png\"\n\nformat:\n  html:\n    code-fold: true\n    code-tools: true\n    code-copy: true\n\nexecute:\n\n  warning: false\n---\n\n\n\n\n\n\n# Introduction\n\nRetrieval-Augmented Generation (RAG) is a powerful technique that combines the generative capabilities of Large Language Models (LLMs) with the precision of information retrieval. By grounding LLM responses in external, verifiable data, RAG reduces hallucinations and enables the model to answer questions about specific, private, or up-to-date information.\n\nIn this tutorial, we will build a RAG system using both R and Python. \n\nIn R, we'll leverage the `ragnar` package for RAG workflows and `ellmer` for chat interfaces. \n\nIn Python, we'll use `LangChain` for the RAG pipeline, `ChromaDB` for the vector store, and `OpenAI` for model interaction.\n\nOur goal is to create a system that can answer questions about the OpenRouter API by scraping its documentation.\n\n# Data Collection\n\n::: {.panel-tabset}\n\n\n## R\n\nFirst, we need to gather the data for our knowledge base. We'll use the `rvest` package to scrape URLs from the OpenRouter documentation. This will give us a list of pages to ingest.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ragnar)\nlibrary(ellmer)\nlibrary(dotenv)\nload_dot_env(file = \".env\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rvest)\n\n# URL to scrape\nurl <- \"https://openrouter.ai/docs/quickstart\"\n\n# Read the HTML content of the page\npage <- read_html(url)\n\n# Extract all <a> tags with href\nlinks <- page %>%\n    html_nodes(\"a\") %>%\n    html_attr(\"href\")\n\n# Remove NAs and duplicates\nlinks <- unique(na.omit(links))\n\n# Optional: keep only full URLs\nlinks_full <- paste0(\"https://openrouter.ai\", links[grepl(\"^/docs/\", links)])\n\n# Print all links\nprint(links_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"https://openrouter.ai/docs/api-reference/overview\"                               \n [2] \"https://openrouter.ai/docs/quickstart\"                                           \n [3] \"https://openrouter.ai/docs/api/reference/overview\"                               \n [4] \"https://openrouter.ai/docs/sdks/call-model/overview\"                             \n [5] \"https://openrouter.ai/docs/guides/overview/principles\"                           \n [6] \"https://openrouter.ai/docs/guides/overview/models\"                               \n [7] \"https://openrouter.ai/docs/faq\"                                                  \n [8] \"https://openrouter.ai/docs/guides/overview/report-feedback\"                      \n [9] \"https://openrouter.ai/docs/guides/routing/model-fallbacks\"                       \n[10] \"https://openrouter.ai/docs/guides/routing/provider-selection\"                    \n[11] \"https://openrouter.ai/docs/guides/features/presets\"                              \n[12] \"https://openrouter.ai/docs/guides/features/tool-calling\"                         \n[13] \"https://openrouter.ai/docs/guides/features/structured-outputs\"                   \n[14] \"https://openrouter.ai/docs/guides/features/message-transforms\"                   \n[15] \"https://openrouter.ai/docs/guides/features/zero-completion-insurance\"            \n[16] \"https://openrouter.ai/docs/guides/features/zdr\"                                  \n[17] \"https://openrouter.ai/docs/app-attribution\"                                      \n[18] \"https://openrouter.ai/docs/faq#how-are-rate-limits-calculated\"                   \n[19] \"https://openrouter.ai/docs/api/reference/streaming\"                              \n[20] \"https://openrouter.ai/docs/guides/community/frameworks-and-integrations-overview\"\n```\n\n\n:::\n:::\n\n\n\n## Python \n\nFirst, we need to gather the data for our knowledge base. We'll use `requests` and `BeautifulSoup` to scrape URLs from the OpenRouter documentation. This will give us a list of pages to ingest.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport sys\nprint(sys.executable)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n/Library/Frameworks/Python.framework/Versions/3.13/bin/python3\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport requests\nfrom bs4 import BeautifulSoup\nfrom dotenv import load_dotenv\nimport os\nfrom markitdown import MarkItDown\nfrom io import BytesIO\nimport re\n\n# Load environment variables\nload_dotenv()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue\n```\n\n\n:::\n\n```{.python .cell-code}\n# Helper functions\ndef fetch_html(url: str) -> bytes:\n    \"\"\"Fetch HTML content from URL and return as bytes.\"\"\"\n    resp = requests.get(url)\n    resp.raise_for_status()\n    return resp.content\n\ndef html_to_markdown(html_bytes: bytes) -> str:\n    \"\"\"Convert HTML bytes to markdown using MarkItDown.\"\"\"\n    md = MarkItDown()\n    stream = BytesIO(html_bytes)\n    result = md.convert_stream(stream, mime_type=\"text/html\")\n    return result.markdown\n\ndef save_markdown(md_content: str, output_path: str):\n    \"\"\"Save markdown content to file.\"\"\"\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(md_content)\n\ndef sanitize_filename(filename: str) -> str:\n    \"\"\"Sanitize URL to create a valid filename.\"\"\"\n    filename = re.sub(r'^https?://[^/]+', '', filename)\n    filename = re.sub(r'[^\\w\\-_.]', '_', filename)\n    filename = filename.strip('_')\n    if not filename.endswith('.md'):\n        filename += '.md'\n    return filename\n\n# URL to scrape\nurl = \"https://openrouter.ai/docs/quickstart\"\n\n# Read the HTML content of the page\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.content, 'html.parser')\n\n# Extract all <a> tags with href\nlinks = [a['href'] for a in soup.find_all('a', href=True)]\n\n# Remove duplicates\nlinks = list(set(links))\n\n# Keep only full URLs for docs\nlinks_full = [f\"https://openrouter.ai{link}\" for link in links if link.startswith(\"/docs/\")]\n\n# Explicitly add FAQ\nlinks_full.append(\"https://openrouter.ai/docs/faq\")\nlinks_full = list(set(links_full))\n\n# Print all links\nprint(f\"Found {len(links_full)} documentation URLs\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFound 20 documentation URLs\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(links_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['https://openrouter.ai/docs/guides/routing/provider-selection', 'https://openrouter.ai/docs/quickstart', 'https://openrouter.ai/docs/guides/overview/models', 'https://openrouter.ai/docs/guides/features/zero-completion-insurance', 'https://openrouter.ai/docs/api/reference/streaming', 'https://openrouter.ai/docs/guides/community/frameworks-and-integrations-overview', 'https://openrouter.ai/docs/guides/features/presets', 'https://openrouter.ai/docs/faq#how-are-rate-limits-calculated', 'https://openrouter.ai/docs/app-attribution', 'https://openrouter.ai/docs/api/reference/overview', 'https://openrouter.ai/docs/api-reference/overview', 'https://openrouter.ai/docs/guides/overview/principles', 'https://openrouter.ai/docs/faq', 'https://openrouter.ai/docs/guides/features/structured-outputs', 'https://openrouter.ai/docs/guides/features/zdr', 'https://openrouter.ai/docs/guides/routing/model-fallbacks', 'https://openrouter.ai/docs/guides/overview/report-feedback', 'https://openrouter.ai/docs/guides/features/tool-calling', 'https://openrouter.ai/docs/sdks/call-model/overview', 'https://openrouter.ai/docs/guides/features/message-transforms']\n```\n\n\n:::\n:::\n\n\n\n:::\n\n# Save web to local data\n\n::: {.panel-tabset}\n\n## R\n\nTo perform semantic search, we need to store our text data as vectors (embeddings). We'll use `DuckDB` as our local vector store. We also need an embedding model to convert text into vectors. Here, we configure `ragnar` to use a specific embedding model via an OpenAI-compatible API (SiliconFlow).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# pages <- ragnar_find_links(base_url)\npages <- links_full\nstore_location <- \"openrouter.duckdb\"\n\nstore <- ragnar_store_create(\n    store_location,\n    overwrite = TRUE,\n    embed = \\(x) ragnar::embed_openai(x,\n        model = \"BAAI/bge-m3\",\n        base_url = \"https://api.siliconflow.cn/v1\",\n        api_key = Sys.getenv(\"siliconflow\")\n    )\n)\n```\n:::\n\n\n\nWith our store initialized, we can now ingest the data. We iterate through the list of pages we scraped earlier. For each page, we:\n1.  Read the content as markdown.\n2.  Split the content into smaller chunks (approx. 600 characters).\n3.  Insert these chunks into our vector store.\n\nThis process builds the index that we'll search against.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# page=\"https://openrouter.ai/docs/faq\"\n# chunks <- page |>read_as_markdown() |>markdown_chunk(target_size = 2000)\n# ragnar_chunks_view(chunks)\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (page in pages) {\n    message(\"ingesting: \", page)\n    print(page)\n    chunks <- page |>\n        read_as_markdown() |>\n        markdown_chunk(target_size = 2000)\n    # print(chunks)\n    # print('chunks done')\n    ragnar_store_insert(store, chunks)\n    print(\"insrt done\")\n}\n```\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nragnar_store_build_index(store)\n```\n:::\n\n\n\n\n## Python DuckDB\n\n::: {.cell}\n\n```{.python .cell-code}\nimport os\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, Settings\nfrom llama_index.vector_stores.duckdb import DuckDBVectorStore\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\n# --- 1. Configuration ---\n\n# Ensure your API key is available\nopenrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\") # or paste string directly\n\n# Initialize the embedding model pointing to OpenRouter\n# We use the OpenAI class because OpenRouter uses an OpenAI-compatible API structure\nembed_model = OpenAIEmbedding(\n    api_key=openrouter_api_key,\n    base_url=\"https://openrouter.ai/api/v1\",\n    model=\"qwen/qwen3-embedding-8b\"  \n)\n\n# Update the global settings so LlamaIndex knows to use this model\nSettings.embed_model = embed_model\nSettings.chunk_size = 2000\nSettings.chunk_overlap = 200\n# --- 2. Ingestion and Indexing ---\n\n# Load data\ndocuments = SimpleDirectoryReader(\"markdown_docs\").load_data()\n\n# Initialize DuckDB Vector Store\nvector_store = DuckDBVectorStore(\"open.duckdb\", persist_dir=\"./persist/\")\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n# Create the index\n# This will now automatically use the Qwen embeddings defined in Settings\nindex = VectorStoreIndex.from_documents(\n    documents, \n    storage_context=storage_context\n)\n```\n:::\n\n\n\n\n\n\n## Python Chroma\n\nTo perform semantic search, we need to store our text data as vectors (embeddings). We'll use `ChromaDB` as our local vector store. We also need an embedding model to convert text into vectors. Here, we configure a custom `OpenRouterEmbeddings` class to use the `qwen/qwen3-embedding-8b` model via the OpenRouter API.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom openai import OpenAI\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_chroma import Chroma\nfrom typing import List\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Custom embeddings class for OpenRouter API\nclass OpenRouterEmbeddings(Embeddings):\n    \"\"\"Custom embeddings class for OpenRouter API.\"\"\"\n    \n    def __init__(self, api_key: str, model: str = \"text-embedding-3-small\"):\n        self.client = OpenAI(\n            base_url=\"https://openrouter.ai/api/v1\",\n            api_key=api_key,\n        )\n        self.model = model\n    \n    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"Embed a list of documents.\"\"\"\n        response = self.client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self.model,\n            input=texts,\n            encoding_format=\"float\"\n        )\n        return [item.embedding for item in response.data]\n    \n    def embed_query(self, text: str) -> List[float]:\n        \"\"\"Embed a single query.\"\"\"\n        response = self.client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self.model,\n            input=text,\n            encoding_format=\"float\"\n        )\n        return response.data[0].embedding\n\n# Get OpenRouter API key\nopenrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\nif not openrouter_api_key:\n    raise ValueError(\"OPENROUTER_API_KEY not found in environment variables\")\n\n# Create embeddings instance using OpenRouter\nembeddings = OpenRouterEmbeddings(\n    api_key=openrouter_api_key,\n    model=\"qwen/qwen3-embedding-8b\"\n)\n\n# Define vector store location\npersist_directory = \"chroma_db_data\"\n```\n:::\n\n\n\n\n\nWith our store initialized, we can now ingest the data. We iterate through the markdown files we saved earlier. For each file, we:\n1.  Load the content.\n2.  Split the content into smaller chunks (approx. 2000 characters) using `RecursiveCharacterTextSplitter`.\n3.  Create a new `Chroma` vector store from these chunks.\n\nThis process builds the index that we'll search against.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom langchain_core.documents import Document\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nimport shutil\n\n# Helper function to load markdown files\ndef load_markdown_files(directory: str) -> list[Document]:\n    \"\"\"Load all markdown files from directory and create Document objects.\"\"\"\n    documents = []\n    if not os.path.exists(directory):\n        return documents\n        \n    for filename in os.listdir(directory):\n        if filename.endswith('.md'):\n            filepath = os.path.join(directory, filename)\n            with open(filepath, 'r', encoding='utf-8') as f:\n                content = f.read()\n                doc = Document(\n                    page_content=content,\n                    metadata={\n                        \"source\": filename,\n                        \"filepath\": filepath\n                    }\n                )\n                documents.append(doc)\n    return documents\n\n# Create output directory for markdown files\noutput_dir = \"markdown_docs\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Convert each URL to markdown and save\nfor i, link_url in enumerate(links_full, 1):\n    try:\n        print(f\"Processing {i}/{len(links_full)}: {link_url}\")\n        html_content = fetch_html(link_url)\n        markdown_content = html_to_markdown(html_content)\n        filename = sanitize_filename(link_url)\n        output_path = os.path.join(output_dir, filename)\n        save_markdown(markdown_content, output_path)\n        print(f\"  ✓ Saved to {output_path}\")\n    except Exception as e:\n        print(f\"  ✗ Error processing {link_url}: {str(e)}\")\n\n# Load markdown documents\ndocuments = load_markdown_files(output_dir)\nprint(f\"\\nLoaded {len(documents)} markdown documents\")\n\n# Split documents into chunks\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=2000,\n    chunk_overlap=200,\n    length_function=len,\n    is_separator_regex=False,\n)\n\nsplits = text_splitter.split_documents(documents)\nprint(f\"Split into {len(splits)} chunks\")\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Remove existing database if it exists\nif os.path.exists(persist_directory):\n    print(f\"Removing existing database at {persist_directory}...\")\n    shutil.rmtree(persist_directory)\n\n# Create new vector store\nvectorstore = Chroma.from_documents(\n    documents=splits,\n    embedding=embeddings,\n    persist_directory=persist_directory\n)\n\nprint(f\"\\n✓ Successfully created ChromaDB with {len(splits)} chunks!\")\nprint(f\"✓ Database saved to: {persist_directory}\")\n```\n:::\n\n\n\n\n:::\n\n\n\n\n\n\n\n\n\n\n# Retrieval\n\n::: {.panel-tabset}\n\n## R\n\nNow that our knowledge base is populated, we can test the retrieval system. We can ask a specific question, like \"What are model variants?\", and query the store to see which text chunks are most relevant. This confirms that our embeddings and search are working correctly.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstore_location <- \"openrouter.duckdb\"\nstore <- ragnar_store_connect(store_location)\n\ntext <- \"What are model variants?\"\n\n#' # Retrieving Chunks\n#' Once the store is set up, retrieve the most relevant text chunks like this\n\nrelevant_chunks <- ragnar_retrieve(store, text, top_k = 3)\ncat(\"Retrieved\", nrow(relevant_chunks), \"chunks:\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRetrieved 6 chunks:\n```\n\n\n:::\n\n```{.r .cell-code}\nfor (i in seq_len(nrow(relevant_chunks))) {\n    cat(sprintf(\"--- Chunk %d ---\\n%s\\n\\n\", i, relevant_chunks$text[i]))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n````\n--- Chunk 1 ---\n[Zero Completion Insurance](/docs/features/zero-completion-insurance)\n  + [Provisioning API Keys](/docs/features/provisioning-api-keys)\n  + [App Attribution](/docs/app-attribution)\n* API Reference\n\n  + [Overview](/docs/api-reference/overview)\n  + [Streaming](/docs/api-reference/streaming)\n  + [Embeddings](/docs/api-reference/embeddings)\n  + [Limits](/docs/api-reference/limits)\n  + [Authentication](/docs/api-reference/authentication)\n  + [Parameters](/docs/api-reference/parameters)\n  + [Errors](/docs/api-reference/errors)\n  + Responses API\n  + beta.responses\n  + Analytics\n  + Credits\n  + Embeddings\n  + Generations\n  + Models\n  + Endpoints\n  + Parameters\n  + Providers\n  + API Keys\n  + O Auth\n  + Chat\n  + Completions\n* SDK Reference (BETA)\n\n  + [Python SDK](/docs/sdks/python)\n  + [TypeScript SDK](/docs/sdks/typescript)\n* Use Cases\n\n  + [BYOK](/docs/use-cases/byok)\n  + [Crypto API](/docs/use-cases/crypto-api)\n  + [OAuth PKCE](/docs/use-cases/oauth-pkce)\n  + [MCP Servers](/docs/use-cases/mcp-servers)\n  + [Organization Management](/docs/use-cases/organization-management)\n  + [For Providers](/docs/use-cases/for-providers)\n  + [Reasoning Tokens](/docs/use-cases/reasoning-tokens)\n  + [Usage Accounting](/docs/use-cases/usage-accounting)\n  + [User Tracking](/docs/use-cases/user-tracking)\n* Community\n\n  + [Frameworks and Integrations Overview](/docs/community/frameworks-and-integrations-overview)\n  + [Effect AI SDK](/docs/community/effect-ai-sdk)\n  + [Arize](/docs/community/arize)\n  + [LangChain](/docs/community/lang-chain)\n  + [LiveKit](/docs/community/live-kit)\n  + [Langfuse](/docs/community/langfuse)\n  + [Mastra](/docs/community/mastra)\n  + [OpenAI SDK](/docs/community/open-ai-sdk)\n  + [PydanticAI](/docs/community/pydantic-ai)\n  + [Vercel AI SDK](/docs/community/vercel-ai-sdk)\n  + [Xcode](/docs/community/xcode)\n  + [Zapier](/docs/community/zapier)\n  + [Discord](https://discord.gg/openrouter)\n\nLight\n\nOn this page\n\n* [Requests](#requests)\n* [Completions Request Format](#completions-request-format)\n* [Headers](#headers)\n* [Assistant Prefill](#assistant-prefill)\n* [Responses](#responses)\n* [CompletionsResponse Format](#completionsresponse-format)\n* [Finish Reason](#finish-reason)\n* [Querying Cost and Stats](#querying-cost-and-stats)\n\n[API Reference](/docs/api-reference/overview)\n\n\n\n--- Chunk 2 ---\n###### How frequently are new models added?\n\nWe work on adding models as quickly as we can. We often have partnerships with\nthe labs releasing models and can release models as soon as they are\navailable. If there is a model missing that you’d like OpenRouter to support, feel free to message us on\n[Discord](https://discord.gg/openrouter).\n\n###### What are model variants?\n\nVariants are suffixes that can be added to the model slug to change its behavior.\n\nStatic variants can only be used with specific models and these are listed in our [models api](https://openrouter.ai/api/v1/models).\n\n1. `:free` - The model is always provided for free and has low rate limits.\n2. `:beta` - The model is not moderated by OpenRouter.\n3. `:extended` - The model has longer than usual context length.\n4. `:exacto` - The model only uses OpenRouter-curated high-quality endpoints.\n5. `:thinking` - The model supports reasoning by default.\n\nDynamic variants can be used on all models and they change the behavior of how the request is routed or used.\n\n1. `:online` - All requests will run a query to extract web results that are attached to the prompt.\n2. `:nitro` - Providers will be sorted by throughput rather than the default sort, optimizing for faster response times.\n3. `:floor` - Providers will be sorted by price rather than the default sort, prioritizing the most cost-effective options.\n\n###### I am an inference provider, how can I get listed on OpenRouter?\n\nYou can read our requirements at the [Providers\npage](/docs/use-cases/for-providers). If you would like to contact us, the best\nplace to reach us is over email.\n\n###### What is the expected latency/response time for different models?\n\nFor each model on OpenRouter we show the latency (time to first token) and the token\nthroughput for all providers. You can use this to estimate how long requests\nwill take. If you would like to optimize for throughput you can use the\n`:nitro` variant to route to the fastest provider.\n\n\n\n--- Chunk 3 ---\n###### How frequently are new models added?\n\nWe work on adding models as quickly as we can. We often have partnerships with\nthe labs releasing models and can release models as soon as they are\navailable. If there is a model missing that you’d like OpenRouter to support, feel free to message us on\n[Discord](https://discord.gg/openrouter).\n\n###### What are model variants?\n\nVariants are suffixes that can be added to the model slug to change its behavior.\n\nStatic variants can only be used with specific models and these are listed in our [models api](https://openrouter.ai/api/v1/models).\n\n1. `:free` - The model is always provided for free and has low rate limits.\n2. `:beta` - The model is not moderated by OpenRouter.\n3. `:extended` - The model has longer than usual context length.\n4. `:exacto` - The model only uses OpenRouter-curated high-quality endpoints.\n5. `:thinking` - The model supports reasoning by default.\n\nDynamic variants can be used on all models and they change the behavior of how the request is routed or used.\n\n1. `:online` - All requests will run a query to extract web results that are attached to the prompt.\n2. `:nitro` - Providers will be sorted by throughput rather than the default sort, optimizing for faster response times.\n3. `:floor` - Providers will be sorted by price rather than the default sort, prioritizing the most cost-effective options.\n\n###### I am an inference provider, how can I get listed on OpenRouter?\n\nYou can read our requirements at the [Providers\npage](/docs/use-cases/for-providers). If you would like to contact us, the best\nplace to reach us is over email.\n\n###### What is the expected latency/response time for different models?\n\nFor each model on OpenRouter we show the latency (time to first token) and the token\nthroughput for all providers. You can use this to estimate how long requests\nwill take. If you would like to optimize for throughput you can use the\n`:nitro` variant to route to the fastest provider.\n\n\n\n--- Chunk 4 ---\n## The `models` parameter\n\nThe `models` parameter lets you automatically try other models if the primary model’s providers are down, rate-limited, or refuse to reply due to content moderation.\n\nTypeScript SDKTypeScript (fetch)Python\n\n```code-block-root not-prose rounded-b-[inherit] rounded-t-none\n|  |  |\n| --- | --- |\n| 1 | import { OpenRouter } from '@openrouter/sdk'; |\n| 2 |  |\n| 3 | const openRouter = new OpenRouter({ |\n| 4 | apiKey: '<OPENROUTER_API_KEY>', |\n| 5 | }); |\n| 6 |  |\n| 7 | const completion = await openRouter.chat.send({ |\n| 8 | models: ['anthropic/claude-3.5-sonnet', 'gryphe/mythomax-l2-13b'], |\n| 9 | messages: [ |\n| 10 | { |\n| 11 | role: 'user', |\n| 12 | content: 'What is the meaning of life?', |\n| 13 | }, |\n| 14 | ], |\n| 15 | }); |\n| 16 |  |\n| 17 | console.log(completion.choices[0].message.content); |\n```\n\nIf the model you selected returns an error, OpenRouter will try to use the fallback model instead. If the fallback model is down or returns an error, OpenRouter will return that error.\n\nBy default, any error can trigger the use of a fallback model, including context length validation errors, moderation flags for filtered models, rate-limiting, and downtime.\n\nRequests are priced using the model that was ultimately used, which will be returned in the `model` attribute of the response body.\n\n## Using with OpenAI SDK\n\nTo use the `models` array with the OpenAI SDK, include it in the `extra_body` parameter. In the example below, gpt-4o will be tried first, and the `models` array will be tried in order as fallbacks.\n\nPythonTypeScript\n\n\n\n--- Chunk 5 ---\n[Web Search](/docs/features/web-search)\n  + [Zero Completion Insurance](/docs/features/zero-completion-insurance)\n  + [Provisioning API Keys](/docs/features/provisioning-api-keys)\n  + [App Attribution](/docs/app-attribution)\n* API Reference\n\n  + [Overview](/docs/api-reference/overview)\n  + [Streaming](/docs/api-reference/streaming)\n  + [Embeddings](/docs/api-reference/embeddings)\n  + [Limits](/docs/api-reference/limits)\n  + [Authentication](/docs/api-reference/authentication)\n  + [Parameters](/docs/api-reference/parameters)\n  + [Errors](/docs/api-reference/errors)\n  + Responses API\n  + beta.responses\n  + Analytics\n  + Credits\n  + Embeddings\n  + Generations\n  + Models\n  + Endpoints\n  + Parameters\n  + Providers\n  + API Keys\n  + O Auth\n  + Chat\n  + Completions\n* SDK Reference (BETA)\n\n  + [Python SDK](/docs/sdks/python)\n  + [TypeScript SDK](/docs/sdks/typescript)\n* Use Cases\n\n  + [BYOK](/docs/use-cases/byok)\n  + [Crypto API](/docs/use-cases/crypto-api)\n  + [OAuth PKCE](/docs/use-cases/oauth-pkce)\n  + [MCP Servers](/docs/use-cases/mcp-servers)\n  + [Organization Management](/docs/use-cases/organization-management)\n  + [For Providers](/docs/use-cases/for-providers)\n  + [Reasoning Tokens](/docs/use-cases/reasoning-tokens)\n  + [Usage Accounting](/docs/use-cases/usage-accounting)\n  + [User Tracking](/docs/use-cases/user-tracking)\n* Community\n\n  + [Frameworks and Integrations Overview](/docs/community/frameworks-and-integrations-overview)\n  + [Effect AI SDK](/docs/community/effect-ai-sdk)\n  + [Arize](/docs/community/arize)\n  + [LangChain](/docs/community/lang-chain)\n  + [LiveKit](/docs/community/live-kit)\n  + [Langfuse](/docs/community/langfuse)\n  + [Mastra](/docs/community/mastra)\n  + [OpenAI SDK](/docs/community/open-ai-sdk)\n  + [PydanticAI](/docs/community/pydantic-ai)\n  + [Vercel AI SDK](/docs/community/vercel-ai-sdk)\n  + [Xcode](/docs/community/xcode)\n  + [Zapier](/docs/community/zapier)\n  + [Discord](https://discord.gg/openrouter)\n\nLight\n\nOn this page\n\n* [Within OpenRouter](#within-openrouter)\n* [Provider Policies](#provider-policies)\n* [Training on Prompts](#training-on-prompts)\n* [Data Retention & Logging](#data-retention--logging)\n* [Enterprise EU in-region routing](#enterprise-eu-in-region-routing)\n\n[Features](/docs/features/privacy-and-logging)\n\n\n\n--- Chunk 6 ---\n[Web Search](/docs/features/web-search)\n  + [Zero Completion Insurance](/docs/features/zero-completion-insurance)\n  + [Provisioning API Keys](/docs/features/provisioning-api-keys)\n  + [App Attribution](/docs/app-attribution)\n* API Reference\n\n  + [Overview](/docs/api-reference/overview)\n  + [Streaming](/docs/api-reference/streaming)\n  + [Embeddings](/docs/api-reference/embeddings)\n  + [Limits](/docs/api-reference/limits)\n  + [Authentication](/docs/api-reference/authentication)\n  + [Parameters](/docs/api-reference/parameters)\n  + [Errors](/docs/api-reference/errors)\n  + Responses API\n  + beta.responses\n  + Analytics\n  + Credits\n  + Embeddings\n  + Generations\n  + Models\n  + Endpoints\n  + Parameters\n  + Providers\n  + API Keys\n  + O Auth\n  + Chat\n  + Completions\n* SDK Reference (BETA)\n\n  + [Python SDK](/docs/sdks/python)\n  + [TypeScript SDK](/docs/sdks/typescript)\n* Use Cases\n\n  + [BYOK](/docs/use-cases/byok)\n  + [Crypto API](/docs/use-cases/crypto-api)\n  + [OAuth PKCE](/docs/use-cases/oauth-pkce)\n  + [MCP Servers](/docs/use-cases/mcp-servers)\n  + [Organization Management](/docs/use-cases/organization-management)\n  + [For Providers](/docs/use-cases/for-providers)\n  + [Reasoning Tokens](/docs/use-cases/reasoning-tokens)\n  + [Usage Accounting](/docs/use-cases/usage-accounting)\n  + [User Tracking](/docs/use-cases/user-tracking)\n* Community\n\n  + [Frameworks and Integrations Overview](/docs/community/frameworks-and-integrations-overview)\n  + [Effect AI SDK](/docs/community/effect-ai-sdk)\n  + [Arize](/docs/community/arize)\n  + [LangChain](/docs/community/lang-chain)\n  + [LiveKit](/docs/community/live-kit)\n  + [Langfuse](/docs/community/langfuse)\n  + [Mastra](/docs/community/mastra)\n  + [OpenAI SDK](/docs/community/open-ai-sdk)\n  + [PydanticAI](/docs/community/pydantic-ai)\n  + [Vercel AI SDK](/docs/community/vercel-ai-sdk)\n  + [Xcode](/docs/community/xcode)\n  + [Zapier](/docs/community/zapier)\n  + [Discord](https://discord.gg/openrouter)\n\nLight\n\nOn this page\n\n* [How OpenRouter Manages Data Policies](#how-openrouter-manages-data-policies)\n* [Per-Request ZDR Enforcement](#per-request-zdr-enforcement)\n* [Usage](#usage)\n* [Caching](#caching)\n* [OpenRouter’s Retention Policy](#openrouters-retention-policy)\n* [Zero Retention Endpoints](#zero-retention-endpoints)\n\n[Features](/docs/features/privacy-and-logging)\n````\n\n\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ragnar_store_inspect(store)\n#ragnar_chunks_view(chunks)\n```\n:::\n\n\n## Python DuckDB\n\nIn Python, we can use `LlamaIndex` to interact with our DuckDB vector store. In this step, we'll configure the embedding model and retrieve the top relevant chunks for our query, saving them to a file for inspection. We won't use an LLM for generation yet, focusing solely on verifying the retrieval quality.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport os\nimport sys\nprint(f\"Python executable: {sys.executable}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPython executable: /Library/Frameworks/Python.framework/Versions/3.13/bin/python3\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(f\"Python path: {sys.path}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPython path: ['', '/Library/Frameworks/Python.framework/Versions/3.13/bin', '/Library/Frameworks/Python.framework/Versions/3.13/lib/python313.zip', '/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13', '/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/lib-dynload', '/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages', '/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/reticulate/python']\n```\n\n\n:::\n\n```{.python .cell-code}\nfrom typing import Any, List\nfrom openai import OpenAI\nfrom llama_index.core import VectorStoreIndex, Settings\nfrom llama_index.core.embeddings import BaseEmbedding\nfrom llama_index.vector_stores.duckdb import DuckDBVectorStore\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue\n```\n\n\n:::\n\n```{.python .cell-code}\n# Ensure your API key is available\nopenrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n\n# Custom OpenRouter Embedding class for LlamaIndex\nclass OpenRouterEmbedding(BaseEmbedding):\n    \"\"\"Custom embedding class for OpenRouter API compatible with LlamaIndex.\"\"\"\n    \n    def __init__(\n        self,\n        api_key: str,\n        model: str = \"qwen/qwen3-embedding-8b\",\n        **kwargs: Any\n    ):\n        super().__init__(**kwargs)\n        self._client = OpenAI(\n            base_url=\"https://openrouter.ai/api/v1\",\n            api_key=api_key,\n        )\n        self._model = model\n    \n    def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get embedding for a query string.\"\"\"\n        response = self._client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self._model,\n            input=query,\n            encoding_format=\"float\"\n        )\n        return response.data[0].embedding\n    \n    def _get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get embedding for a text string.\"\"\"\n        return self._get_query_embedding(text)\n    \n    async def _aget_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Async version of get_query_embedding.\"\"\"\n        return self._get_query_embedding(query)\n    \n    async def _aget_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Async version of get_text_embedding.\"\"\"\n        return self._get_text_embedding(text)\n\n# 1. Configure Embedding Model using custom OpenRouter class\nembed_model = OpenRouterEmbedding(\n    api_key=openrouter_api_key,\n    model=\"qwen/qwen3-embedding-8b\"\n)\n\n# 2. Apply Settings\nSettings.embed_model = embed_model\n\n# 3. Load and Retrieve\n# Load the existing DuckDB vector store\nvector_store = DuckDBVectorStore(database_name=\"openrouter.duckdb\", persist_dir=\"./persist/\")\nindex = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n\n# Retrieve top 3 chunks\nretriever = index.as_retriever(similarity_top_k=3)\nnodes = retriever.retrieve(\"What are model variants?\")\n\n# Save retrieved chunks to a markdown file for easy inspection\nwith open(\"retriever.md\", \"w\", encoding=\"utf-8\") as f:\n    f.write(f\"# Retrieved {len(nodes)} chunks\\n\\n\")\n    for i, node in enumerate(nodes, 1):\n        content = f\"## Chunk {i}\\n\\n{node.text}\\n\\n\"\n        print(content)\n        f.write(content)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n22\n```\n\n\n:::\n:::\n\n\n\n\n\n## Python Chroma\n\nNow that our knowledge base is populated, we can test the retrieval system. We can ask a specific question, like \"What are model variants?\", and query the `Chroma` store to see which text chunks are most relevant. This confirms that our embeddings and search are working correctly.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom openai import OpenAI\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_chroma import Chroma\nfrom typing import List\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue\n```\n\n\n:::\n\n```{.python .cell-code}\n# Custom embeddings class for OpenRouter API\nclass OpenRouterEmbeddings(Embeddings):\n    \"\"\"Custom embeddings class for OpenRouter API.\"\"\"\n    \n    def __init__(self, api_key: str, model: str = \"qwen/qwen3-embedding-8b\"):\n        self.client = OpenAI(\n            base_url=\"https://openrouter.ai/api/v1\",\n            api_key=api_key,\n        )\n        self.model = model\n    \n    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"Embed a list of documents.\"\"\"\n        response = self.client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self.model,\n            input=texts,\n            encoding_format=\"float\"\n        )\n        return [item.embedding for item in response.data]\n    \n    def embed_query(self, text: str) -> List[float]:\n        \"\"\"Embed a single query.\"\"\"\n        response = self.client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self.model,\n            input=text,\n            encoding_format=\"float\"\n        )\n        return response.data[0].embedding\n\n# Get OpenRouter API key\nopenrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\nif not openrouter_api_key:\n    raise ValueError(\"OPENROUTER_API_KEY not found in environment variables\")\n\n# Create embeddings instance using OpenRouter\nembeddings = OpenRouterEmbeddings(\n    api_key=openrouter_api_key,\n    model=\"qwen/qwen3-embedding-8b\"\n)\n\n# Define vector store location\npersist_directory = \"chroma_db_data\"\n\n# Load existing vector store\nvectorstore = Chroma(\n    persist_directory=persist_directory,\n    embedding_function=embeddings\n)\n\n# Test query\nquery = \"What are model variants?\"\n\n# Perform similarity search\nresults = vectorstore.similarity_search(query, k=5)\n\nprint(f\"\\nQuery: '{query}'\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nQuery: 'What are model variants?'\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(f\"Found {len(results)} relevant chunks:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFound 5 relevant chunks:\n```\n\n\n:::\n\n```{.python .cell-code}\nfor i, doc in enumerate(results, 1):\n    print(f\"Result {i}:\")\n    print(f\"Source: {doc.metadata.get('source', 'unknown')}\")\n    print(f\"Content preview: {doc.page_content[:800]}...\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResult 1:\nSource: docs_faq_how-are-rate-limits-calculated.md\nContent preview: ###### What are model variants?\n\nVariants are suffixes that can be added to the model slug to change its behavior.\n\nStatic variants can only be used with specific models and these are listed in our [models api](https://openrouter.ai/api/v1/models).\n\n1. `:free` - The model is always provided for free and has low rate limits.\n2. `:beta` - The model is not moderated by OpenRouter.\n3. `:extended` - The model has longer than usual context length.\n4. `:exacto` - The model only uses OpenRouter-curated high-quality endpoints.\n5. `:thinking` - The model supports reasoning by default.\n\nDynamic variants can be used on all models and they change the behavior of how the request is routed or used.\n\n1. `:online` - All requests will run a query to extract web results that are attached to the prompt.\n2. `:...\nResult 2:\nSource: docs_faq.md\nContent preview: ###### What are model variants?\n\nVariants are suffixes that can be added to the model slug to change its behavior.\n\nStatic variants can only be used with specific models and these are listed in our [models api](https://openrouter.ai/api/v1/models).\n\n1. `:free` - The model is always provided for free and has low rate limits.\n2. `:beta` - The model is not moderated by OpenRouter.\n3. `:extended` - The model has longer than usual context length.\n4. `:exacto` - The model only uses OpenRouter-curated high-quality endpoints.\n5. `:thinking` - The model supports reasoning by default.\n\nDynamic variants can be used on all models and they change the behavior of how the request is routed or used.\n\n1. `:online` - All requests will run a query to extract web results that are attached to the prompt.\n2. `:...\nResult 3:\nSource: docs_use-cases_crypto-api.md\nContent preview: [API](/docs/api-reference/overview)[Models](https://openrouter.ai/models)[Chat](https://openrouter.ai/chat)[Ranking](https://openrouter.ai/rankings)...\nResult 4:\nSource: docs_sdks_typescript.md\nContent preview: [API](/docs/api-reference/overview)[Models](https://openrouter.ai/models)[Chat](https://openrouter.ai/chat)[Ranking](https://openrouter.ai/rankings)...\nResult 5:\nSource: docs_features_provider-routing.md\nContent preview: Route requests through OpenRouter-curated providers\n\nNext](/docs/features/exacto-variant)[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=openrouter.ai)\n\n[![Logo](https://files.buildwithfern.com/openrouter.docs.buildwithfern.com/docs/2025-11-21T16:36:36.134Z/content/assets/logo.svg)![Logo](https://files.buildwithfern.com/openrouter.docs.buildwithfern.com/docs/2025-11-21T16:36:36.134Z/content/assets/logo-white.svg)](https://openrouter.ai/)\n\n[API](/docs/api-reference/overview)[Models](https://openrouter.ai/models)[Chat](https://openrouter.ai/chat)[Ranking](https://openrouter.ai/rankings)...\n```\n\n\n:::\n:::\n\n\n:::\n\n\n\n# Chat with RAG\n\n::: {.panel-tabset}\n\n## R\n\nThe final piece is to connect this retrieval capability to a chat interface. We use `ellmer` to create a chat client. Crucially, we register a \"retrieval tool\" using `ragnar_register_tool_retrieve`. This gives the LLM the ability to query our vector store whenever it needs information to answer a user's question.\n\nWe also provide a system prompt that instructs the model to always check the knowledge base and cite its sources.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ellmer)\nlibrary(dotenv)\nload_dot_env(file = \".env\")\n\nchat <- chat_openrouter(\n    api_key = Sys.getenv(\"OPENROUTER_API_KEY\"),\n    model = \"openai/gpt-oss-120b\",\n    system_prompt = glue::trim(\"\n  You are an assistant for question-answering tasks. You are concise.\n\n  Before responding, retrieve relevant material from the knowledge store. Quote or\n  paraphrase passages, clearly marking your own words versus the source. Provide a\n  working link for every source cited, as well as any additional relevant links.\n  Do not answer unless you have retrieved and cited a source.If you do not find\n  relevant information, say 'I could not find anything relevant in the knowledge base\n    \")\n) |>\n    ragnar_register_tool_retrieve(store, top_k = 3)\n```\n:::\n\n\n\n\n```{.r .cell-code}\nchat$chat(\"What are model variants?\")\n```\n\n\n**Model variants** are suffixes you can attach to a model’s slug to modify how \nthat model behaves when you call it through OpenRouter.\n\nThere are two kinds:\n\n| Variant | How it works | Example use |\n|--------|--------------|-------------|\n| **Static variants** – only work with certain models (they are listed in \nthe [models API](https://openrouter.ai/api/v1/models)). They change the model’s\nintrinsic properties. <br>• `:free` – always provided at no cost, but with low \nrate limits. <br>• `:beta` – bypasses OpenRouter‑side moderation. <br>• \n`:extended` – gives the model a longer context window than usual. <br>• \n`:exacto` – forces use of OpenRouter‑curated high‑quality endpoints only. <br>•\n`:thinking` – enables built‑in reasoning support. |\n| **Dynamic variants** – can be used with any model. They affect how the \nrequest is routed or processed. <br>• `:online` – runs a web‑search query and \nappends the results to the prompt. <br>• `:nitro` – sorts providers by \nthroughput, favoring the fastest response. <br>• `:floor` – sorts providers by \nprice, favoring the cheapest option. |\n\nThus, by appending one of these suffixes (e.g., `gpt-4o:free` or \n`claude-3.5-sonnet:online`), you tell OpenRouter to apply the corresponding \nbehavior to that request.  \n\n*Source: OpenRouter FAQ – “What are model \nvariants?”*【https://openrouter.ai/docs/faq】\n\n\n## Python chatlas\n\nWe can also use the `chatlas` library to create a chat interface. Here, we define a custom tool `retrieve_trusted_content` that queries our DuckDB index. We then register this tool with the chat model, allowing it to pull in relevant information when answering user questions.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport os\nimport chatlas as ctl\nfrom llama_index.core import VectorStoreIndex, Settings\nfrom llama_index.vector_stores.duckdb import DuckDBVectorStore\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom dotenv import load_dotenv\nload_dotenv()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue\n```\n\n\n:::\n\n```{.python .cell-code}\n# Ensure API key\nopenrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n\n# 1. Configure Embedding Model (LlamaIndex)\nembed_model = OpenAIEmbedding(\n    api_key=openrouter_api_key,\n    base_url=\"https://openrouter.ai/api/v1\",\n    model=\"text-embedding-3-small\"  # Use standard OpenAI model name; OpenRouter requires this\n)\nSettings.embed_model = embed_model\n\n# 2. Load Vector Store\nvector_store = DuckDBVectorStore(database_name=\"open.duckdb\", persist_dir=\"./persist/\")\nindex = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n\n# 3. Define Retrieval Tool\ndef retrieve_trusted_content(query: str, top_k: int = 5):\n    \"\"\"\n    Retrieve relevant content from the knowledge store.\n\n    Parameters\n    ----------\n    query\n        The query used to semantically search the knowledge store.\n    top_k\n        The number of results to retrieve from the knowledge store.\n    \"\"\"\n    #print(f\"Retrieving content for query: '{query}'\")\n    retriever = index.as_retriever(similarity_top_k=top_k)\n    nodes = retriever.retrieve(query)\n    return [f\"<excerpt>{x.text}</excerpt>\" for x in nodes]\n\n# 4. Initialize Chat with System Prompt\nchat = ctl.ChatOpenAI(\n    model=\"openai/gpt-oss-120b\",\n    api_key=openrouter_api_key,\n    base_url=\"https://openrouter.ai/api/v1\",\n    system_prompt=(\n        \"You are an assistant for question-answering tasks. \"\n        \"Use the retrieve_trusted_content tool to find relevant information from the knowledge store. \"\n        \"Answer questions based on the retrieved content. \"\n        \"If you cannot find relevant information, say so clearly.\"\n    )\n)\n\nchat.register_tool(retrieve_trusted_content)\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nresponse=chat.chat(\"What are model variants?\", echo=\"none\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(response)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n**Model variants** are alternative versions of a base machine‑learning model that share the same overall architecture but differ in one or more of the following attributes:\n\n| Attribute that can differ | What it means for the variant |\n|---------------------------|--------------------------------|\n| **Number of parameters / layers** | Smaller variants (e.g., “base”, “small”, “medium”) have fewer weights, run faster, and cost less to use, while larger variants (e.g., “large”, “XL”) tend to be more accurate but are slower and more expensive. |\n| **Training data** | Some variants are trained on a broader or more recent corpus, while others might be trained on domain‑specific data (e.g., legal texts, code, biomedical literature). |\n| **Fine‑tuning / instruction‑tuning** | A base model can be further fine‑tuned for a particular task (e.g., chat, summarisation, code generation) or to follow user instructions more reliably. The resulting model is a *fine‑tuned variant*. |\n| **Safety/Alignment filters** | Certain variants incorporate additional alignment or content‑filtering steps to reduce harmful or biased outputs. |\n| **Context‑window size** | Some variants are engineered to accept longer input sequences (e.g., 32 k tokens vs. the standard 8 k). |\n| **Output format** | Variants may be specialised for embeddings, token‑level classification, function‑calling APIs, or other downstream formats. |\n| **Hardware optimisation** | A variant might be quantised (e.g., 8‑bit) or otherwise optimised for specific hardware (GPU, CPU, mobile). |\n| **Pricing tier** | In commercial APIs, different variants are often exposed as separate pricing tiers (e.g., *gpt‑3.5‑turbo*, *gpt‑4‑turbo*, *gpt‑4‑32k*). |\n\n### Why create variants?\n\n1. **Trade‑off between cost and performance** – Smaller variants let users run inference cheaply and quickly; larger variants give higher accuracy when that matters.\n2. **Specialisation** – Fine‑tuning on a particular domain or task yields a model that outperforms the generic base model for that use‑case.\n3. **Safety & compliance** – Adding alignment or content‑moderation layers helps meet regulatory or product‑safety requirements.\n4. **Technical constraints** – Longer context windows or quantised weights enable use‑cases (e.g., processing whole documents, running on edge devices) that the original model cannot handle efficiently.\n\n### Common real‑world examples\n\n| Platform | Base model | Notable variants |\n|----------|------------|-------------------|\n| OpenAI   | GPT‑4      | **gpt‑4‑turbo** (faster, cheaper, same quality), **gpt‑4‑32k** (longer context), **gpt‑4‑code‑interpreter** (adds execution capability) |\n| OpenAI   | GPT‑3.5    | **gpt‑3.5‑turbo** (optimised for chat), **gpt‑3.5‑code‑davinci** (code‑focused) |\n| Anthropic| Claude     | **Claude‑2**, **Claude‑2‑Instant** (faster, lower‑cost) |\n| Meta     | LLaMA      | **LLaMA‑7B**, **LLaMA‑13B**, **LLaMA‑30B**, **LLaMA‑70B** (size variants), plus instruction‑tuned “LLaMA‑Chat” variants |\n| Cohere   | Command    | **Command‑R** (retrieval‑augmented), **Command‑Light** (smaller, cheaper) |\n\n### How to think about them\n\n- **All variants stem from the same underlying architecture.** If you think of the base model as a “template,” a variant is a concrete instance of that template with a specific configuration.\n- **Choosing a variant is a matter of matching requirements.** If your application needs sub‑second responses on cheap hardware, you’d pick a small, fast variant. If you need the highest possible reasoning ability on long documents, you’d choose a larger, longer‑context variant, possibly with safety filters turned on or off depending on your risk tolerance.\n- **Variants can be stacked.** A model can first be quantised (hardware optimisation) *and* instruction‑tuned (task specialization), yielding a variant that is both fast and better at following instructions.\n\nIn short, **model variants are purpose‑built versions of a core model, each tuned—by size, data, training objective, safety controls, or hardware optimisation—to serve a particular trade‑off or use‑case.**\n```\n\n\n:::\n:::\n\n\n\n\n## Python LangChain\n\nThe final piece is to connect this retrieval capability to a chat interface. We use `LangChain` to create a RAG chain. We create a `retriever` from our vector store and combine it with a `ChatOpenAI` model (using OpenRouter) and a prompt template. This gives the LLM the ability to query our vector store whenever it needs information to answer a user's question.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\nfrom openai import OpenAI\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_chroma import Chroma\nfrom typing import List\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue\n```\n\n\n:::\n\n```{.python .cell-code}\n# Custom embeddings class for OpenRouter API\nclass OpenRouterEmbeddings(Embeddings):\n    \"\"\"Custom embeddings class for OpenRouter API.\"\"\"\n    \n    def __init__(self, api_key: str, model: str = \"qwen/qwen3-embedding-8b\"):\n        self.client = OpenAI(\n            base_url=\"https://openrouter.ai/api/v1\",\n            api_key=api_key,\n        )\n        self.model = model\n    \n    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"Embed a list of documents.\"\"\"\n        response = self.client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self.model,\n            input=texts,\n            encoding_format=\"float\"\n        )\n        return [item.embedding for item in response.data]\n    \n    def embed_query(self, text: str) -> List[float]:\n        \"\"\"Embed a single query.\"\"\"\n        response = self.client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self.model,\n            input=text,\n            encoding_format=\"float\"\n        )\n        return response.data[0].embedding\n\n# Get OpenRouter API key\nopenrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\nif not openrouter_api_key:\n    raise ValueError(\"OPENROUTER_API_KEY not found in environment variables\")\n\n# Create embeddings instance using OpenRouter\nembeddings = OpenRouterEmbeddings(\n    api_key=openrouter_api_key,\n    model=\"qwen/qwen3-embedding-8b\"\n)\n\n# Define vector store location\npersist_directory = \"chroma_db_data\"\n\n# Load existing vector store\nprint(f\"Loading existing vectorstore from {persist_directory}...\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLoading existing vectorstore from chroma_db_data...\n```\n\n\n:::\n\n```{.python .cell-code}\nvectorstore = Chroma(\n    persist_directory=persist_directory,\n    embedding_function=embeddings\n)\nprint(f\"✓ Loaded vectorstore\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n✓ Loaded vectorstore\n```\n\n\n:::\n\n```{.python .cell-code}\n# Initialize LLM using OpenRouter\nllm = ChatOpenAI(\n    model=\"openai/gpt-oss-120b\",\n    openai_api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n    openai_api_base=\"https://openrouter.ai/api/v1\"\n)\n\n# Create prompt template\nsystem_prompt = (\n    \"You are an assistant for question-answering tasks. \"\n    \"Use the following pieces of retrieved context to answer \"\n    \"the question. If you don't know the answer, say that you \"\n    \"don't know. Use three sentences maximum and keep the \"\n    \"answer concise.\"\n    \"\\n\\n\"\n    \"Context: {context}\"\n    \"\\n\\n\"\n    \"Question: {question}\"\n)\n\nprompt = ChatPromptTemplate.from_template(system_prompt)\n\n# Create retriever\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n\n# Helper function to format documents\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n# Create RAG chain using LCEL\nrag_chain = (\n    {\n        \"context\": retriever | format_docs,\n        \"question\": RunnablePassthrough()\n    }\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\nprint(\"✓ RAG chain created successfully!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n✓ RAG chain created successfully!\n```\n\n\n:::\n\n```{.python .cell-code}\n# Test the RAG chain\nquestion = \"What are model variants?\"\n\nprint(f\"\\nQuestion: {question}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nQuestion: What are model variants?\n```\n\n\n:::\n\n```{.python .cell-code}\n# Get context documents separately for display\ncontext_docs = retriever.invoke(question)\n\n# Invoke the RAG chain\nanswer = rag_chain.invoke(question)\nimport textwrap\n\nfor line in answer.split('\\n'):\n    print(textwrap.fill(line, width=80))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel variants are suffixes you append to a model’s slug to modify its behavior.\nStatic variants (e.g., `:free`, `:beta`, `:extended`, `:exacto`, `:thinking`)\napply only to specific models, while dynamic variants\n(e.g., `:online`, `:nitro`, `:floor`) work with any model and affect routing or\nusage. These modifiers let you control factors like cost, moderation, context\nlength, reasoning support, web‑search integration, speed, or price.\n```\n\n\n:::\n:::\n\n\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}