{
  "hash": "1399f5b690a32fce0d42712e39d27789",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"YOLO Object Detection App with Streamlit\"\nauthor: \"Tony D\"\ndate: \"2025-11-05\"\ncategories: [AI, API, tutorial]\nimage: \"images/0.png\"\n\nformat:\n  html:\n    code-fold: true\n    code-tools: true\n    code-copy: true\n    \nexecute:\n  eval: false\n  warning: false\n  \n  \n---\n\n\n# Overview\n\nThe application is a comprehensive Streamlit web app that provides object detection capabilities using YOLO11 (Ultralytics framework) with support for multiple input sources and processing backends. What makes this project special is its multi-model architecture and production-ready features.\n\n\nLive Demo: [https://yolo-live.streamlit.app/](https://yolo-live.streamlit.app/)\n\nGithub: [https://github.com/JCwinning/YOLO_app](https://github.com/JCwinning/YOLO_app)\n\n\n::: {.panel-tabset}\n\n## Object detection\n\n![Application Screenshot - Main Interface](images/0.png){width=\"100%\"}\n\n## 物体识别\n\n![Application Screenshot - Detection Results](images/1.png){width=\"100%\"}\n:::\n\n\n## Key Features\n\n### Multi-Input Support\nThe application supports various input methods:\n- **File Upload**: Images and videos from local storage\n- **URL Input**: Direct image URLs from the web\n- **Live Camera**: Real-time photo capture using device cameras\n\n### Multi-Model Architecture\nOne of the standout features is the support for different AI models:\n\n#### 1. Local YOLO11 Models\n- Five different model variants (nano, small, medium, large, extra-large)\n- Automatic device detection with MPS acceleration for Apple Silicon\n- CPU fallback for broader compatibility\n\n#### 2. Cloud-Based Models\n- **Qwen-Image-Edit** via DashScope API for advanced image annotation\n- **Gemini 2.5 Flash Image** via OpenRouter API for cutting-edge processing\n\n### Advanced Features\n- **Bilingual Interface**: Full English/Chinese support with 113+ translated strings\n- **Smart UI Management**: Automatic hiding of input images after processing\n- **Download Capabilities**: Save annotated results locally\n- **Progress Tracking**: Real-time progress updates for video processing\n- **Session Management**: Persistent state across user interactions\n\n## Technical Architecture\n\n```{mermaid}\n%%| fig-cap: \"System Architecture Diagram\"\nflowchart LR\n    A[User Interface<br/>Streamlit] --> B[Input Sources]\n\n    B --> C[File Upload]\n    B --> D[Image URL]\n    B --> E[Live Camera]\n\n    A --> F[Processing Models]\n\n    F --> G[Local YOLO11<br/>n/s/m/l/x]\n    F --> H[Qwen-Image-Edit<br/>DashScope API]\n    F --> I[Gemini 2.5 Flash<br/>OpenRouter API]\n\n    G --> J[Device Detection<br/>MPS/CPU]\n\n    J --> K[Results<br/>Annotated Images/Videos]\n    H --> K\n    I --> K\n\n    A --> L[Features]\n    L --> M[Bilingual UI<br/>EN/ZH]\n    L --> N[Download Results]\n    L --> O[Session Management]\n```\n\n### Core Dependencies\n\n```toml\n[project]\nname = \"yolo-app\"\nrequires-python = \">=3.12\"\ndependencies = [\n    \"dashscope>=1.17.0\",      # Alibaba Cloud API\n    \"opencv-python>=4.11.0.86\", # Computer vision\n    \"streamlit>=1.50.0\",       # Web framework\n    \"torch>=2.2\",              # Deep learning\n    \"ultralytics>=8.3.0\",      # YOLO framework\n]\n```\n\n### Application Structure\n\nThe main application (`app.py`) consists of over 1,000 lines of well-structured Python code organized into several key components:\n\n#### 1. Internationalization System\n\n::: {#3b8b774a .cell execution_count=1}\n``` {.python .cell-code}\nfrom language import translations\n\ndef get_translation(key, **kwargs):\n    \"\"\"Translation function that uses the current session language\"\"\"\n    lang = st.session_state.get(\"language\", \"en\")\n    text = translations[lang].get(key, translations[\"en\"].get(key, key))\n    return text.format(**kwargs) if kwargs else text\n```\n:::\n\n\n#### 2. Device Optimization\n\n::: {#732a5006 .cell execution_count=2}\n``` {.python .cell-code}\ndef get_device():\n    \"\"\"Automatically detect the best available device\"\"\"\n    if torch.backends.mps.is_available():\n        return \"mps\"  # Apple Silicon GPU acceleration\n    return \"cpu\"      # Fallback to CPU\n\n# Model loading with device optimization\ndevice = get_device()\nmodel = YOLO(selected_model).to(device)\nst.info(f\"Using device: {device.upper()}\")\n```\n:::\n\n\n#### 3. Image Processing Pipeline\n\n::: {#0d2154c5 .cell execution_count=3}\n``` {.python .cell-code}\ndef encode_image_to_base64(image):\n    \"\"\"Encode PIL Image to base64 string with size compression\"\"\"\n    max_size_bytes = 8 * 1024 * 1024  # 8MB limit\n\n    formats_and_qualities = [\n        (\"JPEG\", 95), (\"JPEG\", 85), (\"JPEG\", 75),\n        (\"WEBP\", 95), (\"WEBP\", 85), (\"WEBP\", 75),\n    ]\n\n    for fmt, quality in formats_and_qualities:\n        # Try different compression strategies\n        # ... compression logic\n```\n:::\n\n\n### Multi-Model Processing\n\n#### Local YOLO Processing\nThe app supports all YOLO11 model variants with automatic performance optimization:\n\n::: {#34ee6418 .cell execution_count=4}\n``` {.python .cell-code}\n# Model selection interface\nmodel_options = [\"yolo11n.pt\", \"yolo11s.pt\", \"yolo11m.pt\", \"yolo11l.pt\", \"yolo11x.pt\"]\nmodel_descriptions = {\n    \"yolo11n.pt\": \"Nano - Fastest, lowest accuracy\",\n    \"yolo11s.pt\": \"Small - Good balance\",\n    \"yolo11m.pt\": \"Medium - Recommended\",\n    \"yolo11l.pt\": \"Large - Higher accuracy\",\n    \"yolo11x.pt\": \"Extra Large - Highest accuracy\"\n}\n\nselected_model = st.sidebar.selectbox(\n    get_translation(\"model_selection\"),\n    model_options,\n    index=model_options.index(\"yolo11s.pt\"),\n    format_func=lambda x: f\"{model_descriptions[x]} ({x})\"\n)\n\n# Detection process with progress tracking\ndef detect_objects(image, model, confidence_threshold=0.5):\n    \"\"\"Perform object detection with progress tracking\"\"\"\n    with st.spinner(get_translation(\"processing\")):\n        results = model.predict(image, conf=confidence_threshold)\n\n        # Process results\n        detections = []\n        for result in results:\n            boxes = result.boxes\n            for box in boxes:\n                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n                conf = box.conf[0].cpu().numpy()\n                cls = int(box.cls[0].cpu().numpy())\n                class_name = model.names[cls]\n\n                detections.append({\n                    'class': class_name,\n                    'confidence': conf,\n                    'bbox': [x1, y1, x2, y2]\n                })\n\n    return detections, results\n```\n:::\n\n\n#### Cloud API Integration\nFor cloud-based models, the app handles API authentication and request formatting:\n\n::: {#1760b2b2 .cell execution_count=5}\n``` {.python .cell-code}\ndef process_with_qwen(image, api_key):\n    \"\"\"Process image using Qwen-Image-Edit via DashScope\"\"\"\n    try:\n        response = MultiModalConversation.call(\n            model='qwen-image-edit',\n            input=[\n                {\n                    'role': 'user',\n                    'content': [\n                        {'image': f\"data:image/jpeg;base64,{image_b64}\"},\n                        {'text': 'Please identify and label all objects in this image.'}\n                    ]\n                }\n            ]\n        )\n        return response\n    except Exception as e:\n        st.error(f\"API Error: {str(e)}\")\n        return None\n```\n:::\n\n\n## User Interface Design\n\n### Layout Structure\nThe application uses a professional three-column layout:\n\n1. **Sidebar**: Model selection, confidence threshold, language settings\n2. **Main Area**: Input method selection, image/video display, results\n3. **Results Panel**: Detection statistics, download options\n\n### Bilingual Support\nThe translation system handles all UI elements:\n\n::: {#af5be7e6 .cell execution_count=6}\n``` {.python .cell-code}\ntranslations = {\n    \"en\": {\n        \"title\": \"YOLO11 Object Detection\",\n        \"upload_file\": \"Upload File\",\n        \"camera_input\": \"Use Camera\",\n        # ... more strings\n    },\n    \"zh\": {\n        \"title\": \"YOLO11 目标检测\",\n        \"upload_file\": \"上传文件\",\n        \"camera_input\": \"使用相机\",\n        # ... corresponding translations\n    }\n}\n```\n:::\n\n\n## Performance Optimizations\n\n### Model Performance Comparison\n\n| Model | Parameters | mAP | Inference Time (CPU) | Inference Time (MPS) |\n|-------|------------|-----|---------------------|---------------------|\n| YOLO11n | 2.6M | 37.2 | 15ms | 3ms |\n| YOLO11s | 9.4M | 45.5 | 28ms | 6ms |\n| YOLO11m | 25.4M | 51.2 | 52ms | 12ms |\n| YOLO11l | 43.7M | 53.4 | 84ms | 18ms |\n| YOLO11x | 68.2M | 54.7 | 126ms | 26ms |\n\n### Apple Silicon Acceleration\nThe app automatically detects and utilizes Metal Performance Shaders (MPS) on Apple Silicon devices:\n\n::: {#3c8ceb12 .cell execution_count=7}\n``` {.python .cell-code}\ndevice = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\nmodel = YOLO(selected_model).to(device)\n\n# Performance monitoring\nimport time\nstart_time = time.time()\nresults = model.predict(image)\ninference_time = (time.time() - start_time) * 1000\n\nst.metric(f\"Inference Time ({device.upper()})\", f\"{inference_time:.1f}ms\")\n```\n:::\n\n\n### Image Compression for Cloud APIs\nTo meet API size limits, the app implements smart image compression:\n\n::: {#04bf9668 .cell execution_count=8}\n``` {.python .cell-code}\ndef compress_image_for_api(image, max_size=8*1024*1024):\n    \"\"\"Compress image to meet API requirements\"\"\"\n    for quality in [95, 85, 75, 65]:\n        for fmt in [\"JPEG\", \"WEBP\"]:\n            buffer = BytesIO()\n            image.save(buffer, format=fmt, quality=quality)\n            if buffer.tell() <= max_size:\n                return buffer.getvalue()\n    return None\n```\n:::\n\n\n## Deployment and Production Features\n\n### Session Management\nThe application maintains comprehensive session state:\n\n::: {#c87f9cb1 .cell execution_count=9}\n``` {.python .cell-code}\nsession_state_vars = [\n    \"current_image\", \"uploaded_image_bytes\",\n    \"current_video\", \"uploaded_video_bytes\",\n    \"camera_active\", \"camera_frame\",\n    \"qwen_processed\", \"gemini_processed\",\n    \"language\", \"input_method_index\"\n]\n```\n:::\n\n\n### Error Handling\nRobust error handling ensures graceful degradation:\n\n::: {#48624ce5 .cell execution_count=10}\n``` {.python .cell-code}\ntry:\n    result = model.predict(image, conf=confidence_threshold)\n    st.success(get_translation(\"detection_success\"))\nexcept Exception as e:\n    st.error(f\"Detection failed: {str(e)}\")\n    # Fallback to alternative processing method\n```\n:::\n\n\n## Getting Started\n\n### Prerequisites\n- Python 3.12 or higher\n- Modern package manager (uv recommended)\n- For cloud models: API keys from DashScope and OpenRouter\n\n### Installation\n```bash\n# Clone the repository\ngit clone <repository-url>\ncd YOLO_app\n\n# Install dependencies with uv (recommended)\nuv sync\n\n# Alternative: pip install\npip install -r requirements.txt\n\n# Run the application\nstreamlit run app.py\n```\n\n### API Configuration\nCreate a `.env` file with your API keys:\n```bash\n# Alibaba Cloud DashScope API\nDASHSCOPE_API_KEY=your_dashscope_key\n\n# OpenRouter API (for Gemini)\nOPENROUTER_API_KEY=your_openrouter_key\n```\n\n### Quick Usage Examples\n\n#### Basic Image Detection\n1. Launch the application\n2. Upload an image or provide an image URL\n3. Select your preferred YOLO11 model (yolo11s.pt recommended)\n4. Adjust confidence threshold if needed\n5. Click \"Detect Objects\"\n6. View results and download annotated image\n\n#### Real-time Camera Detection\n1. Select \"Use Camera\" input method\n2. Grant camera permissions when prompted\n3. Capture a photo\n4. Choose detection model\n5. Get instant object detection results\n\n#### Cloud Model Processing\n1. Enter your API keys in the sidebar\n2. Upload an image\n3. Select \"Qwen-Image-Edit\" or \"Gemini 2.5 Flash\" model\n4. Process image with advanced AI capabilities\n5. Compare results with local YOLO models\n\n## Future Enhancements\n\nPotential improvements for future versions:\n\n1. **Additional Models**: Integration with more cloud AI services\n2. **Real-time Video Processing**: Enhanced video streaming capabilities\n3. **Custom Model Training**: Allow users to train custom YOLO models\n4. **Mobile Optimization**: PWA features for mobile device support\n5. **Batch Processing**: Process multiple images simultaneously\n\n## Conclusion\n\nThis YOLO object detection application demonstrates how to build a sophisticated, production-ready computer vision system. The combination of local and cloud-based models, bilingual support, and comprehensive error handling makes it suitable for both development and production environments.\n\nThe project showcases best practices in:\n- Modern Python development with dependency management\n- Streamlit web application architecture\n- Computer vision API integration\n- Internationalization and accessibility\n- Performance optimization for different hardware platforms\n\nWhether you're interested in computer vision, web development, or AI applications, this project provides an excellent foundation for building advanced AI-powered web applications.\n\n---\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}