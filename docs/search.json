[
  {
    "objectID": "posts/yolo-app/index.html",
    "href": "posts/yolo-app/index.html",
    "title": "YOLO Object Detection App with Streamlit",
    "section": "",
    "text": "The application is a comprehensive Streamlit web app that provides object detection capabilities using YOLO11 (Ultralytics framework) with support for multiple input sources and processing backends. What makes this project special is its multi-model architecture and production-ready features.\nLive Demo: https://yolo-live.streamlit.app/\nGithub: https://github.com/JCwinning/YOLO_app\n\nObject detection物体识别\n\n\n\n\n\nApplication Screenshot - Main Interface\n\n\n\n\n\n\n\nApplication Screenshot - Detection Results\n\n\n\n\n\n\n\n\n\nThe application supports various input methods: - File Upload: Images and videos from local storage - URL Input: Direct image URLs from the web - Live Camera: Real-time photo capture using device cameras\n\n\n\nOne of the standout features is the support for different AI models:\n\n\n\nFive different model variants (nano, small, medium, large, extra-large)\nAutomatic device detection with MPS acceleration for Apple Silicon\nCPU fallback for broader compatibility\n\n\n\n\n\nQwen-Image-Edit via DashScope API for advanced image annotation\nGemini 2.5 Flash Image via OpenRouter API for cutting-edge processing\n\n\n\n\n\n\nBilingual Interface: Full English/Chinese support with 113+ translated strings\nSmart UI Management: Automatic hiding of input images after processing\nDownload Capabilities: Save annotated results locally\nProgress Tracking: Real-time progress updates for video processing\nSession Management: Persistent state across user interactions\n\n\n\n\n\n\n\n\n\n\n\n\n System Architecture Diagram \n\n\n\n\n\n[project]\nname = \"yolo-app\"\nrequires-python = \"&gt;=3.12\"\ndependencies = [\n    \"dashscope&gt;=1.17.0\",      # Alibaba Cloud API\n    \"opencv-python&gt;=4.11.0.86\", # Computer vision\n    \"streamlit&gt;=1.50.0\",       # Web framework\n    \"torch&gt;=2.2\",              # Deep learning\n    \"ultralytics&gt;=8.3.0\",      # YOLO framework\n]\n\n\n\nThe main application (app.py) consists of over 1,000 lines of well-structured Python code organized into several key components:\n\n\n\n\nCode\nfrom language import translations\n\ndef get_translation(key, **kwargs):\n    \"\"\"Translation function that uses the current session language\"\"\"\n    lang = st.session_state.get(\"language\", \"en\")\n    text = translations[lang].get(key, translations[\"en\"].get(key, key))\n    return text.format(**kwargs) if kwargs else text\n\n\n\n\n\n\n\nCode\ndef get_device():\n    \"\"\"Automatically detect the best available device\"\"\"\n    if torch.backends.mps.is_available():\n        return \"mps\"  # Apple Silicon GPU acceleration\n    return \"cpu\"      # Fallback to CPU\n\n# Model loading with device optimization\ndevice = get_device()\nmodel = YOLO(selected_model).to(device)\nst.info(f\"Using device: {device.upper()}\")\n\n\n\n\n\n\n\nCode\ndef encode_image_to_base64(image):\n    \"\"\"Encode PIL Image to base64 string with size compression\"\"\"\n    max_size_bytes = 8 * 1024 * 1024  # 8MB limit\n\n    formats_and_qualities = [\n        (\"JPEG\", 95), (\"JPEG\", 85), (\"JPEG\", 75),\n        (\"WEBP\", 95), (\"WEBP\", 85), (\"WEBP\", 75),\n    ]\n\n    for fmt, quality in formats_and_qualities:\n        # Try different compression strategies\n        # ... compression logic\n\n\n\n\n\n\n\n\nThe app supports all YOLO11 model variants with automatic performance optimization:\n\n\nCode\n# Model selection interface\nmodel_options = [\"yolo11n.pt\", \"yolo11s.pt\", \"yolo11m.pt\", \"yolo11l.pt\", \"yolo11x.pt\"]\nmodel_descriptions = {\n    \"yolo11n.pt\": \"Nano - Fastest, lowest accuracy\",\n    \"yolo11s.pt\": \"Small - Good balance\",\n    \"yolo11m.pt\": \"Medium - Recommended\",\n    \"yolo11l.pt\": \"Large - Higher accuracy\",\n    \"yolo11x.pt\": \"Extra Large - Highest accuracy\"\n}\n\nselected_model = st.sidebar.selectbox(\n    get_translation(\"model_selection\"),\n    model_options,\n    index=model_options.index(\"yolo11s.pt\"),\n    format_func=lambda x: f\"{model_descriptions[x]} ({x})\"\n)\n\n# Detection process with progress tracking\ndef detect_objects(image, model, confidence_threshold=0.5):\n    \"\"\"Perform object detection with progress tracking\"\"\"\n    with st.spinner(get_translation(\"processing\")):\n        results = model.predict(image, conf=confidence_threshold)\n\n        # Process results\n        detections = []\n        for result in results:\n            boxes = result.boxes\n            for box in boxes:\n                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n                conf = box.conf[0].cpu().numpy()\n                cls = int(box.cls[0].cpu().numpy())\n                class_name = model.names[cls]\n\n                detections.append({\n                    'class': class_name,\n                    'confidence': conf,\n                    'bbox': [x1, y1, x2, y2]\n                })\n\n    return detections, results\n\n\n\n\n\nFor cloud-based models, the app handles API authentication and request formatting:\n\n\nCode\ndef process_with_qwen(image, api_key):\n    \"\"\"Process image using Qwen-Image-Edit via DashScope\"\"\"\n    try:\n        response = MultiModalConversation.call(\n            model='qwen-image-edit',\n            input=[\n                {\n                    'role': 'user',\n                    'content': [\n                        {'image': f\"data:image/jpeg;base64,{image_b64}\"},\n                        {'text': 'Please identify and label all objects in this image.'}\n                    ]\n                }\n            ]\n        )\n        return response\n    except Exception as e:\n        st.error(f\"API Error: {str(e)}\")\n        return None\n\n\n\n\n\n\n\n\n\nThe application uses a professional three-column layout:\n\nSidebar: Model selection, confidence threshold, language settings\nMain Area: Input method selection, image/video display, results\nResults Panel: Detection statistics, download options\n\n\n\n\nThe translation system handles all UI elements:\n\n\nCode\ntranslations = {\n    \"en\": {\n        \"title\": \"YOLO11 Object Detection\",\n        \"upload_file\": \"Upload File\",\n        \"camera_input\": \"Use Camera\",\n        # ... more strings\n    },\n    \"zh\": {\n        \"title\": \"YOLO11 目标检测\",\n        \"upload_file\": \"上传文件\",\n        \"camera_input\": \"使用相机\",\n        # ... corresponding translations\n    }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nParameters\nmAP\nInference Time (CPU)\nInference Time (MPS)\n\n\n\n\nYOLO11n\n2.6M\n37.2\n15ms\n3ms\n\n\nYOLO11s\n9.4M\n45.5\n28ms\n6ms\n\n\nYOLO11m\n25.4M\n51.2\n52ms\n12ms\n\n\nYOLO11l\n43.7M\n53.4\n84ms\n18ms\n\n\nYOLO11x\n68.2M\n54.7\n126ms\n26ms\n\n\n\n\n\n\nThe app automatically detects and utilizes Metal Performance Shaders (MPS) on Apple Silicon devices:\n\n\nCode\ndevice = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\nmodel = YOLO(selected_model).to(device)\n\n# Performance monitoring\nimport time\nstart_time = time.time()\nresults = model.predict(image)\ninference_time = (time.time() - start_time) * 1000\n\nst.metric(f\"Inference Time ({device.upper()})\", f\"{inference_time:.1f}ms\")\n\n\n\n\n\nTo meet API size limits, the app implements smart image compression:\n\n\nCode\ndef compress_image_for_api(image, max_size=8*1024*1024):\n    \"\"\"Compress image to meet API requirements\"\"\"\n    for quality in [95, 85, 75, 65]:\n        for fmt in [\"JPEG\", \"WEBP\"]:\n            buffer = BytesIO()\n            image.save(buffer, format=fmt, quality=quality)\n            if buffer.tell() &lt;= max_size:\n                return buffer.getvalue()\n    return None\n\n\n\n\n\n\n\n\nThe application maintains comprehensive session state:\n\n\nCode\nsession_state_vars = [\n    \"current_image\", \"uploaded_image_bytes\",\n    \"current_video\", \"uploaded_video_bytes\",\n    \"camera_active\", \"camera_frame\",\n    \"qwen_processed\", \"gemini_processed\",\n    \"language\", \"input_method_index\"\n]\n\n\n\n\n\nRobust error handling ensures graceful degradation:\n\n\nCode\ntry:\n    result = model.predict(image, conf=confidence_threshold)\n    st.success(get_translation(\"detection_success\"))\nexcept Exception as e:\n    st.error(f\"Detection failed: {str(e)}\")\n    # Fallback to alternative processing method\n\n\n\n\n\n\n\n\n\nPython 3.12 or higher\nModern package manager (uv recommended)\nFor cloud models: API keys from DashScope and OpenRouter\n\n\n\n\n# Clone the repository\ngit clone &lt;repository-url&gt;\ncd YOLO_app\n\n# Install dependencies with uv (recommended)\nuv sync\n\n# Alternative: pip install\npip install -r requirements.txt\n\n# Run the application\nstreamlit run app.py\n\n\n\nCreate a .env file with your API keys:\n# Alibaba Cloud DashScope API\nDASHSCOPE_API_KEY=your_dashscope_key\n\n# OpenRouter API (for Gemini)\nOPENROUTER_API_KEY=your_openrouter_key\n\n\n\n\n\n\nLaunch the application\nUpload an image or provide an image URL\nSelect your preferred YOLO11 model (yolo11s.pt recommended)\nAdjust confidence threshold if needed\nClick “Detect Objects”\nView results and download annotated image\n\n\n\n\n\nSelect “Use Camera” input method\nGrant camera permissions when prompted\nCapture a photo\nChoose detection model\nGet instant object detection results\n\n\n\n\n\nEnter your API keys in the sidebar\nUpload an image\nSelect “Qwen-Image-Edit” or “Gemini 2.5 Flash” model\nProcess image with advanced AI capabilities\nCompare results with local YOLO models\n\n\n\n\n\n\nPotential improvements for future versions:\n\nAdditional Models: Integration with more cloud AI services\nReal-time Video Processing: Enhanced video streaming capabilities\nCustom Model Training: Allow users to train custom YOLO models\nMobile Optimization: PWA features for mobile device support\nBatch Processing: Process multiple images simultaneously\n\n\n\n\nThis YOLO object detection application demonstrates how to build a sophisticated, production-ready computer vision system. The combination of local and cloud-based models, bilingual support, and comprehensive error handling makes it suitable for both development and production environments.\nThe project showcases best practices in: - Modern Python development with dependency management - Streamlit web application architecture - Computer vision API integration - Internationalization and accessibility - Performance optimization for different hardware platforms\nWhether you’re interested in computer vision, web development, or AI applications, this project provides an excellent foundation for building advanced AI-powered web applications."
  },
  {
    "objectID": "posts/yolo-app/index.html#key-features",
    "href": "posts/yolo-app/index.html#key-features",
    "title": "YOLO Object Detection App with Streamlit",
    "section": "",
    "text": "The application supports various input methods: - File Upload: Images and videos from local storage - URL Input: Direct image URLs from the web - Live Camera: Real-time photo capture using device cameras\n\n\n\nOne of the standout features is the support for different AI models:\n\n\n\nFive different model variants (nano, small, medium, large, extra-large)\nAutomatic device detection with MPS acceleration for Apple Silicon\nCPU fallback for broader compatibility\n\n\n\n\n\nQwen-Image-Edit via DashScope API for advanced image annotation\nGemini 2.5 Flash Image via OpenRouter API for cutting-edge processing\n\n\n\n\n\n\nBilingual Interface: Full English/Chinese support with 113+ translated strings\nSmart UI Management: Automatic hiding of input images after processing\nDownload Capabilities: Save annotated results locally\nProgress Tracking: Real-time progress updates for video processing\nSession Management: Persistent state across user interactions"
  },
  {
    "objectID": "posts/yolo-app/index.html#technical-architecture",
    "href": "posts/yolo-app/index.html#technical-architecture",
    "title": "YOLO Object Detection App with Streamlit",
    "section": "",
    "text": "System Architecture Diagram \n\n\n\n\n\n[project]\nname = \"yolo-app\"\nrequires-python = \"&gt;=3.12\"\ndependencies = [\n    \"dashscope&gt;=1.17.0\",      # Alibaba Cloud API\n    \"opencv-python&gt;=4.11.0.86\", # Computer vision\n    \"streamlit&gt;=1.50.0\",       # Web framework\n    \"torch&gt;=2.2\",              # Deep learning\n    \"ultralytics&gt;=8.3.0\",      # YOLO framework\n]\n\n\n\nThe main application (app.py) consists of over 1,000 lines of well-structured Python code organized into several key components:\n\n\n\n\nCode\nfrom language import translations\n\ndef get_translation(key, **kwargs):\n    \"\"\"Translation function that uses the current session language\"\"\"\n    lang = st.session_state.get(\"language\", \"en\")\n    text = translations[lang].get(key, translations[\"en\"].get(key, key))\n    return text.format(**kwargs) if kwargs else text\n\n\n\n\n\n\n\nCode\ndef get_device():\n    \"\"\"Automatically detect the best available device\"\"\"\n    if torch.backends.mps.is_available():\n        return \"mps\"  # Apple Silicon GPU acceleration\n    return \"cpu\"      # Fallback to CPU\n\n# Model loading with device optimization\ndevice = get_device()\nmodel = YOLO(selected_model).to(device)\nst.info(f\"Using device: {device.upper()}\")\n\n\n\n\n\n\n\nCode\ndef encode_image_to_base64(image):\n    \"\"\"Encode PIL Image to base64 string with size compression\"\"\"\n    max_size_bytes = 8 * 1024 * 1024  # 8MB limit\n\n    formats_and_qualities = [\n        (\"JPEG\", 95), (\"JPEG\", 85), (\"JPEG\", 75),\n        (\"WEBP\", 95), (\"WEBP\", 85), (\"WEBP\", 75),\n    ]\n\n    for fmt, quality in formats_and_qualities:\n        # Try different compression strategies\n        # ... compression logic\n\n\n\n\n\n\n\n\nThe app supports all YOLO11 model variants with automatic performance optimization:\n\n\nCode\n# Model selection interface\nmodel_options = [\"yolo11n.pt\", \"yolo11s.pt\", \"yolo11m.pt\", \"yolo11l.pt\", \"yolo11x.pt\"]\nmodel_descriptions = {\n    \"yolo11n.pt\": \"Nano - Fastest, lowest accuracy\",\n    \"yolo11s.pt\": \"Small - Good balance\",\n    \"yolo11m.pt\": \"Medium - Recommended\",\n    \"yolo11l.pt\": \"Large - Higher accuracy\",\n    \"yolo11x.pt\": \"Extra Large - Highest accuracy\"\n}\n\nselected_model = st.sidebar.selectbox(\n    get_translation(\"model_selection\"),\n    model_options,\n    index=model_options.index(\"yolo11s.pt\"),\n    format_func=lambda x: f\"{model_descriptions[x]} ({x})\"\n)\n\n# Detection process with progress tracking\ndef detect_objects(image, model, confidence_threshold=0.5):\n    \"\"\"Perform object detection with progress tracking\"\"\"\n    with st.spinner(get_translation(\"processing\")):\n        results = model.predict(image, conf=confidence_threshold)\n\n        # Process results\n        detections = []\n        for result in results:\n            boxes = result.boxes\n            for box in boxes:\n                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n                conf = box.conf[0].cpu().numpy()\n                cls = int(box.cls[0].cpu().numpy())\n                class_name = model.names[cls]\n\n                detections.append({\n                    'class': class_name,\n                    'confidence': conf,\n                    'bbox': [x1, y1, x2, y2]\n                })\n\n    return detections, results\n\n\n\n\n\nFor cloud-based models, the app handles API authentication and request formatting:\n\n\nCode\ndef process_with_qwen(image, api_key):\n    \"\"\"Process image using Qwen-Image-Edit via DashScope\"\"\"\n    try:\n        response = MultiModalConversation.call(\n            model='qwen-image-edit',\n            input=[\n                {\n                    'role': 'user',\n                    'content': [\n                        {'image': f\"data:image/jpeg;base64,{image_b64}\"},\n                        {'text': 'Please identify and label all objects in this image.'}\n                    ]\n                }\n            ]\n        )\n        return response\n    except Exception as e:\n        st.error(f\"API Error: {str(e)}\")\n        return None"
  },
  {
    "objectID": "posts/yolo-app/index.html#user-interface-design",
    "href": "posts/yolo-app/index.html#user-interface-design",
    "title": "YOLO Object Detection App with Streamlit",
    "section": "",
    "text": "The application uses a professional three-column layout:\n\nSidebar: Model selection, confidence threshold, language settings\nMain Area: Input method selection, image/video display, results\nResults Panel: Detection statistics, download options\n\n\n\n\nThe translation system handles all UI elements:\n\n\nCode\ntranslations = {\n    \"en\": {\n        \"title\": \"YOLO11 Object Detection\",\n        \"upload_file\": \"Upload File\",\n        \"camera_input\": \"Use Camera\",\n        # ... more strings\n    },\n    \"zh\": {\n        \"title\": \"YOLO11 目标检测\",\n        \"upload_file\": \"上传文件\",\n        \"camera_input\": \"使用相机\",\n        # ... corresponding translations\n    }\n}"
  },
  {
    "objectID": "posts/yolo-app/index.html#performance-optimizations",
    "href": "posts/yolo-app/index.html#performance-optimizations",
    "title": "YOLO Object Detection App with Streamlit",
    "section": "",
    "text": "Model\nParameters\nmAP\nInference Time (CPU)\nInference Time (MPS)\n\n\n\n\nYOLO11n\n2.6M\n37.2\n15ms\n3ms\n\n\nYOLO11s\n9.4M\n45.5\n28ms\n6ms\n\n\nYOLO11m\n25.4M\n51.2\n52ms\n12ms\n\n\nYOLO11l\n43.7M\n53.4\n84ms\n18ms\n\n\nYOLO11x\n68.2M\n54.7\n126ms\n26ms\n\n\n\n\n\n\nThe app automatically detects and utilizes Metal Performance Shaders (MPS) on Apple Silicon devices:\n\n\nCode\ndevice = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\nmodel = YOLO(selected_model).to(device)\n\n# Performance monitoring\nimport time\nstart_time = time.time()\nresults = model.predict(image)\ninference_time = (time.time() - start_time) * 1000\n\nst.metric(f\"Inference Time ({device.upper()})\", f\"{inference_time:.1f}ms\")\n\n\n\n\n\nTo meet API size limits, the app implements smart image compression:\n\n\nCode\ndef compress_image_for_api(image, max_size=8*1024*1024):\n    \"\"\"Compress image to meet API requirements\"\"\"\n    for quality in [95, 85, 75, 65]:\n        for fmt in [\"JPEG\", \"WEBP\"]:\n            buffer = BytesIO()\n            image.save(buffer, format=fmt, quality=quality)\n            if buffer.tell() &lt;= max_size:\n                return buffer.getvalue()\n    return None"
  },
  {
    "objectID": "posts/yolo-app/index.html#deployment-and-production-features",
    "href": "posts/yolo-app/index.html#deployment-and-production-features",
    "title": "YOLO Object Detection App with Streamlit",
    "section": "",
    "text": "The application maintains comprehensive session state:\n\n\nCode\nsession_state_vars = [\n    \"current_image\", \"uploaded_image_bytes\",\n    \"current_video\", \"uploaded_video_bytes\",\n    \"camera_active\", \"camera_frame\",\n    \"qwen_processed\", \"gemini_processed\",\n    \"language\", \"input_method_index\"\n]\n\n\n\n\n\nRobust error handling ensures graceful degradation:\n\n\nCode\ntry:\n    result = model.predict(image, conf=confidence_threshold)\n    st.success(get_translation(\"detection_success\"))\nexcept Exception as e:\n    st.error(f\"Detection failed: {str(e)}\")\n    # Fallback to alternative processing method"
  },
  {
    "objectID": "posts/yolo-app/index.html#live-demo",
    "href": "posts/yolo-app/index.html#live-demo",
    "title": "YOLO Object Detection App with Streamlit",
    "section": "",
    "text": "The application is deployed and accessible at: https://yolo-live.streamlit.app/"
  },
  {
    "objectID": "posts/yolo-app/index.html#getting-started",
    "href": "posts/yolo-app/index.html#getting-started",
    "title": "YOLO Object Detection App with Streamlit",
    "section": "",
    "text": "Python 3.12 or higher\nModern package manager (uv recommended)\nFor cloud models: API keys from DashScope and OpenRouter\n\n\n\n\n# Clone the repository\ngit clone &lt;repository-url&gt;\ncd YOLO_app\n\n# Install dependencies with uv (recommended)\nuv sync\n\n# Alternative: pip install\npip install -r requirements.txt\n\n# Run the application\nstreamlit run app.py\n\n\n\nCreate a .env file with your API keys:\n# Alibaba Cloud DashScope API\nDASHSCOPE_API_KEY=your_dashscope_key\n\n# OpenRouter API (for Gemini)\nOPENROUTER_API_KEY=your_openrouter_key\n\n\n\n\n\n\nLaunch the application\nUpload an image or provide an image URL\nSelect your preferred YOLO11 model (yolo11s.pt recommended)\nAdjust confidence threshold if needed\nClick “Detect Objects”\nView results and download annotated image\n\n\n\n\n\nSelect “Use Camera” input method\nGrant camera permissions when prompted\nCapture a photo\nChoose detection model\nGet instant object detection results\n\n\n\n\n\nEnter your API keys in the sidebar\nUpload an image\nSelect “Qwen-Image-Edit” or “Gemini 2.5 Flash” model\nProcess image with advanced AI capabilities\nCompare results with local YOLO models"
  },
  {
    "objectID": "posts/yolo-app/index.html#future-enhancements",
    "href": "posts/yolo-app/index.html#future-enhancements",
    "title": "YOLO Object Detection App with Streamlit",
    "section": "",
    "text": "Potential improvements for future versions:\n\nAdditional Models: Integration with more cloud AI services\nReal-time Video Processing: Enhanced video streaming capabilities\nCustom Model Training: Allow users to train custom YOLO models\nMobile Optimization: PWA features for mobile device support\nBatch Processing: Process multiple images simultaneously"
  },
  {
    "objectID": "posts/yolo-app/index.html#conclusion",
    "href": "posts/yolo-app/index.html#conclusion",
    "title": "YOLO Object Detection App with Streamlit",
    "section": "",
    "text": "This YOLO object detection application demonstrates how to build a sophisticated, production-ready computer vision system. The combination of local and cloud-based models, bilingual support, and comprehensive error handling makes it suitable for both development and production environments.\nThe project showcases best practices in: - Modern Python development with dependency management - Streamlit web application architecture - Computer vision API integration - Internationalization and accessibility - Performance optimization for different hardware platforms\nWhether you’re interested in computer vision, web development, or AI applications, this project provides an excellent foundation for building advanced AI-powered web applications."
  },
  {
    "objectID": "posts/weight-tracking/index.html",
    "href": "posts/weight-tracking/index.html",
    "title": "Weight Tracking Dashboard with Shiny,streamlit and AI Integration",
    "section": "",
    "text": "The weight tracking application is a comprehensive R Shiny web application that helps users monitor their weight trends, calculate BMI, and receive personalized AI-powered health suggestions. What makes this project particularly interesting is its integration of multiple AI providers and real-time data synchronization capabilities.\nLive Demo: https://jcflyingco.shinyapps.io/weight_tracking/\nGithub: https://github.com/JCwinning/weight_tracking\n\nWeight Tracking trendAI feedback体重趋势图AI反馈\n\n\n\n\n\nApplication Dashboard - Weight Tracking View\n\n\n\n\n\n\n\nAI Configuration Panel\n\n\n\n\n\n\n\nData Table View\n\n\n\n\n\n\n\nCode Viewer Section\n\n\n\n\n\n\n\n\n\n\nWeight Tracking: Interactive logging and visualization of weight over time\nBMI Calculator: Automatic BMI calculation with health range comparisons\nUnit Conversion: Seamless switching between metric (kg/cm) and imperial (pounds/inches)\nReal-time Updates: Automatic data refresh when Excel files are modified\n\n\n\n\n\nMulti-provider AI Integration: Support for Modelscope, OpenRouter, Gemini, and OpenAI-compatible APIs\nComplete Internationalization: Full English/Chinese language support with 50+ translated strings\nReal-time File Monitoring: Automatic UI updates when data files change\nInteractive Visualizations: Plotly charts with zoom, pan, and hover capabilities\nData Management: Excel import/export with reactive data updates\n\n\n\n\n\n\n\n\n\n\n\n\n Weight Tracking Application Architecture \n\n\n\n\n\n\nPrimary Framework: R Shiny (Classic architecture)\nUI Components: Bootstrap-based responsive design with bslib\nData Processing: tidyverse, readxl, openxlsx\nVisualization: plotly for interactive charts\nAI Integration: ellmer for multi-provider AI support\n\n\n\n\nweight_tracking/\n├── ui.R              # Multi-tab interface with controls\n├── server.R          # Server logic and data management\n├── global.R          # URL bookmarking configuration\n├── ai_config.R       # Centralized AI provider management\n├── language.R        # Complete internationalization system\n├── weight.xlsx       # Primary data storage\n├── www/logo.png      # Application branding\n└── images/           # Application screenshots\n\n\n\n\nThe application implements a sophisticated data processing workflow that ensures real-time synchronization and efficient data handling.\n\n\n\n\n\n\n\n\n\n Data Processing Workflow \n\n\n\n\n\n\nThe application uses reactivePoll() to monitor the Excel data file for changes:\n\n\nCode\n# Real-time data monitoring with 1000ms interval\nweight_data &lt;- reactivePoll(\n  intervalMillis = 1000,\n  session = session,\n  checkFunc = function() {\n    # Check file modification time\n    if (file.exists(\"weight.xlsx\")) {\n      file.info(\"weight.xlsx\")$mtime\n    } else {\n      0\n    }\n  },\n  valueFunc = function() {\n    # Read and process the Excel file\n    if (file.exists(\"weight.xlsx\")) {\n      data &lt;- read_excel(\"weight.xlsx\") %&gt;%\n        mutate(\n          Date = anytime(Date),\n          BMI = case_when(\n            Unit == \"kg\" ~ Weight / (Height/100)^2,\n            Unit == \"pound\" ~ (Weight * 0.453592) / ((Height * 2.54)/100)^2\n          )\n        )\n      return(data)\n    }\n    return(data.frame())\n  }\n)\n\n\n\n\n\nThe application implements comprehensive BMI calculations with unit conversions:\n\n\n\n\n\nBMI Range\nCategory\nHealth Risk\nColor Code\n\n\n\n\n&lt; 18.5\nUnderweight\nModerate risk\nYellow\n\n\n18.5 - 24.9\nNormal weight\nMinimal risk\nGreen\n\n\n25.0 - 29.9\nOverweight\nIncreased risk\nOrange\n\n\n≥ 30.0\nObese\nHigh risk\nRed\n\n\n\n\n\n\n\n\nCode\n# BMI calculation for metric and imperial units\ncalculate_bmi &lt;- function(weight, height, unit) {\n  if (unit == \"kg\") {\n    # Metric calculation\n    bmi &lt;- weight / ((height/100)^2)\n  } else {\n    # Imperial calculation with conversion\n    weight_kg &lt;- weight * 0.453592  # pounds to kg\n    height_m &lt;- height * 2.54 / 100  # inches to meters\n    bmi &lt;- weight_kg / (height_m^2)\n  }\n\n  # Categorize BMI\n  category &lt;- case_when(\n    bmi &lt; 18.5 ~ \"underweight\",\n    bmi &lt; 25 ~ \"normal\",\n    bmi &lt; 30 ~ \"overweight\",\n    TRUE ~ \"obese\"\n  )\n\n  return(list(bmi = round(bmi, 1), category = category))\n}\n\n\n\n\n\n\n\n\n\nThe application supports multiple AI providers with dynamic switching:\n\n\nCode\n# AI Provider Configuration\nai_providers &lt;- list(\n  modelscope = list(\n    provider_url = \"https://api-inference.modelscope.cn/v1\",\n    models = c(\"zhipuAI/GLM-4.6\", \"Qwen/Qwen3-Next-80B-A3B-Instruct\")\n  ),\n  openrouter = list(\n    provider_url = \"https://openrouter.ai/api/v1\",\n    models = c(\"openai/gpt-oss-120b:exacto\", \"minimax/minimax-m2:free\")\n  ),\n  Gemini = list(\n    provider_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n    models = c(\"gemini-2.5-flash\", \"gemini-2.5-pro\")\n  )\n)\n\n# Dynamic provider selection\nobserveEvent(input$ai_provider, {\n  set_current_provider(input$ai_provider)\n  updateSelectInput(session, \"ai_model\",\n                    choices = get_provider_models(input$ai_provider))\n})\n\n\n\n\n\nThe AI generates personalized health advice based on weight trends:\n\n\nCode\n# AI suggestion generation\nget_ai_suggestion &lt;- function(weight_data, api_key, model, provider) {\n  # Prepare data for AI analysis\n  recent_trend &lt;- analyze_weight_trend(weight_data)\n  current_bmi &lt;- get_current_bmi(weight_data)\n\n  # Create language-specific prompts\n  prompt &lt;- if (current_language == \"zh\") {\n    paste(\"基于以下体重数据分析，请提供个性化的健康建议：\",\n          \"最近体重趋势：\", recent_trend,\n          \"当前BMI：\", current_bmi,\n          \"请用中文回复，包含具体的饮食和运动建议。\")\n  } else {\n    paste(\"Based on the following weight data analysis, please provide personalized health advice:\",\n          \"Recent weight trend:\", recent_trend,\n          \"Current BMI:\", current_bmi,\n          \"Please respond in English with specific diet and exercise recommendations.\")\n  }\n\n  # Make API call to selected provider\n  response &lt;- ellmer::chat_completion(\n    model = model,\n    messages = list(\n      list(role = \"user\", content = prompt)\n    ),\n    api_key = api_key,\n    base_url = get_provider_url(provider)\n  )\n\n  return(response$choices[[1]]$message$content)\n}\n\n\n\n\n\n\n\n\n\n\nCode\n# Tab-based interface organization\nmainPanel(\n  tabsetPanel(\n    tabPanel(get_text(\"plot_tab\"),\n             # Weight and BMI charts\n             fluidRow(\n               column(6, plotlyOutput(\"weight_plot\")),\n               column(6, plotlyOutput(\"bmi_plot\"))\n             )\n    ),\n    tabPanel(get_text(\"ui_tab\"),\n             # UI code viewer with syntax highlighting\n             verbatimTextOutput(\"ui_code\")\n    ),\n    tabPanel(get_text(\"server_tab\"),\n             # Server code viewer\n             verbatimTextOutput(\"server_code\")\n    ),\n    tabPanel(get_text(\"data_tab\"),\n             # Interactive data table\n             DT::dataTableOutput(\"data_table\")\n    )\n  )\n)\n\n\n\n\n\nComplete bilingual support with dynamic language switching:\n\n\nCode\n# Language translation system\ntranslations &lt;- list(\n  en = list(\n    app_title = \"Weight tracking\",\n    your_weight = \"Your weight:\",\n    your_bmi = \"Your BMI:\",\n    get_ai_suggestion = \"Get AI Suggestion\"\n    # ... more translations\n  ),\n  zh = list(\n    app_title = \"体重追踪\",\n    your_weight = \"您的体重:\",\n    your_bmi = \"您的BMI:\",\n    get_ai_suggestion = \"获取AI建议\"\n    # ... more translations\n  )\n)\n\n# Language switching handlers\nobserveEvent(input$lang_en, {\n  current_lang(\"en\")\n  set_language(\"en\")\n  updateUI()  # Trigger UI update\n})\n\n\n\n\n\n\n\n\nThe application features dynamic charts with health indicators:\n\n\nCode\n# BMI chart with health ranges\noutput$bmi_plot &lt;- renderPlotly({\n  data &lt;- weight_data()\n\n  plot_ly(data, x = ~Date, y = ~BMI, type = 'scatter', mode = 'lines+markers',\n          name = get_text(\"chart_bmi_legend\"),\n          line = list(color = 'blue', width = 3),\n          marker = list(size = 8)) %&gt;%\n    # Add health range bands\n    add_trace(y = rep(18.5, nrow(data)), mode = 'lines',\n              line = list(color = 'green', dash = 'dash'), name = \"Good Range Start\") %&gt;%\n    add_trace(y = rep(24.9, nrow(data)), mode = 'lines',\n              line = list(color = 'red', dash = 'dash'), name = \"Good Range End\") %&gt;%\n    layout(\n      title = get_text(\"chart_bmi_title\"),\n      xaxis = list(title = \"Date\"),\n      yaxis = list(title = \"BMI\", range = c(15, 35)),\n      hovermode = 'x unified'\n    )\n})\n\n\n\n\n\n\n\n\nThe application is deployed and accessible at: https://jcflyingco.shinyapps.io/weight-tracking/\n\n\n\n\n\nCode\n# Enable URL bookmarking in global.R\nshinyServer(\n  function(input, output, session) {\n    # Bookmarking configuration\n    enableBookmarking(\"url\")\n\n    # Save/restore UI state\n    setBookmarkExclude(c(\"lang_en\", \"lang_zh\"))  # Exclude language buttons\n  }\n)\n\n\n\n\n\n\n\n\nThe application implements efficient reactive programming:\n\n\nCode\n# Efficient data processing with caching\nprocessed_data &lt;- reactive({\n  data &lt;- weight_data()\n  if (nrow(data) == 0) return(NULL)\n\n  # Calculate derived metrics once\n  data %&gt;%\n    mutate(\n      Weight_Change = c(NA, diff(Weight)),\n      BMI_Category = case_when(\n        BMI &lt; 18.5 ~ \"underweight\",\n        BMI &lt; 25 ~ \"normal\",\n        BMI &lt; 30 ~ \"overweight\",\n        TRUE ~ \"obese\"\n      ),\n      Date_Formatted = format(Date, \"%Y-%m-%d\")\n    )\n})\n\n# Shared calculation for multiple outputs\ncurrent_stats &lt;- reactive({\n  data &lt;- processed_data()\n  if (is.null(data) || nrow(data) == 0) return(NULL)\n\n  list(\n    last_weight = tail(data$Weight, 1),\n    last_bmi = tail(data$BMI, 1),\n    trend = calculate_trend(data$Weight),\n    days_tracked = nrow(data)\n  )\n})\n\n\n\n\n\n\n\n\n\n\nCode\n# Secure API key handling (session-only storage)\nobserveEvent(input$get_ai_suggestion, {\n  if (is.null(input$api_key) || input$api_key == \"\") {\n    showNotification(get_text(\"please_provide_api_key\"), type = \"error\")\n    return()\n  }\n\n  # Use API key from session (no persistence)\n  withBusyIndicator(\"Getting AI suggestion...\", {\n    suggestion &lt;- get_ai_suggestion(\n      weight_data = weight_data(),\n      api_key = input$api_key,\n      model = input$ai_model,\n      provider = input$ai_provider\n    )\n\n    output$ai_response &lt;- renderUI({\n      div(class = \"markdown-content\",\n          HTML(markdown::renderMarkdown(suggestion))\n      )\n    })\n  })\n})\n\n\n\n\n\n\n\nCode\n# Comprehensive error handling\nget_ai_suggestion &lt;- function(...) {\n  tryCatch({\n    # API call logic\n    response &lt;- ellmer::chat_completion(...)\n    return(response$choices[[1]]$message$content)\n  }, error = function(e) {\n    # Error logging and user feedback\n    log_error(paste(\"AI API Error:\", e$message))\n    return(get_text(\"ai_error_check_config\"))\n  })\n}\n\n\n\n\n\n\n\n\n\nOpen the application in browser\nView current weight trends and BMI charts\nAdd new weight data via Excel upload\nGet AI-powered health recommendations\nDownload updated data for offline use\n\n\n\n\n\nSwitch between EN/中文 using top-right buttons\nAll UI elements update dynamically\nAI prompts adapt to selected language\nCharts and data maintain context\n\n\n\n\n\n\n\n\nReal-time Excel Synchronization: Automatic UI updates when data changes\nMulti-provider AI Integration: Flexible AI provider switching\nComplete Internationalization: Full bilingual support\nProfessional UI Design: Bootstrap-based responsive layout\nInteractive Visualizations: Zoomable, hoverable Plotly charts\n\n\n\n\n\nData Refresh: 1-second polling interval\nChart Rendering: &lt;100ms for typical datasets\nAPI Response: 2-5 seconds for AI suggestions\nMemory Usage: &lt;50MB for 1000+ records\n\n\n\n\n\nPotential improvements for next versions:\n\nDatabase Integration: Replace Excel with SQLite/PostgreSQL\nUser Authentication: Multi-user support with personal data isolation\nMobile Optimization: PWA features for mobile devices\nAdvanced Analytics: Weight prediction models using time series\nIntegration APIs: Connect with fitness trackers and health apps\n\n\n\n\nThis weight tracking application demonstrates the power of combining R Shiny’s reactive programming with modern AI technologies. The project showcases:\n\nSophisticated Data Management: Real-time file monitoring and reactive updates\nAI Integration: Multi-provider support with personalized health recommendations\nInternationalization: Complete bilingual implementation\nProfessional UI/UX: Modern responsive design with interactive visualizations\n\nWhether you’re interested in health monitoring, data visualization, or AI integration, this project provides an excellent example of building production-grade web applications with R Shiny."
  },
  {
    "objectID": "posts/weight-tracking/index.html#core-features",
    "href": "posts/weight-tracking/index.html#core-features",
    "title": "Weight Tracking Dashboard with Shiny,streamlit and AI Integration",
    "section": "",
    "text": "Weight Tracking: Interactive logging and visualization of weight over time\nBMI Calculator: Automatic BMI calculation with health range comparisons\nUnit Conversion: Seamless switching between metric (kg/cm) and imperial (pounds/inches)\nReal-time Updates: Automatic data refresh when Excel files are modified\n\n\n\n\n\nMulti-provider AI Integration: Support for Modelscope, OpenRouter, Gemini, and OpenAI-compatible APIs\nComplete Internationalization: Full English/Chinese language support with 50+ translated strings\nReal-time File Monitoring: Automatic UI updates when data files change\nInteractive Visualizations: Plotly charts with zoom, pan, and hover capabilities\nData Management: Excel import/export with reactive data updates"
  },
  {
    "objectID": "posts/weight-tracking/index.html#technical-architecture",
    "href": "posts/weight-tracking/index.html#technical-architecture",
    "title": "Weight Tracking Dashboard with Shiny,streamlit and AI Integration",
    "section": "",
    "text": "Weight Tracking Application Architecture \n\n\n\n\n\n\nPrimary Framework: R Shiny (Classic architecture)\nUI Components: Bootstrap-based responsive design with bslib\nData Processing: tidyverse, readxl, openxlsx\nVisualization: plotly for interactive charts\nAI Integration: ellmer for multi-provider AI support\n\n\n\n\nweight_tracking/\n├── ui.R              # Multi-tab interface with controls\n├── server.R          # Server logic and data management\n├── global.R          # URL bookmarking configuration\n├── ai_config.R       # Centralized AI provider management\n├── language.R        # Complete internationalization system\n├── weight.xlsx       # Primary data storage\n├── www/logo.png      # Application branding\n└── images/           # Application screenshots"
  },
  {
    "objectID": "posts/weight-tracking/index.html#data-management-system",
    "href": "posts/weight-tracking/index.html#data-management-system",
    "title": "Weight Tracking Dashboard with Shiny,streamlit and AI Integration",
    "section": "",
    "text": "The application implements a sophisticated data processing workflow that ensures real-time synchronization and efficient data handling.\n\n\n\n\n\n\n\n\n\n Data Processing Workflow \n\n\n\n\n\n\nThe application uses reactivePoll() to monitor the Excel data file for changes:\n\n\nCode\n# Real-time data monitoring with 1000ms interval\nweight_data &lt;- reactivePoll(\n  intervalMillis = 1000,\n  session = session,\n  checkFunc = function() {\n    # Check file modification time\n    if (file.exists(\"weight.xlsx\")) {\n      file.info(\"weight.xlsx\")$mtime\n    } else {\n      0\n    }\n  },\n  valueFunc = function() {\n    # Read and process the Excel file\n    if (file.exists(\"weight.xlsx\")) {\n      data &lt;- read_excel(\"weight.xlsx\") %&gt;%\n        mutate(\n          Date = anytime(Date),\n          BMI = case_when(\n            Unit == \"kg\" ~ Weight / (Height/100)^2,\n            Unit == \"pound\" ~ (Weight * 0.453592) / ((Height * 2.54)/100)^2\n          )\n        )\n      return(data)\n    }\n    return(data.frame())\n  }\n)\n\n\n\n\n\nThe application implements comprehensive BMI calculations with unit conversions:\n\n\n\n\n\nBMI Range\nCategory\nHealth Risk\nColor Code\n\n\n\n\n&lt; 18.5\nUnderweight\nModerate risk\nYellow\n\n\n18.5 - 24.9\nNormal weight\nMinimal risk\nGreen\n\n\n25.0 - 29.9\nOverweight\nIncreased risk\nOrange\n\n\n≥ 30.0\nObese\nHigh risk\nRed\n\n\n\n\n\n\n\n\nCode\n# BMI calculation for metric and imperial units\ncalculate_bmi &lt;- function(weight, height, unit) {\n  if (unit == \"kg\") {\n    # Metric calculation\n    bmi &lt;- weight / ((height/100)^2)\n  } else {\n    # Imperial calculation with conversion\n    weight_kg &lt;- weight * 0.453592  # pounds to kg\n    height_m &lt;- height * 2.54 / 100  # inches to meters\n    bmi &lt;- weight_kg / (height_m^2)\n  }\n\n  # Categorize BMI\n  category &lt;- case_when(\n    bmi &lt; 18.5 ~ \"underweight\",\n    bmi &lt; 25 ~ \"normal\",\n    bmi &lt; 30 ~ \"overweight\",\n    TRUE ~ \"obese\"\n  )\n\n  return(list(bmi = round(bmi, 1), category = category))\n}"
  },
  {
    "objectID": "posts/weight-tracking/index.html#ai-integration-architecture",
    "href": "posts/weight-tracking/index.html#ai-integration-architecture",
    "title": "Weight Tracking Dashboard with Shiny,streamlit and AI Integration",
    "section": "",
    "text": "The application supports multiple AI providers with dynamic switching:\n\n\nCode\n# AI Provider Configuration\nai_providers &lt;- list(\n  modelscope = list(\n    provider_url = \"https://api-inference.modelscope.cn/v1\",\n    models = c(\"zhipuAI/GLM-4.6\", \"Qwen/Qwen3-Next-80B-A3B-Instruct\")\n  ),\n  openrouter = list(\n    provider_url = \"https://openrouter.ai/api/v1\",\n    models = c(\"openai/gpt-oss-120b:exacto\", \"minimax/minimax-m2:free\")\n  ),\n  Gemini = list(\n    provider_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n    models = c(\"gemini-2.5-flash\", \"gemini-2.5-pro\")\n  )\n)\n\n# Dynamic provider selection\nobserveEvent(input$ai_provider, {\n  set_current_provider(input$ai_provider)\n  updateSelectInput(session, \"ai_model\",\n                    choices = get_provider_models(input$ai_provider))\n})\n\n\n\n\n\nThe AI generates personalized health advice based on weight trends:\n\n\nCode\n# AI suggestion generation\nget_ai_suggestion &lt;- function(weight_data, api_key, model, provider) {\n  # Prepare data for AI analysis\n  recent_trend &lt;- analyze_weight_trend(weight_data)\n  current_bmi &lt;- get_current_bmi(weight_data)\n\n  # Create language-specific prompts\n  prompt &lt;- if (current_language == \"zh\") {\n    paste(\"基于以下体重数据分析，请提供个性化的健康建议：\",\n          \"最近体重趋势：\", recent_trend,\n          \"当前BMI：\", current_bmi,\n          \"请用中文回复，包含具体的饮食和运动建议。\")\n  } else {\n    paste(\"Based on the following weight data analysis, please provide personalized health advice:\",\n          \"Recent weight trend:\", recent_trend,\n          \"Current BMI:\", current_bmi,\n          \"Please respond in English with specific diet and exercise recommendations.\")\n  }\n\n  # Make API call to selected provider\n  response &lt;- ellmer::chat_completion(\n    model = model,\n    messages = list(\n      list(role = \"user\", content = prompt)\n    ),\n    api_key = api_key,\n    base_url = get_provider_url(provider)\n  )\n\n  return(response$choices[[1]]$message$content)\n}"
  },
  {
    "objectID": "posts/weight-tracking/index.html#user-interface-design",
    "href": "posts/weight-tracking/index.html#user-interface-design",
    "title": "Weight Tracking Dashboard with Shiny,streamlit and AI Integration",
    "section": "",
    "text": "Code\n# Tab-based interface organization\nmainPanel(\n  tabsetPanel(\n    tabPanel(get_text(\"plot_tab\"),\n             # Weight and BMI charts\n             fluidRow(\n               column(6, plotlyOutput(\"weight_plot\")),\n               column(6, plotlyOutput(\"bmi_plot\"))\n             )\n    ),\n    tabPanel(get_text(\"ui_tab\"),\n             # UI code viewer with syntax highlighting\n             verbatimTextOutput(\"ui_code\")\n    ),\n    tabPanel(get_text(\"server_tab\"),\n             # Server code viewer\n             verbatimTextOutput(\"server_code\")\n    ),\n    tabPanel(get_text(\"data_tab\"),\n             # Interactive data table\n             DT::dataTableOutput(\"data_table\")\n    )\n  )\n)\n\n\n\n\n\nComplete bilingual support with dynamic language switching:\n\n\nCode\n# Language translation system\ntranslations &lt;- list(\n  en = list(\n    app_title = \"Weight tracking\",\n    your_weight = \"Your weight:\",\n    your_bmi = \"Your BMI:\",\n    get_ai_suggestion = \"Get AI Suggestion\"\n    # ... more translations\n  ),\n  zh = list(\n    app_title = \"体重追踪\",\n    your_weight = \"您的体重:\",\n    your_bmi = \"您的BMI:\",\n    get_ai_suggestion = \"获取AI建议\"\n    # ... more translations\n  )\n)\n\n# Language switching handlers\nobserveEvent(input$lang_en, {\n  current_lang(\"en\")\n  set_language(\"en\")\n  updateUI()  # Trigger UI update\n})"
  },
  {
    "objectID": "posts/weight-tracking/index.html#data-visualization",
    "href": "posts/weight-tracking/index.html#data-visualization",
    "title": "Weight Tracking Dashboard with Shiny,streamlit and AI Integration",
    "section": "",
    "text": "The application features dynamic charts with health indicators:\n\n\nCode\n# BMI chart with health ranges\noutput$bmi_plot &lt;- renderPlotly({\n  data &lt;- weight_data()\n\n  plot_ly(data, x = ~Date, y = ~BMI, type = 'scatter', mode = 'lines+markers',\n          name = get_text(\"chart_bmi_legend\"),\n          line = list(color = 'blue', width = 3),\n          marker = list(size = 8)) %&gt;%\n    # Add health range bands\n    add_trace(y = rep(18.5, nrow(data)), mode = 'lines',\n              line = list(color = 'green', dash = 'dash'), name = \"Good Range Start\") %&gt;%\n    add_trace(y = rep(24.9, nrow(data)), mode = 'lines',\n              line = list(color = 'red', dash = 'dash'), name = \"Good Range End\") %&gt;%\n    layout(\n      title = get_text(\"chart_bmi_title\"),\n      xaxis = list(title = \"Date\"),\n      yaxis = list(title = \"BMI\", range = c(15, 35)),\n      hovermode = 'x unified'\n    )\n})"
  },
  {
    "objectID": "posts/weight-tracking/index.html#deployment-and-configuration",
    "href": "posts/weight-tracking/index.html#deployment-and-configuration",
    "title": "Weight Tracking Dashboard with Shiny,streamlit and AI Integration",
    "section": "",
    "text": "The application is deployed and accessible at: https://jcflyingco.shinyapps.io/weight-tracking/\n\n\n\n\n\nCode\n# Enable URL bookmarking in global.R\nshinyServer(\n  function(input, output, session) {\n    # Bookmarking configuration\n    enableBookmarking(\"url\")\n\n    # Save/restore UI state\n    setBookmarkExclude(c(\"lang_en\", \"lang_zh\"))  # Exclude language buttons\n  }\n)"
  },
  {
    "objectID": "posts/weight-tracking/index.html#performance-optimizations",
    "href": "posts/weight-tracking/index.html#performance-optimizations",
    "title": "Weight Tracking Dashboard with Shiny,streamlit and AI Integration",
    "section": "",
    "text": "The application implements efficient reactive programming:\n\n\nCode\n# Efficient data processing with caching\nprocessed_data &lt;- reactive({\n  data &lt;- weight_data()\n  if (nrow(data) == 0) return(NULL)\n\n  # Calculate derived metrics once\n  data %&gt;%\n    mutate(\n      Weight_Change = c(NA, diff(Weight)),\n      BMI_Category = case_when(\n        BMI &lt; 18.5 ~ \"underweight\",\n        BMI &lt; 25 ~ \"normal\",\n        BMI &lt; 30 ~ \"overweight\",\n        TRUE ~ \"obese\"\n      ),\n      Date_Formatted = format(Date, \"%Y-%m-%d\")\n    )\n})\n\n# Shared calculation for multiple outputs\ncurrent_stats &lt;- reactive({\n  data &lt;- processed_data()\n  if (is.null(data) || nrow(data) == 0) return(NULL)\n\n  list(\n    last_weight = tail(data$Weight, 1),\n    last_bmi = tail(data$BMI, 1),\n    trend = calculate_trend(data$Weight),\n    days_tracked = nrow(data)\n  )\n})"
  },
  {
    "objectID": "posts/weight-tracking/index.html#security-and-best-practices",
    "href": "posts/weight-tracking/index.html#security-and-best-practices",
    "title": "Weight Tracking Dashboard with Shiny,streamlit and AI Integration",
    "section": "",
    "text": "Code\n# Secure API key handling (session-only storage)\nobserveEvent(input$get_ai_suggestion, {\n  if (is.null(input$api_key) || input$api_key == \"\") {\n    showNotification(get_text(\"please_provide_api_key\"), type = \"error\")\n    return()\n  }\n\n  # Use API key from session (no persistence)\n  withBusyIndicator(\"Getting AI suggestion...\", {\n    suggestion &lt;- get_ai_suggestion(\n      weight_data = weight_data(),\n      api_key = input$api_key,\n      model = input$ai_model,\n      provider = input$ai_provider\n    )\n\n    output$ai_response &lt;- renderUI({\n      div(class = \"markdown-content\",\n          HTML(markdown::renderMarkdown(suggestion))\n      )\n    })\n  })\n})\n\n\n\n\n\n\n\nCode\n# Comprehensive error handling\nget_ai_suggestion &lt;- function(...) {\n  tryCatch({\n    # API call logic\n    response &lt;- ellmer::chat_completion(...)\n    return(response$choices[[1]]$message$content)\n  }, error = function(e) {\n    # Error logging and user feedback\n    log_error(paste(\"AI API Error:\", e$message))\n    return(get_text(\"ai_error_check_config\"))\n  })\n}"
  },
  {
    "objectID": "posts/weight-tracking/index.html#usage-examples-and-user-workflows",
    "href": "posts/weight-tracking/index.html#usage-examples-and-user-workflows",
    "title": "Weight Tracking Dashboard with Shiny,streamlit and AI Integration",
    "section": "",
    "text": "Open the application in browser\nView current weight trends and BMI charts\nAdd new weight data via Excel upload\nGet AI-powered health recommendations\nDownload updated data for offline use\n\n\n\n\n\nSwitch between EN/中文 using top-right buttons\nAll UI elements update dynamically\nAI prompts adapt to selected language\nCharts and data maintain context"
  },
  {
    "objectID": "posts/weight-tracking/index.html#technical-achievements",
    "href": "posts/weight-tracking/index.html#technical-achievements",
    "title": "Weight Tracking Dashboard with Shiny,streamlit and AI Integration",
    "section": "",
    "text": "Real-time Excel Synchronization: Automatic UI updates when data changes\nMulti-provider AI Integration: Flexible AI provider switching\nComplete Internationalization: Full bilingual support\nProfessional UI Design: Bootstrap-based responsive layout\nInteractive Visualizations: Zoomable, hoverable Plotly charts\n\n\n\n\n\nData Refresh: 1-second polling interval\nChart Rendering: &lt;100ms for typical datasets\nAPI Response: 2-5 seconds for AI suggestions\nMemory Usage: &lt;50MB for 1000+ records"
  },
  {
    "objectID": "posts/weight-tracking/index.html#future-enhancements",
    "href": "posts/weight-tracking/index.html#future-enhancements",
    "title": "Weight Tracking Dashboard with Shiny,streamlit and AI Integration",
    "section": "",
    "text": "Potential improvements for next versions:\n\nDatabase Integration: Replace Excel with SQLite/PostgreSQL\nUser Authentication: Multi-user support with personal data isolation\nMobile Optimization: PWA features for mobile devices\nAdvanced Analytics: Weight prediction models using time series\nIntegration APIs: Connect with fitness trackers and health apps"
  },
  {
    "objectID": "posts/weight-tracking/index.html#conclusion",
    "href": "posts/weight-tracking/index.html#conclusion",
    "title": "Weight Tracking Dashboard with Shiny,streamlit and AI Integration",
    "section": "",
    "text": "This weight tracking application demonstrates the power of combining R Shiny’s reactive programming with modern AI technologies. The project showcases:\n\nSophisticated Data Management: Real-time file monitoring and reactive updates\nAI Integration: Multi-provider support with personalized health recommendations\nInternationalization: Complete bilingual implementation\nProfessional UI/UX: Modern responsive design with interactive visualizations\n\nWhether you’re interested in health monitoring, data visualization, or AI integration, this project provides an excellent example of building production-grade web applications with R Shiny."
  },
  {
    "objectID": "posts/shop-map-manager/index.html",
    "href": "posts/shop-map-manager/index.html",
    "title": "Shop Map Manager (店铺地图管理系统)",
    "section": "",
    "text": "A robust and elegant personal shop management system. Whether you’re tracking your favorite coffee shops, scenic spots, or planning future visits, this application provides a powerful platform for visualization and data management.\nLive Demo: https://china-map.streamlit.app"
  },
  {
    "objectID": "posts/shop-map-manager/index.html#live-demo",
    "href": "posts/shop-map-manager/index.html#live-demo",
    "title": "Shop Map Manager (店铺地图管理系统)",
    "section": "Live Demo",
    "text": "Live Demo\n\n\n\nLive App"
  },
  {
    "objectID": "posts/shop-map-manager/index.html#key-architectural-shift-dual-mode-storage",
    "href": "posts/shop-map-manager/index.html#key-architectural-shift-dual-mode-storage",
    "title": "Shop Map Manager (店铺地图管理系统)",
    "section": "Key Architectural Shift: Dual-Mode Storage",
    "text": "Key Architectural Shift: Dual-Mode Storage\nThe most significant update to the project is the introduction of Dual Data Storage Modes, allowing for both privacy and portability:\n\nLocal Mode: Data is stored securely in a local shops.csv file. Perfect for quick use without any setup.\nCloud Mode: Leverages Supabase for real-time synchronization across devices. Your journeys follow you wherever you go."
  },
  {
    "objectID": "posts/shop-map-manager/index.html#features",
    "href": "posts/shop-map-manager/index.html#features",
    "title": "Shop Map Manager (店铺地图管理系统)",
    "section": "Features",
    "text": "Features\n\n🔍 Infinite Discovery: Seamlessly integrate with the Gaode (Amap) Map API. Search for any location and add it to your collection with a single click.\n🎨 Dynamic Visualization: Interactive maps powered by Folium, featuring:\n\nCustom category-specific icons (Coffee, Dining, Scenery, etc.).\nIntuitive color coding: Red for “Visited”, Green for “Want to Visit”.\n\n📸 Visual Memories: (Cloud Mode) Capture and upload multiple photos for each location, hosted securely on Supabase Storage.\n⭐ Deep Insights: Track more than just locations. Store star ratings (1-5), personal notes, and visit timestamps.\n⚡ Live Filtering: Instantly filter your view by journey type or visit status to focus on what matters."
  },
  {
    "objectID": "posts/shop-map-manager/index.html#tech-stack",
    "href": "posts/shop-map-manager/index.html#tech-stack",
    "title": "Shop Map Manager (店铺地图管理系统)",
    "section": "Tech Stack",
    "text": "Tech Stack\n\n\n\nComponent\nTechnology\n\n\n\n\nFrontend\nStreamlit\n\n\nMapping\nFolium\n\n\nDatabase/Auth\nSupabase\n\n\nAPI\nGaode Maps API\n\n\nData Logic\nPython & Pandas"
  },
  {
    "objectID": "posts/shop-map-manager/index.html#getting-started",
    "href": "posts/shop-map-manager/index.html#getting-started",
    "title": "Shop Map Manager (店铺地图管理系统)",
    "section": "Getting Started",
    "text": "Getting Started\n\n1. Installation\ngit clone https://github.com/JCwinning/map_app.git\ncd map_app\npip install -r requirements.txt\n\n\n2. Environment Configuration\nCreate a .env file in the root directory:\n# Essential\nGAODE_API_KEY=your_gaode_key\n\n# Optional: For Cloud Mode\nSUPABASE_URL=your_supabase_url\nSUPABASE_KEY=your_supabase_anon_key\n\n\n3. Launch\nstreamlit run app.py"
  },
  {
    "objectID": "posts/shop-map-manager/index.html#data-mastery",
    "href": "posts/shop-map-manager/index.html#data-mastery",
    "title": "Shop Map Manager (店铺地图管理系统)",
    "section": "Data Mastery",
    "text": "Data Mastery\nThe application handles complex data structures gracefully, whether stored in a flat CSV or a relational Supabase table:\n\n\n\nColumn\nDescription\n\n\n\n\nshop_name\nName of the location\n\n\nvisit_status\nCurrent status (Visited / Want to Visit)\n\n\nrating\nYour personal star rating (1-5)\n\n\nshop_type\nCategory (e.g., Coffee, Retail, Scenery)\n\n\nimages\n(Cloud Mode) Array of hosted image URLs"
  },
  {
    "objectID": "posts/shop-map-manager/index.html#conclusion",
    "href": "posts/shop-map-manager/index.html#conclusion",
    "title": "Shop Map Manager (店铺地图管理系统)",
    "section": "Conclusion",
    "text": "Conclusion\nThe Shop Map Manager has evolved from a simple CSV visualizer into a comprehensive personal travel and discovery companion. By bridging the gap between local simplicity and cloud power, it offers the ultimate flexibility for modern explorers."
  },
  {
    "objectID": "posts/gdp-trend/index.html",
    "href": "posts/gdp-trend/index.html",
    "title": "Interactive GDP Trend Dashboard with AI SQL",
    "section": "",
    "text": "Live Demo: https://world-GDP-trend.streamlit.app\nGithub: https://github.com/JCwinning/GDP_trend\n\n\n\nGDP Dashboard - Main Interface\n\n\nThe GDP Trend Dashboard is a sophisticated web application that visualizes and analyzes economic data from the World Bank API. What makes this system unique is its dual approach: traditional interactive visualizations combined with AI-powered natural language querying, making economic data accessible to both technical analysts and general users.\n\n\n\n\n\nPrimary Framework: Streamlit for interactive web applications\nData Processing: Pandas for manipulation and analysis\nVisualization: Plotly Express for interactive charts\nDatabase: DuckDB for efficient SQL queries\nAI Integration: ModelScope GLM-4.6 for natural language to SQL conversion\nData Sources: World Bank API via wbgapi library\n\n\n\n\n\nMulti-country GDP trend comparisons\nSupport for multiple economic indicators (GDP, GDP per capita, population, YoY growth)\nAI-powered natural language querying\nDirect SQL interface with DuckDB\nComplete bilingual support (English/Chinese)\nTime-range filtering with interactive controls (2000-present)\n\n\n\n\n\nThe application implements a sophisticated data pipeline that ensures data quality and real-time availability:\n\n\n\n\n\nflowchart TD\n    A[World Bank API] --&gt; B[Data Collection&lt;br/&gt;download_data.py]\n    B --&gt; C[Data Processing&lt;br/&gt;Country Mapping, Indicator Calculation]\n    C --&gt; D[Data Storage&lt;br/&gt;CSV Files]\n\n    D --&gt; E[Streamlit App&lt;br/&gt;app.py]\n    E --&gt; F[DuckDB&lt;br/&gt;In-Memory SQL]\n\n    F --&gt; G[Visualization Engine&lt;br/&gt;Plotly Express]\n    F --&gt; H[AI Query Engine&lt;br/&gt;ModelScope API]\n    F --&gt; I[Session Management&lt;br/&gt;User State]\n\n    G --&gt; J[Interactive Charts]\n    H --&gt; K[Natural Language&lt;br/&gt;to SQL]\n    I --&gt; L[Persistent&lt;br/&gt;Query Results]\n\n    J --&gt; M[User Interface]\n    K --&gt; M\n    L --&gt; M\n\n\n GDP Application Architecture \n\n\n\n\n\n\n\n\nThe application uses the World Bank API to collect comprehensive economic data:\n\n\nCode\nimport wbgapi as wb\nimport pycountry\nimport pandas as pd\n\ndef download_economic_data():\n    \"\"\"Download GDP data from World Bank API for all countries\"\"\"\n\n    # Define economic indicators to download\n    indicators = {\n        'gdp_current_usd': 'NY.GDP.MKTP.CD',\n        'gdp_per_capita_current_usd': 'NY.GDP.PCAP.CD',\n        'population_total': 'SP.POP.TOTL'\n    }\n\n    # Download data from 2000 to present\n    data_frames = []\n    for indicator_name, indicator_code in indicators.items():\n        df = wb.get_series(\n            series=indicator_code,\n            economy='all',\n            time='2000:2024',\n            simplify_index=True\n        )\n\n        # Process and add to collection\n        df = df.reset_index()\n        df['indicator'] = indicator_name\n        data_frames.append(df)\n\n    # Combine all indicators\n    combined_df = pd.concat(data_frames, ignore_index=True)\n    return combined_df\n\ndef create_country_reference_table():\n    \"\"\"Create comprehensive country metadata with ISO codes\"\"\"\n    countries = list(pycountry.countries)\n\n    df_all = pd.DataFrame([{\n        'country_name': country.name,\n        'country_code_2': country.alpha_2,\n        'country_code_3': country.alpha_3\n    } for country in countries])\n\n    # Add continent mapping\n    iso_to_continent = {\n        \"US\": \"North America\", \"CN\": \"Asia\", \"JP\": \"Asia\",\n        \"DE\": \"Europe\", \"GB\": \"Europe\", \"FR\": \"Europe\"\n        # ... complete mapping for all countries\n    }\n\n    df_all['continent'] = df_all['country_code_2'].map(iso_to_continent)\n    return df_all\n\n\n\n\n\nThe application uses a clean, normalized data structure:\n\n\nCode\n-- Main data schema\nCREATE TABLE df_gdp (\n    country_name TEXT,        -- Display name (English)\n    country_code_2 TEXT,      -- ISO alpha-2 code\n    country_code_3 TEXT,      -- ISO alpha-3 code\n    continent TEXT,            -- Continent classification\n    year INTEGER,              -- Data year\n    indicator TEXT,            -- Economic indicator name\n    value REAL                -- Indicator value\n);\n\n-- Example data\nINSERT INTO df_gdp VALUES\n('United States', 'US', 'USA', 'North America', 2023, 'gdp_current_usd', 27444144.3),\n('United States', 'US', 'USA', 'North America', 2023, 'gdp_per_capita_current_usd', 81254.2),\n('United States', 'US', 'USA', 'North America', 2023, 'population_total', 334914895.0);\n\n\n\n\n\n\n\n\nThe application uses Plotly Express for creating interactive charts with consistent color coding:\n\n\nCode\nimport plotly.express as px\n\n@st.cache_data\ndef load_data():\n    \"\"\"Load and cache GDP data\"\"\"\n    return pd.read_csv('data/gdp_data_2000_present.csv')\n\ndef create_gdp_trend_chart(selected_countries, selected_indicator, year_range):\n    \"\"\"Create interactive GDP trend visualization\"\"\"\n\n    # Load filtered data\n    df = load_data()\n\n    # Apply filters\n    filtered_df = df[\n        (df['country_name'].isin(selected_countries)) &\n        (df['indicator'] == selected_indicator) &\n        (df['year'].between(year_range[0], year_range[1]))\n    ]\n\n    # Create interactive line chart\n    fig = px.line(\n        filtered_df,\n        x='year',\n        y='value',\n        color='country_name',\n        title=f'{selected_indicator.replace(\"_\", \" \").title()} Trend',\n        labels={\n            'year': 'Year',\n            'value': format_indicator_label(selected_indicator),\n            'country_name': 'Country'\n        }\n    )\n\n    # Customize chart appearance\n    fig.update_layout(\n        xaxis_title=\"Year\",\n        yaxis_title=format_indicator_label(selected_indicator),\n        hovermode='x unified',\n        showlegend=True,\n        legend=dict(\n            orientation=\"h\",\n            yanchor=\"bottom\",\n            y=1.02,\n            xanchor=\"right\",\n            x=1\n        )\n    )\n\n    return fig\n\ndef format_indicator_label(indicator):\n    \"\"\"Format indicator names for display\"\"\"\n    labels = {\n        'gdp_current_usd': 'GDP (Current USD)',\n        'gdp_per_capita_current_usd': 'GDP Per Capita (Current USD)',\n        'population_total': 'Total Population',\n        'gdp_per_capita_current_usd_yoy': 'GDP Per Capita YoY Growth (%)'\n    }\n    return labels.get(indicator, indicator.replace('_', ' ').title())\n\n\n\n\n\n\n\n\nThe application’s most innovative feature is AI-powered natural language querying:\n\n\nCode\nfrom openai import OpenAI\nimport duckdb\n\ndef generate_sql_from_question(question, language):\n    \"\"\"Convert natural language question to SQL using ModelScope API\"\"\"\n\n    # Get database schema for context\n    schema_info = \"\"\"\n    Table: df_gdp\n    Columns:\n    - country_name (TEXT): Country display name\n    - country_code_2 (TEXT): ISO alpha-2 country code\n    - continent (TEXT): Continent classification\n    - year (INTEGER): Data year (2000-2024)\n    - indicator (TEXT): Economic indicator name\n    - value (REAL): Indicator value\n\n    Available indicators:\n    - gdp_current_usd: GDP in current USD\n    - gdp_per_capita_current_usd: GDP per capita in current USD\n    - population_total: Total population\n    - gdp_per_capita_current_usd_yoy: Year-over-year GDP per capita growth (%)\n    \"\"\"\n\n    # Create language-specific prompt\n    if language == \"zh\":\n        prompt = f\"\"\"请将以下自然语言问题转换为SQL查询，仅返回SQL语句，不要解释。\n\n数据库信息：\n{schema_info}\n\n用户问题：{question}\n\n要求：\n1. 只返回标准的SELECT语句\n2. 不要添加任何解释或注释\n3. 使用LIMIT 50限制结果数量\"\"\"\n    else:\n        prompt = f\"\"\"Convert the following natural language question to SQL query. Return only the SQL statement, no explanation.\n\nDatabase information:\n{schema_info}\n\nUser question: {question}\n\nRequirements:\n1. Return only standard SELECT statement\n2. Do not add any explanation or comments\n3. Use LIMIT 50 to restrict results\"\"\"\n\n    # Call ModelScope API\n    client = OpenAI(\n        api_key=os.getenv('MODELSCOPE_API_KEY'),\n        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n    )\n\n    response = client.chat.completions.create(\n        model=\"qwen/Qwen3-235B\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0.1,  # Low temperature for consistent SQL\n        max_tokens=500\n    )\n\n    return response.choices[0].message.content.strip()\n\n\n\n\n\n\n\nCode\ndef execute_query_with_ai_summary(sql_query, original_question):\n    \"\"\"Execute SQL query and generate AI summary\"\"\"\n\n    try:\n        # Execute query with DuckDB\n        conn = duckdb.connect()\n        result_df = conn.execute(sql_query).fetchdf()\n\n        # Store results in session state\n        st.session_state.sql_result = result_df\n        st.session_state.sql_query = sql_query\n\n        # Generate AI summary if data is available\n        if not result_df.empty:\n            generate_ai_summary(result_df, original_question)\n\n        return result_df\n\n    except Exception as e:\n        st.error(f\"Query execution error: {str(e)}\")\n        return None\n\ndef generate_ai_summary(data_frame, question):\n    \"\"\"Generate AI summary of query results\"\"\"\n\n    # Convert DataFrame to text for analysis\n    data_summary = data_frame.to_string(index=False)\n\n    summary_prompt = f\"\"\"Based on the following data analysis results, provide a concise summary:\n\nOriginal Question: {question}\n\nQuery Results:\n{data_summary}\n\nPlease provide:\n1. A brief analysis of the key findings\n2. Any notable trends or patterns\n3. Important insights from the data\n\nKeep the summary under 200 words and make it easy to understand.\"\"\"\n\n    # Generate summary using AI\n    client = OpenAI(\n        api_key=os.getenv('MODELSCOPE_API_KEY'),\n        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n    )\n\n    response = client.chat.completions.create(\n        model=\"qwen/Qwen3-235B\",\n        messages=[{\"role\": \"user\", \"content\": summary_prompt}],\n        temperature=0.3,\n        max_tokens=300\n    )\n\n    st.session_state.ai_summary = response.choices[0].message.content\n\n\n\n\n\nAI Query Interface - Natural Language Processing\n\n\n\n\n\n\n\n\nThe application implements comprehensive English/Chinese support:\n\n\nCode\n# language.py - Complete translation system\ntranslations = {\n    \"en\": {\n        \"title\": \"🌍 GDP Trend Dashboard\",\n        \"gdp_trend\": \"GDP Trend\",\n        \"ai_powered_chat\": \"AI-Powered Data Chat\",\n        \"default_question\": \"What is the average GDP per capita for China, Japan, and Korea during 2020 to 2023?\"\n    },\n    \"zh\": {\n        \"title\": \"🌍 GDP趋势仪表板\",\n        \"gdp_trend\": \"GDP趋势\",\n        \"ai_powered_chat\": \"AI数据对话\",\n        \"default_question\": \"2020年至2023年期间，中国、日本和韩国的平均人均GDP是多少？\"\n    }\n}\n\ndef get_text(key):\n    \"\"\"Get translated text for current language\"\"\"\n    language = st.session_state.get(\"language\", \"en\")\n    return translations.get(language, {}).get(key, key)\n\n# Language switching in UI\ncol1, col2 = st.columns([1, 1])\nwith col1:\n    if st.button(\"English\"):\n        st.session_state.language = \"en\"\n        st.rerun()\nwith col2:\n    if st.button(\"中文\"):\n        st.session_state.language = \"zh\"\n        st.rerun()\n\n\n\n\n\n\n\n\n\n\nCode\ndef main():\n    \"\"\"Main application with tab-based navigation\"\"\"\n\n    # Language toggle in top-right corner\n    with st.container():\n        st.markdown(\"\"\"\n        &lt;div class=\"language-toggle\"&gt;\n            &lt;button onclick=\"setLanguage('en')\"&gt;EN&lt;/button&gt;\n            &lt;button onclick=\"setLanguage('zh')\"&gt;中文&lt;/button&gt;\n        &lt;/div&gt;\n        \"\"\", unsafe_allow_html=True)\n\n    st.title(get_text(\"title\"))\n\n    # Tab-based interface\n    tab1, tab2 = st.tabs([get_text(\"gdp_trend\"), get_text(\"query\")])\n\n    with tab1:\n        render_gdp_trends_tab()\n\n    with tab2:\n        render_query_interface_tab()\n\ndef render_gdp_trends_tab():\n    \"\"\"Render GDP trends visualization tab\"\"\"\n\n    st.header(get_text(\"gdp_trend\"))\n\n    # Load data\n    df = load_data()\n\n    # Filter controls\n    col1, col2, col3 = st.columns([2, 1, 1])\n\n    with col1:\n        selected_countries = st.multiselect(\n            get_text(\"select_countries\"),\n            options=df['country_name'].unique(),\n            default=['United States', 'China', 'Japan']\n        )\n\n    with col2:\n        selected_indicator = st.selectbox(\n            get_text(\"select_indicator\"),\n            options=[\n                'gdp_current_usd',\n                'gdp_per_capita_current_usd',\n                'population_total',\n                'gdp_per_capita_current_usd_yoy'\n            ],\n            format_func=lambda x: format_indicator_label(x)\n        )\n\n    with col3:\n        year_range = st.slider(\n            get_text(\"select_year_range\"),\n            min_value=2000,\n            max_value=2024,\n            value=(2010, 2024),\n            step=1\n        )\n\n    # Generate and display chart\n    if selected_countries and selected_indicator:\n        fig = create_gdp_trend_chart(selected_countries, selected_indicator, year_range)\n        st.plotly_chart(fig, use_container_width=True)\n\n        # Display raw data option\n        if st.checkbox(get_text(\"show_raw_data\")):\n            display_filtered_data_table(selected_countries, selected_indicator, year_range)\n\ndef render_query_interface_tab():\n    \"\"\"Render AI-powered query interface\"\"\"\n\n    st.header(get_text(\"ai_powered_chat\"))\n    st.markdown(get_text(\"ai_chat_description\"))\n\n    # Natural language input\n    user_question = st.text_input(\n        get_text(\"your_question\"),\n        value=st.session_state.get(\"user_question\", get_text(\"default_question\"))\n    )\n\n    col1, col2 = st.columns([1, 1])\n\n    with col1:\n        if st.button(get_text(\"run_ai\"), type=\"primary\"):\n            if user_question:\n                with st.spinner(\"Processing with AI...\"):\n                    # Generate SQL from natural language\n                    sql_query = generate_sql_from_question(\n                        user_question,\n                        st.session_state.language\n                    )\n\n                    if sql_query:\n                        # Execute query and generate summary\n                        result_df = execute_query_with_ai_summary(sql_query, user_question)\n\n                        st.session_state.user_question = user_question\n                        st.session_state.should_generate_ai_summary = True\n\n    # Display results\n    if st.session_state.get(\"sql_result\") is not None:\n        display_query_results()\n\n\n\n\n\n\n\n\n\n\nCode\n# Streamlit caching for data operations\n@st.cache_data(ttl=3600)  # Cache for 1 hour\ndef load_data():\n    \"\"\"Load and cache GDP data\"\"\"\n    return pd.read_csv('data/gdp_data_2000_present.csv')\n\n@st.cache_data(ttl=1800)  # Cache for 30 minutes\ndef get_country_list():\n    \"\"\"Get and cache unique country list\"\"\"\n    df = load_data()\n    return sorted(df['country_name'].unique())\n\n# Session state management for AI interactions\ndef init_session_state():\n    \"\"\"Initialize all session state variables\"\"\"\n    if \"sql_result\" not in st.session_state:\n        st.session_state.sql_result = None\n    if \"ai_summary\" not in st.session_state:\n        st.session_state.ai_summary = None\n    if \"should_generate_ai_summary\" not in st.session_state:\n        st.session_state.should_generate_ai_summary = False\n\n\n\n\n\n\n\nCode\ndef safe_api_call(func, *args, **kwargs):\n    \"\"\"Safe API call with error handling\"\"\"\n    try:\n        return func(*args, **kwargs)\n    except Exception as e:\n        st.error(f\"API Error: {str(e)}\")\n        return None\n\ndef validate_sql_query(sql_query):\n    \"\"\"Basic SQL query validation\"\"\"\n    sql_lower = sql_query.lower().strip()\n\n    # Basic security checks\n    dangerous_keywords = ['drop', 'delete', 'update', 'insert', 'alter']\n    for keyword in dangerous_keywords:\n        if keyword in sql_lower:\n            raise ValueError(f\"Dangerous SQL keyword detected: {keyword}\")\n\n    # Ensure query starts with SELECT\n    if not sql_lower.startswith('select'):\n        raise ValueError(\"Only SELECT queries are allowed\")\n\n    return True\n\n\n\n\n\n\n\n\n# Environment configuration\n# .env file\nMODELSCOPE_API_KEY=your_modelscope_key\n\n# Installation\npip install -r requirements.txt\n\n# Data collection\npython download_data.py\n\n# Run application\nstreamlit run app.py\n\n\n\nstreamlit&gt;=1.28.0\npandas&gt;=1.5.0\nplotly&gt;=5.15.0\nduckdb&gt;=0.8.0\nopenai&gt;=1.0.0\npython-dotenv&gt;=1.0.0\nwbgapi&gt;=1.0.0\npycountry&gt;=22.0.0\nnumpy&gt;=1.24.0\n\n\n\n\n\n\n\nDual Interface Approach: Both visual and natural language access to data\nAI-Powered SQL Generation: Complex economic queries in plain English/Chinese\nReal-time Data Processing: Efficient caching and session management\nComprehensive Internationalization: True bilingual support with localized data\nProduction-Ready Deployment: Robust error handling and performance optimization\n\n\n\n\nAI Summary Generation\n\n\n\n\n\n\n\n\n\nComparative Analysis\n\n“Compare GDP growth between China and Japan from 2010 to 2020”\n“Which countries had the highest GDP per capita in 2023?”\n\nTime-Series Analysis\n\n“Show GDP trends for BRIC countries over the last decade”\n“What was the population growth rate for India from 2000 to 2020?”\n\nStatistical Queries\n\n“Calculate average GDP growth rate for European countries”\n“Find countries with GDP per capita above $50,000 in 2022”\n\nComplex Multi-Variable Analysis\n\n“What is the correlation between population and GDP for Asian countries?”\n“List countries with GDP per capita growth above 5% for 3 consecutive years”\n\n\n\n\n\n\nThis GDP Trend Dashboard represents an innovative approach to economic data analysis, combining traditional visualization techniques with cutting-edge AI capabilities. The project demonstrates:\n\nAdvanced Data Integration: World Bank API with comprehensive economic indicators\nNatural Language Processing: AI-powered SQL generation for accessible data querying\nProfessional Visualization: Interactive Plotly charts with consistent design\nBilingual Support: Complete English/Chinese localization\nProduction-Grade Architecture: Robust error handling, caching, and deployment\n\nWhether you’re an economist, data scientist, or policy analyst, this application showcases how modern AI technologies can make complex economic data more accessible and actionable through intuitive interfaces and intelligent automation.\n\nTechnology Stack: Streamlit, DuckDB, Plotly, ModelScope API, World Bank API\nData Source: World Bank (2000-2024, 200+ countries, 15K+ data points)"
  },
  {
    "objectID": "posts/gdp-trend/index.html#core-architecture",
    "href": "posts/gdp-trend/index.html#core-architecture",
    "title": "Interactive GDP Trend Dashboard with AI SQL",
    "section": "",
    "text": "Primary Framework: Streamlit for interactive web applications\nData Processing: Pandas for manipulation and analysis\nVisualization: Plotly Express for interactive charts\nDatabase: DuckDB for efficient SQL queries\nAI Integration: ModelScope GLM-4.6 for natural language to SQL conversion\nData Sources: World Bank API via wbgapi library\n\n\n\n\n\nMulti-country GDP trend comparisons\nSupport for multiple economic indicators (GDP, GDP per capita, population, YoY growth)\nAI-powered natural language querying\nDirect SQL interface with DuckDB\nComplete bilingual support (English/Chinese)\nTime-range filtering with interactive controls (2000-present)"
  },
  {
    "objectID": "posts/gdp-trend/index.html#data-pipeline-architecture",
    "href": "posts/gdp-trend/index.html#data-pipeline-architecture",
    "title": "Interactive GDP Trend Dashboard with AI SQL",
    "section": "",
    "text": "The application implements a sophisticated data pipeline that ensures data quality and real-time availability:\n\n\n\n\n\nflowchart TD\n    A[World Bank API] --&gt; B[Data Collection&lt;br/&gt;download_data.py]\n    B --&gt; C[Data Processing&lt;br/&gt;Country Mapping, Indicator Calculation]\n    C --&gt; D[Data Storage&lt;br/&gt;CSV Files]\n\n    D --&gt; E[Streamlit App&lt;br/&gt;app.py]\n    E --&gt; F[DuckDB&lt;br/&gt;In-Memory SQL]\n\n    F --&gt; G[Visualization Engine&lt;br/&gt;Plotly Express]\n    F --&gt; H[AI Query Engine&lt;br/&gt;ModelScope API]\n    F --&gt; I[Session Management&lt;br/&gt;User State]\n\n    G --&gt; J[Interactive Charts]\n    H --&gt; K[Natural Language&lt;br/&gt;to SQL]\n    I --&gt; L[Persistent&lt;br/&gt;Query Results]\n\n    J --&gt; M[User Interface]\n    K --&gt; M\n    L --&gt; M\n\n\n GDP Application Architecture"
  },
  {
    "objectID": "posts/gdp-trend/index.html#data-collection-and-processing",
    "href": "posts/gdp-trend/index.html#data-collection-and-processing",
    "title": "Interactive GDP Trend Dashboard with AI SQL",
    "section": "",
    "text": "The application uses the World Bank API to collect comprehensive economic data:\n\n\nCode\nimport wbgapi as wb\nimport pycountry\nimport pandas as pd\n\ndef download_economic_data():\n    \"\"\"Download GDP data from World Bank API for all countries\"\"\"\n\n    # Define economic indicators to download\n    indicators = {\n        'gdp_current_usd': 'NY.GDP.MKTP.CD',\n        'gdp_per_capita_current_usd': 'NY.GDP.PCAP.CD',\n        'population_total': 'SP.POP.TOTL'\n    }\n\n    # Download data from 2000 to present\n    data_frames = []\n    for indicator_name, indicator_code in indicators.items():\n        df = wb.get_series(\n            series=indicator_code,\n            economy='all',\n            time='2000:2024',\n            simplify_index=True\n        )\n\n        # Process and add to collection\n        df = df.reset_index()\n        df['indicator'] = indicator_name\n        data_frames.append(df)\n\n    # Combine all indicators\n    combined_df = pd.concat(data_frames, ignore_index=True)\n    return combined_df\n\ndef create_country_reference_table():\n    \"\"\"Create comprehensive country metadata with ISO codes\"\"\"\n    countries = list(pycountry.countries)\n\n    df_all = pd.DataFrame([{\n        'country_name': country.name,\n        'country_code_2': country.alpha_2,\n        'country_code_3': country.alpha_3\n    } for country in countries])\n\n    # Add continent mapping\n    iso_to_continent = {\n        \"US\": \"North America\", \"CN\": \"Asia\", \"JP\": \"Asia\",\n        \"DE\": \"Europe\", \"GB\": \"Europe\", \"FR\": \"Europe\"\n        # ... complete mapping for all countries\n    }\n\n    df_all['continent'] = df_all['country_code_2'].map(iso_to_continent)\n    return df_all\n\n\n\n\n\nThe application uses a clean, normalized data structure:\n\n\nCode\n-- Main data schema\nCREATE TABLE df_gdp (\n    country_name TEXT,        -- Display name (English)\n    country_code_2 TEXT,      -- ISO alpha-2 code\n    country_code_3 TEXT,      -- ISO alpha-3 code\n    continent TEXT,            -- Continent classification\n    year INTEGER,              -- Data year\n    indicator TEXT,            -- Economic indicator name\n    value REAL                -- Indicator value\n);\n\n-- Example data\nINSERT INTO df_gdp VALUES\n('United States', 'US', 'USA', 'North America', 2023, 'gdp_current_usd', 27444144.3),\n('United States', 'US', 'USA', 'North America', 2023, 'gdp_per_capita_current_usd', 81254.2),\n('United States', 'US', 'USA', 'North America', 2023, 'population_total', 334914895.0);"
  },
  {
    "objectID": "posts/gdp-trend/index.html#interactive-visualization-system",
    "href": "posts/gdp-trend/index.html#interactive-visualization-system",
    "title": "Interactive GDP Trend Dashboard with AI SQL",
    "section": "",
    "text": "The application uses Plotly Express for creating interactive charts with consistent color coding:\n\n\nCode\nimport plotly.express as px\n\n@st.cache_data\ndef load_data():\n    \"\"\"Load and cache GDP data\"\"\"\n    return pd.read_csv('data/gdp_data_2000_present.csv')\n\ndef create_gdp_trend_chart(selected_countries, selected_indicator, year_range):\n    \"\"\"Create interactive GDP trend visualization\"\"\"\n\n    # Load filtered data\n    df = load_data()\n\n    # Apply filters\n    filtered_df = df[\n        (df['country_name'].isin(selected_countries)) &\n        (df['indicator'] == selected_indicator) &\n        (df['year'].between(year_range[0], year_range[1]))\n    ]\n\n    # Create interactive line chart\n    fig = px.line(\n        filtered_df,\n        x='year',\n        y='value',\n        color='country_name',\n        title=f'{selected_indicator.replace(\"_\", \" \").title()} Trend',\n        labels={\n            'year': 'Year',\n            'value': format_indicator_label(selected_indicator),\n            'country_name': 'Country'\n        }\n    )\n\n    # Customize chart appearance\n    fig.update_layout(\n        xaxis_title=\"Year\",\n        yaxis_title=format_indicator_label(selected_indicator),\n        hovermode='x unified',\n        showlegend=True,\n        legend=dict(\n            orientation=\"h\",\n            yanchor=\"bottom\",\n            y=1.02,\n            xanchor=\"right\",\n            x=1\n        )\n    )\n\n    return fig\n\ndef format_indicator_label(indicator):\n    \"\"\"Format indicator names for display\"\"\"\n    labels = {\n        'gdp_current_usd': 'GDP (Current USD)',\n        'gdp_per_capita_current_usd': 'GDP Per Capita (Current USD)',\n        'population_total': 'Total Population',\n        'gdp_per_capita_current_usd_yoy': 'GDP Per Capita YoY Growth (%)'\n    }\n    return labels.get(indicator, indicator.replace('_', ' ').title())"
  },
  {
    "objectID": "posts/gdp-trend/index.html#ai-powered-analytics",
    "href": "posts/gdp-trend/index.html#ai-powered-analytics",
    "title": "Interactive GDP Trend Dashboard with AI SQL",
    "section": "",
    "text": "The application’s most innovative feature is AI-powered natural language querying:\n\n\nCode\nfrom openai import OpenAI\nimport duckdb\n\ndef generate_sql_from_question(question, language):\n    \"\"\"Convert natural language question to SQL using ModelScope API\"\"\"\n\n    # Get database schema for context\n    schema_info = \"\"\"\n    Table: df_gdp\n    Columns:\n    - country_name (TEXT): Country display name\n    - country_code_2 (TEXT): ISO alpha-2 country code\n    - continent (TEXT): Continent classification\n    - year (INTEGER): Data year (2000-2024)\n    - indicator (TEXT): Economic indicator name\n    - value (REAL): Indicator value\n\n    Available indicators:\n    - gdp_current_usd: GDP in current USD\n    - gdp_per_capita_current_usd: GDP per capita in current USD\n    - population_total: Total population\n    - gdp_per_capita_current_usd_yoy: Year-over-year GDP per capita growth (%)\n    \"\"\"\n\n    # Create language-specific prompt\n    if language == \"zh\":\n        prompt = f\"\"\"请将以下自然语言问题转换为SQL查询，仅返回SQL语句，不要解释。\n\n数据库信息：\n{schema_info}\n\n用户问题：{question}\n\n要求：\n1. 只返回标准的SELECT语句\n2. 不要添加任何解释或注释\n3. 使用LIMIT 50限制结果数量\"\"\"\n    else:\n        prompt = f\"\"\"Convert the following natural language question to SQL query. Return only the SQL statement, no explanation.\n\nDatabase information:\n{schema_info}\n\nUser question: {question}\n\nRequirements:\n1. Return only standard SELECT statement\n2. Do not add any explanation or comments\n3. Use LIMIT 50 to restrict results\"\"\"\n\n    # Call ModelScope API\n    client = OpenAI(\n        api_key=os.getenv('MODELSCOPE_API_KEY'),\n        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n    )\n\n    response = client.chat.completions.create(\n        model=\"qwen/Qwen3-235B\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0.1,  # Low temperature for consistent SQL\n        max_tokens=500\n    )\n\n    return response.choices[0].message.content.strip()\n\n\n\n\n\n\n\nCode\ndef execute_query_with_ai_summary(sql_query, original_question):\n    \"\"\"Execute SQL query and generate AI summary\"\"\"\n\n    try:\n        # Execute query with DuckDB\n        conn = duckdb.connect()\n        result_df = conn.execute(sql_query).fetchdf()\n\n        # Store results in session state\n        st.session_state.sql_result = result_df\n        st.session_state.sql_query = sql_query\n\n        # Generate AI summary if data is available\n        if not result_df.empty:\n            generate_ai_summary(result_df, original_question)\n\n        return result_df\n\n    except Exception as e:\n        st.error(f\"Query execution error: {str(e)}\")\n        return None\n\ndef generate_ai_summary(data_frame, question):\n    \"\"\"Generate AI summary of query results\"\"\"\n\n    # Convert DataFrame to text for analysis\n    data_summary = data_frame.to_string(index=False)\n\n    summary_prompt = f\"\"\"Based on the following data analysis results, provide a concise summary:\n\nOriginal Question: {question}\n\nQuery Results:\n{data_summary}\n\nPlease provide:\n1. A brief analysis of the key findings\n2. Any notable trends or patterns\n3. Important insights from the data\n\nKeep the summary under 200 words and make it easy to understand.\"\"\"\n\n    # Generate summary using AI\n    client = OpenAI(\n        api_key=os.getenv('MODELSCOPE_API_KEY'),\n        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n    )\n\n    response = client.chat.completions.create(\n        model=\"qwen/Qwen3-235B\",\n        messages=[{\"role\": \"user\", \"content\": summary_prompt}],\n        temperature=0.3,\n        max_tokens=300\n    )\n\n    st.session_state.ai_summary = response.choices[0].message.content\n\n\n\n\n\nAI Query Interface - Natural Language Processing"
  },
  {
    "objectID": "posts/gdp-trend/index.html#bilingual-support-system",
    "href": "posts/gdp-trend/index.html#bilingual-support-system",
    "title": "Interactive GDP Trend Dashboard with AI SQL",
    "section": "",
    "text": "The application implements comprehensive English/Chinese support:\n\n\nCode\n# language.py - Complete translation system\ntranslations = {\n    \"en\": {\n        \"title\": \"🌍 GDP Trend Dashboard\",\n        \"gdp_trend\": \"GDP Trend\",\n        \"ai_powered_chat\": \"AI-Powered Data Chat\",\n        \"default_question\": \"What is the average GDP per capita for China, Japan, and Korea during 2020 to 2023?\"\n    },\n    \"zh\": {\n        \"title\": \"🌍 GDP趋势仪表板\",\n        \"gdp_trend\": \"GDP趋势\",\n        \"ai_powered_chat\": \"AI数据对话\",\n        \"default_question\": \"2020年至2023年期间，中国、日本和韩国的平均人均GDP是多少？\"\n    }\n}\n\ndef get_text(key):\n    \"\"\"Get translated text for current language\"\"\"\n    language = st.session_state.get(\"language\", \"en\")\n    return translations.get(language, {}).get(key, key)\n\n# Language switching in UI\ncol1, col2 = st.columns([1, 1])\nwith col1:\n    if st.button(\"English\"):\n        st.session_state.language = \"en\"\n        st.rerun()\nwith col2:\n    if st.button(\"中文\"):\n        st.session_state.language = \"zh\"\n        st.rerun()"
  },
  {
    "objectID": "posts/gdp-trend/index.html#user-interface-design",
    "href": "posts/gdp-trend/index.html#user-interface-design",
    "title": "Interactive GDP Trend Dashboard with AI SQL",
    "section": "",
    "text": "Code\ndef main():\n    \"\"\"Main application with tab-based navigation\"\"\"\n\n    # Language toggle in top-right corner\n    with st.container():\n        st.markdown(\"\"\"\n        &lt;div class=\"language-toggle\"&gt;\n            &lt;button onclick=\"setLanguage('en')\"&gt;EN&lt;/button&gt;\n            &lt;button onclick=\"setLanguage('zh')\"&gt;中文&lt;/button&gt;\n        &lt;/div&gt;\n        \"\"\", unsafe_allow_html=True)\n\n    st.title(get_text(\"title\"))\n\n    # Tab-based interface\n    tab1, tab2 = st.tabs([get_text(\"gdp_trend\"), get_text(\"query\")])\n\n    with tab1:\n        render_gdp_trends_tab()\n\n    with tab2:\n        render_query_interface_tab()\n\ndef render_gdp_trends_tab():\n    \"\"\"Render GDP trends visualization tab\"\"\"\n\n    st.header(get_text(\"gdp_trend\"))\n\n    # Load data\n    df = load_data()\n\n    # Filter controls\n    col1, col2, col3 = st.columns([2, 1, 1])\n\n    with col1:\n        selected_countries = st.multiselect(\n            get_text(\"select_countries\"),\n            options=df['country_name'].unique(),\n            default=['United States', 'China', 'Japan']\n        )\n\n    with col2:\n        selected_indicator = st.selectbox(\n            get_text(\"select_indicator\"),\n            options=[\n                'gdp_current_usd',\n                'gdp_per_capita_current_usd',\n                'population_total',\n                'gdp_per_capita_current_usd_yoy'\n            ],\n            format_func=lambda x: format_indicator_label(x)\n        )\n\n    with col3:\n        year_range = st.slider(\n            get_text(\"select_year_range\"),\n            min_value=2000,\n            max_value=2024,\n            value=(2010, 2024),\n            step=1\n        )\n\n    # Generate and display chart\n    if selected_countries and selected_indicator:\n        fig = create_gdp_trend_chart(selected_countries, selected_indicator, year_range)\n        st.plotly_chart(fig, use_container_width=True)\n\n        # Display raw data option\n        if st.checkbox(get_text(\"show_raw_data\")):\n            display_filtered_data_table(selected_countries, selected_indicator, year_range)\n\ndef render_query_interface_tab():\n    \"\"\"Render AI-powered query interface\"\"\"\n\n    st.header(get_text(\"ai_powered_chat\"))\n    st.markdown(get_text(\"ai_chat_description\"))\n\n    # Natural language input\n    user_question = st.text_input(\n        get_text(\"your_question\"),\n        value=st.session_state.get(\"user_question\", get_text(\"default_question\"))\n    )\n\n    col1, col2 = st.columns([1, 1])\n\n    with col1:\n        if st.button(get_text(\"run_ai\"), type=\"primary\"):\n            if user_question:\n                with st.spinner(\"Processing with AI...\"):\n                    # Generate SQL from natural language\n                    sql_query = generate_sql_from_question(\n                        user_question,\n                        st.session_state.language\n                    )\n\n                    if sql_query:\n                        # Execute query and generate summary\n                        result_df = execute_query_with_ai_summary(sql_query, user_question)\n\n                        st.session_state.user_question = user_question\n                        st.session_state.should_generate_ai_summary = True\n\n    # Display results\n    if st.session_state.get(\"sql_result\") is not None:\n        display_query_results()"
  },
  {
    "objectID": "posts/gdp-trend/index.html#performance-optimization",
    "href": "posts/gdp-trend/index.html#performance-optimization",
    "title": "Interactive GDP Trend Dashboard with AI SQL",
    "section": "",
    "text": "Code\n# Streamlit caching for data operations\n@st.cache_data(ttl=3600)  # Cache for 1 hour\ndef load_data():\n    \"\"\"Load and cache GDP data\"\"\"\n    return pd.read_csv('data/gdp_data_2000_present.csv')\n\n@st.cache_data(ttl=1800)  # Cache for 30 minutes\ndef get_country_list():\n    \"\"\"Get and cache unique country list\"\"\"\n    df = load_data()\n    return sorted(df['country_name'].unique())\n\n# Session state management for AI interactions\ndef init_session_state():\n    \"\"\"Initialize all session state variables\"\"\"\n    if \"sql_result\" not in st.session_state:\n        st.session_state.sql_result = None\n    if \"ai_summary\" not in st.session_state:\n        st.session_state.ai_summary = None\n    if \"should_generate_ai_summary\" not in st.session_state:\n        st.session_state.should_generate_ai_summary = False\n\n\n\n\n\n\n\nCode\ndef safe_api_call(func, *args, **kwargs):\n    \"\"\"Safe API call with error handling\"\"\"\n    try:\n        return func(*args, **kwargs)\n    except Exception as e:\n        st.error(f\"API Error: {str(e)}\")\n        return None\n\ndef validate_sql_query(sql_query):\n    \"\"\"Basic SQL query validation\"\"\"\n    sql_lower = sql_query.lower().strip()\n\n    # Basic security checks\n    dangerous_keywords = ['drop', 'delete', 'update', 'insert', 'alter']\n    for keyword in dangerous_keywords:\n        if keyword in sql_lower:\n            raise ValueError(f\"Dangerous SQL keyword detected: {keyword}\")\n\n    # Ensure query starts with SELECT\n    if not sql_lower.startswith('select'):\n        raise ValueError(\"Only SELECT queries are allowed\")\n\n    return True"
  },
  {
    "objectID": "posts/gdp-trend/index.html#deployment-and-accessibility",
    "href": "posts/gdp-trend/index.html#deployment-and-accessibility",
    "title": "Interactive GDP Trend Dashboard with AI SQL",
    "section": "",
    "text": "# Environment configuration\n# .env file\nMODELSCOPE_API_KEY=your_modelscope_key\n\n# Installation\npip install -r requirements.txt\n\n# Data collection\npython download_data.py\n\n# Run application\nstreamlit run app.py\n\n\n\nstreamlit&gt;=1.28.0\npandas&gt;=1.5.0\nplotly&gt;=5.15.0\nduckdb&gt;=0.8.0\nopenai&gt;=1.0.0\npython-dotenv&gt;=1.0.0\nwbgapi&gt;=1.0.0\npycountry&gt;=22.0.0\nnumpy&gt;=1.24.0"
  },
  {
    "objectID": "posts/gdp-trend/index.html#technical-achievements",
    "href": "posts/gdp-trend/index.html#technical-achievements",
    "title": "Interactive GDP Trend Dashboard with AI SQL",
    "section": "",
    "text": "Dual Interface Approach: Both visual and natural language access to data\nAI-Powered SQL Generation: Complex economic queries in plain English/Chinese\nReal-time Data Processing: Efficient caching and session management\nComprehensive Internationalization: True bilingual support with localized data\nProduction-Ready Deployment: Robust error handling and performance optimization\n\n\n\n\nAI Summary Generation"
  },
  {
    "objectID": "posts/gdp-trend/index.html#economic-analysis-examples",
    "href": "posts/gdp-trend/index.html#economic-analysis-examples",
    "title": "Interactive GDP Trend Dashboard with AI SQL",
    "section": "",
    "text": "Comparative Analysis\n\n“Compare GDP growth between China and Japan from 2010 to 2020”\n“Which countries had the highest GDP per capita in 2023?”\n\nTime-Series Analysis\n\n“Show GDP trends for BRIC countries over the last decade”\n“What was the population growth rate for India from 2000 to 2020?”\n\nStatistical Queries\n\n“Calculate average GDP growth rate for European countries”\n“Find countries with GDP per capita above $50,000 in 2022”\n\nComplex Multi-Variable Analysis\n\n“What is the correlation between population and GDP for Asian countries?”\n“List countries with GDP per capita growth above 5% for 3 consecutive years”"
  },
  {
    "objectID": "posts/gdp-trend/index.html#conclusion",
    "href": "posts/gdp-trend/index.html#conclusion",
    "title": "Interactive GDP Trend Dashboard with AI SQL",
    "section": "",
    "text": "This GDP Trend Dashboard represents an innovative approach to economic data analysis, combining traditional visualization techniques with cutting-edge AI capabilities. The project demonstrates:\n\nAdvanced Data Integration: World Bank API with comprehensive economic indicators\nNatural Language Processing: AI-powered SQL generation for accessible data querying\nProfessional Visualization: Interactive Plotly charts with consistent design\nBilingual Support: Complete English/Chinese localization\nProduction-Grade Architecture: Robust error handling, caching, and deployment\n\nWhether you’re an economist, data scientist, or policy analyst, this application showcases how modern AI technologies can make complex economic data more accessible and actionable through intuitive interfaces and intelligent automation.\n\nTechnology Stack: Streamlit, DuckDB, Plotly, ModelScope API, World Bank API\nData Source: World Bank (2000-2024, 200+ countries, 15K+ data points)"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "As 2026-01-08. Total number of posts: 10\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n \n\n\n\nDate\n\n\n\nTitle\n\n\n\nCategories\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2026\n\n\nLLM Summary System: A Multi-Platform AI Summarization Tool\n\n\nAI, Python, LLM, Open Source\n\n\n\n\n\n\n\n\n\nJan 7, 2026\n\n\nAI Chat: A Multilingual Multi-Model application\n\n\nAI, Streamlit, Python, LLM\n\n\n\n\n\n\n\n\n\nNov 18, 2025\n\n\nInteractive GDP Trend Dashboard with AI SQL\n\n\nData Visualization, Streamlit, Economic Analysis, AI\n\n\n\n\n\n\n\n\n\nNov 8, 2025\n\n\nShop Map Manager (店铺地图管理系统)\n\n\nAI, API, map\n\n\n\n\n\n\n\n\n\nNov 6, 2025\n\n\nWeather Forecast App with Streamlit and AI Integration\n\n\nPython, Streamlit, AI, tutorial\n\n\n\n\n\n\n\n\n\nNov 5, 2025\n\n\nWeight Tracking Dashboard with Shiny,streamlit and AI Integration\n\n\nAI, API, tutorial\n\n\n\n\n\n\n\n\n\nNov 5, 2025\n\n\nWhisky Tasting note with RAG\n\n\nAI, API, tutorial\n\n\n\n\n\n\n\n\n\nNov 5, 2025\n\n\nYOLO Object Detection App with Streamlit\n\n\nAI, API, tutorial\n\n\n\n\n\n\n\n\n\nNov 2, 2025\n\n\nRetrieval-Augmented Generation(RAG) in R & Python\n\n\nAI, API, tutorial\n\n\n\n\n\n\n\n\n\nNov 1, 2025\n\n\nOpenRouter: A Unified API for Multiple AI Models\n\n\nAI, API, tutorial\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cv.html#tony-jinchao-duan",
    "href": "cv.html#tony-jinchao-duan",
    "title": "CV / 简历",
    "section": "Tony Jinchao Duan",
    "text": "Tony Jinchao Duan\nTelephone: 13609618820\nEmail: jcflyingco@outlook.com"
  },
  {
    "objectID": "cv.html#career-objective",
    "href": "cv.html#career-objective",
    "title": "CV / 简历",
    "section": "Career Objective",
    "text": "Career Objective\nData-driven Strategy & Analytics professional with a proven track record at eBay, EY, and HSBC. Expert in transforming complex datasets into actionable business intelligence and create predictive models to achieve business goal. Specializing in AI-driven Analytic skills and interactive data tools. Seeking to leverage deep domain expertise in data operations and modern AI engineering to drive scalable business growth."
  },
  {
    "objectID": "cv.html#employment-history",
    "href": "cv.html#employment-history",
    "title": "CV / 简历",
    "section": "Employment History",
    "text": "Employment History\n\neBay | Business Management | Analytics Specialist (2018-2021) / Analytics Manager (2021-2023)\nStrategic Operations: Managed shipping performance for the Greater China seller network, ensuring alignment with global logistics standards.\nBusiness Intelligence: Designed and deployed comprehensive seller dashboards to monitor real-time performance, significantly reducing shipping delays by identifying supply chain bottlenecks.\nPolicy Development: Authored and implemented regional seller policies that balanced marketplace integrity with sustainable seller growth.\n\n\nErnst & Young | Performance Improvement Consulting | Senior Analytics Consultant (2014-2017)\nBanking Sector: Standardized reporting frameworks for a leading Chinese commercial bank, enhancing data transparency for executive stakeholders. Developed Customer Value Forecast and Segmentation models, enabling targeted CRM strategies and increasing high-value client retention.\nInsurance Sector: Designed a customer product-mix recommendation engine for a top-tier insurance provider. Developed anti-fraud modeling systems for one of the largest life insurance companies in China.\n\n\nHSBC | Consumer Credit Risk | Decision Analytics Officer (2012–2013)\nDeveloped analytical solutions to manage credit risk across the Asia-Pacific region. Built credit scorecards and optimized strategies for diverse portfolios, including credit cards and home loans, for one of the largest international banks in the region."
  },
  {
    "objectID": "cv.html#education-background",
    "href": "cv.html#education-background",
    "title": "CV / 简历",
    "section": "Education Background",
    "text": "Education Background\nMacquarie University | Sydney, Australia\nBachelor of Commerce (2009 - 2012)\nMajor in Decision Science | GPA: 3.1/4.0"
  },
  {
    "objectID": "cv.html#technical-skills",
    "href": "cv.html#technical-skills",
    "title": "CV / 简历",
    "section": "Technical Skills",
    "text": "Technical Skills\n\n\n\n\n\n\n\nCategory\nTechnologies\n\n\n\n\nAI/ML\nAgentic Workflows, RAG, LLM Integration, YOLO\n\n\nData Science\nPython (Pandas, Plotly, Scikit-learn), R (Tidyverse, GGplot2, Tidymodels)\n\n\nData Engineering\nSQL (Advanced Querying & Schema Design), Streamlit, Shiny, Quarto\n\n\nMicrosoft Tools\nExcel (VBA/Modeling), PowerPoint, Word"
  },
  {
    "objectID": "cv.html#languages",
    "href": "cv.html#languages",
    "title": "CV / 简历",
    "section": "Languages",
    "text": "Languages\nEnglish (Professional Proficiency), Mandarin (Native), Cantonese (Native)"
  },
  {
    "objectID": "cv.html#honors-and-awards",
    "href": "cv.html#honors-and-awards",
    "title": "CV / 简历",
    "section": "Honors and Awards",
    "text": "Honors and Awards\nEY ExCEED Award (2014)\nThe Ernst & Young ExCEED Award is for extra effort, exceeding expectations and recognizing those who have gone that extra mile to serve their clients."
  },
  {
    "objectID": "cv.html#段晋潮-tony-duan",
    "href": "cv.html#段晋潮-tony-duan",
    "title": "CV / 简历",
    "section": "段晋潮 Tony Duan",
    "text": "段晋潮 Tony Duan\n电话: 13609618820\n邮箱: jcflyingco@outlook.com"
  },
  {
    "objectID": "cv.html#职业目标",
    "href": "cv.html#职业目标",
    "title": "CV / 简历",
    "section": "职业目标",
    "text": "职业目标\n资深数据战略与分析专家，拥有 eBay、安永 (EY) 及汇丰银行 (HSBC) 的多元化实战经验。擅长将复杂数据集转化为可落地的业务洞见，并通过构建预测模型驱动业务目标达成。精通 AI 驱动的分析技术及交互式数据工具，致力于利用在数据运营和现代AI工程领域的深厚背景，为企业实现可扩展的业务增长。"
  },
  {
    "objectID": "cv.html#工作经历",
    "href": "cv.html#工作经历",
    "title": "CV / 简历",
    "section": "工作经历",
    "text": "工作经历\n\neBay | 业务管理部 分析专家 (2018-2021) / 分析经理 (2021-2023)\n战略运营： 统筹大中华区卖家的物流表现管理，确保其符合全球物流时效与合规标准。\n商业智能 (BI)： 设计并部署全维度卖家看板，实现实时业绩监控；通过精准识别供应链瓶颈，显著降低了物流延迟率。\n政策制定： 负责大中华区卖家政策的起草与执行，在维护平台生态健康度与保障卖家可持续增长之间取得战略平衡。\n\n\n安永 (EY) | 业绩改进咨询部 高级数据分析顾问 (2014-2017)\n银行业务： 为国内领先商业银行构建标准化报表框架，提升管理层决策的数据透明度；开发客户价值预测及分群模型，赋能精准 CRM 策略，提升高价值客户留存率。\n保险业务： 为头部保险公司设计产品组合推荐引擎；为国内大型寿险公司研发并部署反欺诈建模系统。\n\n\n汇丰银行 (HSBC) | 风险部 决策分析师 (2012-2013)\n负责亚太地区信用风险管理的分析解决方案。为该地区最具规模的国际银行开发信用评分卡，并针对信用卡及房贷等多元资产组合优化风险策略。"
  },
  {
    "objectID": "cv.html#教育背景",
    "href": "cv.html#教育背景",
    "title": "CV / 简历",
    "section": "教育背景",
    "text": "教育背景\n麦考瑞大学 (Macquarie University) | 悉尼，澳大利亚\n商学学士 (2009 - 2012)\n专业：决策科学 (Decision Science) | 绩点：3.1 / 4.0"
  },
  {
    "objectID": "cv.html#专业技能",
    "href": "cv.html#专业技能",
    "title": "CV / 简历",
    "section": "专业技能",
    "text": "专业技能\n\n\n\n\n\n\n\n类别\n技术栈\n\n\n\n\nAI/机器学习\nAgentic Workflows (智能体工作流), RAG (检索增强生成), LLM 集成应用, YOLO 计算机视觉\n\n\n数据科学\nPython (Pandas, Plotly, Scikit-learn), R (Tidyverse, GGplot2, Tidymodels)\n\n\n数据工程\nSQL (高级查询与架构设计), Streamlit (Web 应用开发), Shiny, Quarto\n\n\n办公工具\nExcel (VBA/建模), PowerPoint, Word"
  },
  {
    "objectID": "cv.html#语言能力",
    "href": "cv.html#语言能力",
    "title": "CV / 简历",
    "section": "语言能力",
    "text": "语言能力\n英语：专业流利 | 普通话/粤语：母语"
  },
  {
    "objectID": "cv.html#荣誉奖项",
    "href": "cv.html#荣誉奖项",
    "title": "CV / 简历",
    "section": "荣誉奖项",
    "text": "荣誉奖项\n安永 ExCEED 优秀员工奖 (2014)\n旨在表彰在客户服务中付出额外努力、表现远超预期并做出卓越贡献的员工。"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tony Tech Blog",
    "section": "",
    "text": "LLM Summary System: A Multi-Platform AI Summarization Tool\n\n\n\nTony D\n\n\nJan 8, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nAI Chat: A Multilingual Multi-Model application\n\n\n\nTony D\n\n\nJan 7, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive GDP Trend Dashboard with AI SQL\n\n\n\n\n\n\nNov 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nShop Map Manager (店铺地图管理系统)\n\n\n\nTony D\n\n\nNov 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeather Forecast App with Streamlit and AI Integration\n\n\n\nTony D\n\n\nNov 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeight Tracking Dashboard with Shiny,streamlit and AI Integration\n\n\n\nTony D\n\n\nNov 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhisky Tasting note with RAG\n\n\n\nTony D\n\n\nNov 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nYOLO Object Detection App with Streamlit\n\n\n\nTony D\n\n\nNov 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nRetrieval-Augmented Generation(RAG) in R & Python\n\n\n\nTony D\n\n\nNov 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpenRouter: A Unified API for Multiple AI Models\n\n\n\nTony D\n\n\nNov 1, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/RAG/index.html",
    "href": "posts/RAG/index.html",
    "title": "Retrieval-Augmented Generation(RAG) in R & Python",
    "section": "",
    "text": "Introduction\nRetrieval-Augmented Generation (RAG) is a powerful technique that combines the generative capabilities of Large Language Models (LLMs) with the precision of information retrieval. By grounding LLM responses in external, verifiable data, RAG reduces hallucinations and enables the model to answer questions about specific, private, or up-to-date information.\nIn this tutorial, we will build a RAG system using both R and Python.\nIn R, we’ll leverage the ragnar package for RAG workflows and ellmer for chat interfaces.\nIn Python, we’ll use LangChain for the RAG pipeline, ChromaDB for the vector store, and OpenAI for model interaction.\nOur goal is to create a system that can answer questions about the OpenRouter API by scraping its documentation.\n\n\nData Collection\n\nRPython\n\n\nFirst, we need to gather the data for our knowledge base. We’ll use the rvest package to scrape URLs from the OpenRouter documentation. This will give us a list of pages to ingest.\n\n\nCode\nlibrary(ragnar)\nlibrary(ellmer)\nlibrary(dotenv)\nload_dot_env(file = \".env\")\n\n\n\n\nCode\nlibrary(rvest)\n\n# URL to scrape\nurl &lt;- \"https://openrouter.ai/docs/quickstart\"\n\n# Read the HTML content of the page\npage &lt;- read_html(url)\n\n# Extract all &lt;a&gt; tags with href\nlinks &lt;- page %&gt;%\n    html_nodes(\"a\") %&gt;%\n    html_attr(\"href\")\n\n# Remove NAs and duplicates\nlinks &lt;- unique(na.omit(links))\n\n# Optional: keep only full URLs\nlinks_full &lt;- paste0(\"https://openrouter.ai\", links[grepl(\"^/docs/\", links)])\n\n# Print all links\nprint(links_full)\n\n\n [1] \"https://openrouter.ai/docs/api-reference/overview\"                               \n [2] \"https://openrouter.ai/docs/quickstart\"                                           \n [3] \"https://openrouter.ai/docs/api/reference/overview\"                               \n [4] \"https://openrouter.ai/docs/sdks/call-model/overview\"                             \n [5] \"https://openrouter.ai/docs/guides/overview/principles\"                           \n [6] \"https://openrouter.ai/docs/guides/overview/models\"                               \n [7] \"https://openrouter.ai/docs/faq\"                                                  \n [8] \"https://openrouter.ai/docs/guides/overview/report-feedback\"                      \n [9] \"https://openrouter.ai/docs/guides/routing/model-fallbacks\"                       \n[10] \"https://openrouter.ai/docs/guides/routing/provider-selection\"                    \n[11] \"https://openrouter.ai/docs/guides/features/presets\"                              \n[12] \"https://openrouter.ai/docs/guides/features/tool-calling\"                         \n[13] \"https://openrouter.ai/docs/guides/features/structured-outputs\"                   \n[14] \"https://openrouter.ai/docs/guides/features/message-transforms\"                   \n[15] \"https://openrouter.ai/docs/guides/features/zero-completion-insurance\"            \n[16] \"https://openrouter.ai/docs/guides/features/zdr\"                                  \n[17] \"https://openrouter.ai/docs/app-attribution\"                                      \n[18] \"https://openrouter.ai/docs/faq#how-are-rate-limits-calculated\"                   \n[19] \"https://openrouter.ai/docs/api/reference/streaming\"                              \n[20] \"https://openrouter.ai/docs/guides/community/frameworks-and-integrations-overview\"\n\n\n\n\nFirst, we need to gather the data for our knowledge base. We’ll use requests and BeautifulSoup to scrape URLs from the OpenRouter documentation. This will give us a list of pages to ingest.\n\n\nCode\nimport sys\nprint(sys.executable)\n\n\n/Library/Frameworks/Python.framework/Versions/3.13/bin/python3\n\n\n\n\nCode\nimport requests\nfrom bs4 import BeautifulSoup\nfrom dotenv import load_dotenv\nimport os\nfrom markitdown import MarkItDown\nfrom io import BytesIO\nimport re\n\n# Load environment variables\nload_dotenv()\n\n\nTrue\n\n\nCode\n# Helper functions\ndef fetch_html(url: str) -&gt; bytes:\n    \"\"\"Fetch HTML content from URL and return as bytes.\"\"\"\n    resp = requests.get(url)\n    resp.raise_for_status()\n    return resp.content\n\ndef html_to_markdown(html_bytes: bytes) -&gt; str:\n    \"\"\"Convert HTML bytes to markdown using MarkItDown.\"\"\"\n    md = MarkItDown()\n    stream = BytesIO(html_bytes)\n    result = md.convert_stream(stream, mime_type=\"text/html\")\n    return result.markdown\n\ndef save_markdown(md_content: str, output_path: str):\n    \"\"\"Save markdown content to file.\"\"\"\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(md_content)\n\ndef sanitize_filename(filename: str) -&gt; str:\n    \"\"\"Sanitize URL to create a valid filename.\"\"\"\n    filename = re.sub(r'^https?://[^/]+', '', filename)\n    filename = re.sub(r'[^\\w\\-_.]', '_', filename)\n    filename = filename.strip('_')\n    if not filename.endswith('.md'):\n        filename += '.md'\n    return filename\n\n# URL to scrape\nurl = \"https://openrouter.ai/docs/quickstart\"\n\n# Read the HTML content of the page\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.content, 'html.parser')\n\n# Extract all &lt;a&gt; tags with href\nlinks = [a['href'] for a in soup.find_all('a', href=True)]\n\n# Remove duplicates\nlinks = list(set(links))\n\n# Keep only full URLs for docs\nlinks_full = [f\"https://openrouter.ai{link}\" for link in links if link.startswith(\"/docs/\")]\n\n# Explicitly add FAQ\nlinks_full.append(\"https://openrouter.ai/docs/faq\")\nlinks_full = list(set(links_full))\n\n# Print all links\nprint(f\"Found {len(links_full)} documentation URLs\")\n\n\nFound 20 documentation URLs\n\n\nCode\nprint(links_full)\n\n\n['https://openrouter.ai/docs/guides/routing/provider-selection', 'https://openrouter.ai/docs/quickstart', 'https://openrouter.ai/docs/guides/overview/models', 'https://openrouter.ai/docs/guides/features/zero-completion-insurance', 'https://openrouter.ai/docs/api/reference/streaming', 'https://openrouter.ai/docs/guides/community/frameworks-and-integrations-overview', 'https://openrouter.ai/docs/guides/features/presets', 'https://openrouter.ai/docs/faq#how-are-rate-limits-calculated', 'https://openrouter.ai/docs/app-attribution', 'https://openrouter.ai/docs/api/reference/overview', 'https://openrouter.ai/docs/api-reference/overview', 'https://openrouter.ai/docs/guides/overview/principles', 'https://openrouter.ai/docs/faq', 'https://openrouter.ai/docs/guides/features/structured-outputs', 'https://openrouter.ai/docs/guides/features/zdr', 'https://openrouter.ai/docs/guides/routing/model-fallbacks', 'https://openrouter.ai/docs/guides/overview/report-feedback', 'https://openrouter.ai/docs/guides/features/tool-calling', 'https://openrouter.ai/docs/sdks/call-model/overview', 'https://openrouter.ai/docs/guides/features/message-transforms']\n\n\n\n\n\n\n\nSave web to local data\n\nRPython DuckDBPython Chroma\n\n\nTo perform semantic search, we need to store our text data as vectors (embeddings). We’ll use DuckDB as our local vector store. We also need an embedding model to convert text into vectors. Here, we configure ragnar to use a specific embedding model via an OpenAI-compatible API (SiliconFlow).\n\n\nCode\n# pages &lt;- ragnar_find_links(base_url)\npages &lt;- links_full\nstore_location &lt;- \"openrouter.duckdb\"\n\nstore &lt;- ragnar_store_create(\n    store_location,\n    overwrite = TRUE,\n    embed = \\(x) ragnar::embed_openai(x,\n        model = \"BAAI/bge-m3\",\n        base_url = \"https://api.siliconflow.cn/v1\",\n        api_key = Sys.getenv(\"siliconflow\")\n    )\n)\n\n\nWith our store initialized, we can now ingest the data. We iterate through the list of pages we scraped earlier. For each page, we: 1. Read the content as markdown. 2. Split the content into smaller chunks (approx. 600 characters). 3. Insert these chunks into our vector store.\nThis process builds the index that we’ll search against.\n\n\nCode\n# page=\"https://openrouter.ai/docs/faq\"\n# chunks &lt;- page |&gt;read_as_markdown() |&gt;markdown_chunk(target_size = 2000)\n# ragnar_chunks_view(chunks)\n\n\n\n\nCode\nfor (page in pages) {\n    message(\"ingesting: \", page)\n    print(page)\n    chunks &lt;- page |&gt;\n        read_as_markdown() |&gt;\n        markdown_chunk(target_size = 2000)\n    # print(chunks)\n    # print('chunks done')\n    ragnar_store_insert(store, chunks)\n    print(\"insrt done\")\n}\n\n\n\n\nCode\nragnar_store_build_index(store)\n\n\n\n\n\n\nCode\nimport os\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, Settings\nfrom llama_index.vector_stores.duckdb import DuckDBVectorStore\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\n# --- 1. Configuration ---\n\n# Ensure your API key is available\nopenrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\") # or paste string directly\n\n# Initialize the embedding model pointing to OpenRouter\n# We use the OpenAI class because OpenRouter uses an OpenAI-compatible API structure\nembed_model = OpenAIEmbedding(\n    api_key=openrouter_api_key,\n    base_url=\"https://openrouter.ai/api/v1\",\n    model=\"qwen/qwen3-embedding-8b\"  \n)\n\n# Update the global settings so LlamaIndex knows to use this model\nSettings.embed_model = embed_model\nSettings.chunk_size = 2000\nSettings.chunk_overlap = 200\n# --- 2. Ingestion and Indexing ---\n\n# Load data\ndocuments = SimpleDirectoryReader(\"markdown_docs\").load_data()\n\n# Initialize DuckDB Vector Store\nvector_store = DuckDBVectorStore(\"open.duckdb\", persist_dir=\"./persist/\")\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n# Create the index\n# This will now automatically use the Qwen embeddings defined in Settings\nindex = VectorStoreIndex.from_documents(\n    documents, \n    storage_context=storage_context\n)\n\n\n\n\nTo perform semantic search, we need to store our text data as vectors (embeddings). We’ll use ChromaDB as our local vector store. We also need an embedding model to convert text into vectors. Here, we configure a custom OpenRouterEmbeddings class to use the qwen/qwen3-embedding-8b model via the OpenRouter API.\n\n\nCode\nfrom openai import OpenAI\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_chroma import Chroma\nfrom typing import List\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Custom embeddings class for OpenRouter API\nclass OpenRouterEmbeddings(Embeddings):\n    \"\"\"Custom embeddings class for OpenRouter API.\"\"\"\n    \n    def __init__(self, api_key: str, model: str = \"text-embedding-3-small\"):\n        self.client = OpenAI(\n            base_url=\"https://openrouter.ai/api/v1\",\n            api_key=api_key,\n        )\n        self.model = model\n    \n    def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:\n        \"\"\"Embed a list of documents.\"\"\"\n        response = self.client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self.model,\n            input=texts,\n            encoding_format=\"float\"\n        )\n        return [item.embedding for item in response.data]\n    \n    def embed_query(self, text: str) -&gt; List[float]:\n        \"\"\"Embed a single query.\"\"\"\n        response = self.client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self.model,\n            input=text,\n            encoding_format=\"float\"\n        )\n        return response.data[0].embedding\n\n# Get OpenRouter API key\nopenrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\nif not openrouter_api_key:\n    raise ValueError(\"OPENROUTER_API_KEY not found in environment variables\")\n\n# Create embeddings instance using OpenRouter\nembeddings = OpenRouterEmbeddings(\n    api_key=openrouter_api_key,\n    model=\"qwen/qwen3-embedding-8b\"\n)\n\n# Define vector store location\npersist_directory = \"chroma_db_data\"\n\n\nWith our store initialized, we can now ingest the data. We iterate through the markdown files we saved earlier. For each file, we: 1. Load the content. 2. Split the content into smaller chunks (approx. 2000 characters) using RecursiveCharacterTextSplitter. 3. Create a new Chroma vector store from these chunks.\nThis process builds the index that we’ll search against.\n\n\nCode\nfrom langchain_core.documents import Document\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nimport shutil\n\n# Helper function to load markdown files\ndef load_markdown_files(directory: str) -&gt; list[Document]:\n    \"\"\"Load all markdown files from directory and create Document objects.\"\"\"\n    documents = []\n    if not os.path.exists(directory):\n        return documents\n        \n    for filename in os.listdir(directory):\n        if filename.endswith('.md'):\n            filepath = os.path.join(directory, filename)\n            with open(filepath, 'r', encoding='utf-8') as f:\n                content = f.read()\n                doc = Document(\n                    page_content=content,\n                    metadata={\n                        \"source\": filename,\n                        \"filepath\": filepath\n                    }\n                )\n                documents.append(doc)\n    return documents\n\n# Create output directory for markdown files\noutput_dir = \"markdown_docs\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Convert each URL to markdown and save\nfor i, link_url in enumerate(links_full, 1):\n    try:\n        print(f\"Processing {i}/{len(links_full)}: {link_url}\")\n        html_content = fetch_html(link_url)\n        markdown_content = html_to_markdown(html_content)\n        filename = sanitize_filename(link_url)\n        output_path = os.path.join(output_dir, filename)\n        save_markdown(markdown_content, output_path)\n        print(f\"  ✓ Saved to {output_path}\")\n    except Exception as e:\n        print(f\"  ✗ Error processing {link_url}: {str(e)}\")\n\n# Load markdown documents\ndocuments = load_markdown_files(output_dir)\nprint(f\"\\nLoaded {len(documents)} markdown documents\")\n\n# Split documents into chunks\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=2000,\n    chunk_overlap=200,\n    length_function=len,\n    is_separator_regex=False,\n)\n\nsplits = text_splitter.split_documents(documents)\nprint(f\"Split into {len(splits)} chunks\")\n\n\n\n\nCode\n# Remove existing database if it exists\nif os.path.exists(persist_directory):\n    print(f\"Removing existing database at {persist_directory}...\")\n    shutil.rmtree(persist_directory)\n\n# Create new vector store\nvectorstore = Chroma.from_documents(\n    documents=splits,\n    embedding=embeddings,\n    persist_directory=persist_directory\n)\n\nprint(f\"\\n✓ Successfully created ChromaDB with {len(splits)} chunks!\")\nprint(f\"✓ Database saved to: {persist_directory}\")\n\n\n\n\n\n\n\nRetrieval\n\nRPython DuckDBPython Chroma\n\n\nNow that our knowledge base is populated, we can test the retrieval system. We can ask a specific question, like “What are model variants?”, and query the store to see which text chunks are most relevant. This confirms that our embeddings and search are working correctly.\n\n\nCode\nstore_location &lt;- \"openrouter.duckdb\"\nstore &lt;- ragnar_store_connect(store_location)\n\ntext &lt;- \"What are model variants?\"\n\n#' # Retrieving Chunks\n#' Once the store is set up, retrieve the most relevant text chunks like this\n\nrelevant_chunks &lt;- ragnar_retrieve(store, text, top_k = 3)\ncat(\"Retrieved\", nrow(relevant_chunks), \"chunks:\\n\\n\")\n\n\nRetrieved 6 chunks:\n\n\nCode\nfor (i in seq_len(nrow(relevant_chunks))) {\n    cat(sprintf(\"--- Chunk %d ---\\n%s\\n\\n\", i, relevant_chunks$text[i]))\n}\n\n\n--- Chunk 1 ---\n[Zero Completion Insurance](/docs/features/zero-completion-insurance)\n  + [Provisioning API Keys](/docs/features/provisioning-api-keys)\n  + [App Attribution](/docs/app-attribution)\n* API Reference\n\n  + [Overview](/docs/api-reference/overview)\n  + [Streaming](/docs/api-reference/streaming)\n  + [Embeddings](/docs/api-reference/embeddings)\n  + [Limits](/docs/api-reference/limits)\n  + [Authentication](/docs/api-reference/authentication)\n  + [Parameters](/docs/api-reference/parameters)\n  + [Errors](/docs/api-reference/errors)\n  + Responses API\n  + beta.responses\n  + Analytics\n  + Credits\n  + Embeddings\n  + Generations\n  + Models\n  + Endpoints\n  + Parameters\n  + Providers\n  + API Keys\n  + O Auth\n  + Chat\n  + Completions\n* SDK Reference (BETA)\n\n  + [Python SDK](/docs/sdks/python)\n  + [TypeScript SDK](/docs/sdks/typescript)\n* Use Cases\n\n  + [BYOK](/docs/use-cases/byok)\n  + [Crypto API](/docs/use-cases/crypto-api)\n  + [OAuth PKCE](/docs/use-cases/oauth-pkce)\n  + [MCP Servers](/docs/use-cases/mcp-servers)\n  + [Organization Management](/docs/use-cases/organization-management)\n  + [For Providers](/docs/use-cases/for-providers)\n  + [Reasoning Tokens](/docs/use-cases/reasoning-tokens)\n  + [Usage Accounting](/docs/use-cases/usage-accounting)\n  + [User Tracking](/docs/use-cases/user-tracking)\n* Community\n\n  + [Frameworks and Integrations Overview](/docs/community/frameworks-and-integrations-overview)\n  + [Effect AI SDK](/docs/community/effect-ai-sdk)\n  + [Arize](/docs/community/arize)\n  + [LangChain](/docs/community/lang-chain)\n  + [LiveKit](/docs/community/live-kit)\n  + [Langfuse](/docs/community/langfuse)\n  + [Mastra](/docs/community/mastra)\n  + [OpenAI SDK](/docs/community/open-ai-sdk)\n  + [PydanticAI](/docs/community/pydantic-ai)\n  + [Vercel AI SDK](/docs/community/vercel-ai-sdk)\n  + [Xcode](/docs/community/xcode)\n  + [Zapier](/docs/community/zapier)\n  + [Discord](https://discord.gg/openrouter)\n\nLight\n\nOn this page\n\n* [Requests](#requests)\n* [Completions Request Format](#completions-request-format)\n* [Headers](#headers)\n* [Assistant Prefill](#assistant-prefill)\n* [Responses](#responses)\n* [CompletionsResponse Format](#completionsresponse-format)\n* [Finish Reason](#finish-reason)\n* [Querying Cost and Stats](#querying-cost-and-stats)\n\n[API Reference](/docs/api-reference/overview)\n\n\n\n--- Chunk 2 ---\n###### How frequently are new models added?\n\nWe work on adding models as quickly as we can. We often have partnerships with\nthe labs releasing models and can release models as soon as they are\navailable. If there is a model missing that you’d like OpenRouter to support, feel free to message us on\n[Discord](https://discord.gg/openrouter).\n\n###### What are model variants?\n\nVariants are suffixes that can be added to the model slug to change its behavior.\n\nStatic variants can only be used with specific models and these are listed in our [models api](https://openrouter.ai/api/v1/models).\n\n1. `:free` - The model is always provided for free and has low rate limits.\n2. `:beta` - The model is not moderated by OpenRouter.\n3. `:extended` - The model has longer than usual context length.\n4. `:exacto` - The model only uses OpenRouter-curated high-quality endpoints.\n5. `:thinking` - The model supports reasoning by default.\n\nDynamic variants can be used on all models and they change the behavior of how the request is routed or used.\n\n1. `:online` - All requests will run a query to extract web results that are attached to the prompt.\n2. `:nitro` - Providers will be sorted by throughput rather than the default sort, optimizing for faster response times.\n3. `:floor` - Providers will be sorted by price rather than the default sort, prioritizing the most cost-effective options.\n\n###### I am an inference provider, how can I get listed on OpenRouter?\n\nYou can read our requirements at the [Providers\npage](/docs/use-cases/for-providers). If you would like to contact us, the best\nplace to reach us is over email.\n\n###### What is the expected latency/response time for different models?\n\nFor each model on OpenRouter we show the latency (time to first token) and the token\nthroughput for all providers. You can use this to estimate how long requests\nwill take. If you would like to optimize for throughput you can use the\n`:nitro` variant to route to the fastest provider.\n\n\n\n--- Chunk 3 ---\n###### How frequently are new models added?\n\nWe work on adding models as quickly as we can. We often have partnerships with\nthe labs releasing models and can release models as soon as they are\navailable. If there is a model missing that you’d like OpenRouter to support, feel free to message us on\n[Discord](https://discord.gg/openrouter).\n\n###### What are model variants?\n\nVariants are suffixes that can be added to the model slug to change its behavior.\n\nStatic variants can only be used with specific models and these are listed in our [models api](https://openrouter.ai/api/v1/models).\n\n1. `:free` - The model is always provided for free and has low rate limits.\n2. `:beta` - The model is not moderated by OpenRouter.\n3. `:extended` - The model has longer than usual context length.\n4. `:exacto` - The model only uses OpenRouter-curated high-quality endpoints.\n5. `:thinking` - The model supports reasoning by default.\n\nDynamic variants can be used on all models and they change the behavior of how the request is routed or used.\n\n1. `:online` - All requests will run a query to extract web results that are attached to the prompt.\n2. `:nitro` - Providers will be sorted by throughput rather than the default sort, optimizing for faster response times.\n3. `:floor` - Providers will be sorted by price rather than the default sort, prioritizing the most cost-effective options.\n\n###### I am an inference provider, how can I get listed on OpenRouter?\n\nYou can read our requirements at the [Providers\npage](/docs/use-cases/for-providers). If you would like to contact us, the best\nplace to reach us is over email.\n\n###### What is the expected latency/response time for different models?\n\nFor each model on OpenRouter we show the latency (time to first token) and the token\nthroughput for all providers. You can use this to estimate how long requests\nwill take. If you would like to optimize for throughput you can use the\n`:nitro` variant to route to the fastest provider.\n\n\n\n--- Chunk 4 ---\n## The `models` parameter\n\nThe `models` parameter lets you automatically try other models if the primary model’s providers are down, rate-limited, or refuse to reply due to content moderation.\n\nTypeScript SDKTypeScript (fetch)Python\n\n```code-block-root not-prose rounded-b-[inherit] rounded-t-none\n|  |  |\n| --- | --- |\n| 1 | import { OpenRouter } from '@openrouter/sdk'; |\n| 2 |  |\n| 3 | const openRouter = new OpenRouter({ |\n| 4 | apiKey: '&lt;OPENROUTER_API_KEY&gt;', |\n| 5 | }); |\n| 6 |  |\n| 7 | const completion = await openRouter.chat.send({ |\n| 8 | models: ['anthropic/claude-3.5-sonnet', 'gryphe/mythomax-l2-13b'], |\n| 9 | messages: [ |\n| 10 | { |\n| 11 | role: 'user', |\n| 12 | content: 'What is the meaning of life?', |\n| 13 | }, |\n| 14 | ], |\n| 15 | }); |\n| 16 |  |\n| 17 | console.log(completion.choices[0].message.content); |\n```\n\nIf the model you selected returns an error, OpenRouter will try to use the fallback model instead. If the fallback model is down or returns an error, OpenRouter will return that error.\n\nBy default, any error can trigger the use of a fallback model, including context length validation errors, moderation flags for filtered models, rate-limiting, and downtime.\n\nRequests are priced using the model that was ultimately used, which will be returned in the `model` attribute of the response body.\n\n## Using with OpenAI SDK\n\nTo use the `models` array with the OpenAI SDK, include it in the `extra_body` parameter. In the example below, gpt-4o will be tried first, and the `models` array will be tried in order as fallbacks.\n\nPythonTypeScript\n\n\n\n--- Chunk 5 ---\n[Web Search](/docs/features/web-search)\n  + [Zero Completion Insurance](/docs/features/zero-completion-insurance)\n  + [Provisioning API Keys](/docs/features/provisioning-api-keys)\n  + [App Attribution](/docs/app-attribution)\n* API Reference\n\n  + [Overview](/docs/api-reference/overview)\n  + [Streaming](/docs/api-reference/streaming)\n  + [Embeddings](/docs/api-reference/embeddings)\n  + [Limits](/docs/api-reference/limits)\n  + [Authentication](/docs/api-reference/authentication)\n  + [Parameters](/docs/api-reference/parameters)\n  + [Errors](/docs/api-reference/errors)\n  + Responses API\n  + beta.responses\n  + Analytics\n  + Credits\n  + Embeddings\n  + Generations\n  + Models\n  + Endpoints\n  + Parameters\n  + Providers\n  + API Keys\n  + O Auth\n  + Chat\n  + Completions\n* SDK Reference (BETA)\n\n  + [Python SDK](/docs/sdks/python)\n  + [TypeScript SDK](/docs/sdks/typescript)\n* Use Cases\n\n  + [BYOK](/docs/use-cases/byok)\n  + [Crypto API](/docs/use-cases/crypto-api)\n  + [OAuth PKCE](/docs/use-cases/oauth-pkce)\n  + [MCP Servers](/docs/use-cases/mcp-servers)\n  + [Organization Management](/docs/use-cases/organization-management)\n  + [For Providers](/docs/use-cases/for-providers)\n  + [Reasoning Tokens](/docs/use-cases/reasoning-tokens)\n  + [Usage Accounting](/docs/use-cases/usage-accounting)\n  + [User Tracking](/docs/use-cases/user-tracking)\n* Community\n\n  + [Frameworks and Integrations Overview](/docs/community/frameworks-and-integrations-overview)\n  + [Effect AI SDK](/docs/community/effect-ai-sdk)\n  + [Arize](/docs/community/arize)\n  + [LangChain](/docs/community/lang-chain)\n  + [LiveKit](/docs/community/live-kit)\n  + [Langfuse](/docs/community/langfuse)\n  + [Mastra](/docs/community/mastra)\n  + [OpenAI SDK](/docs/community/open-ai-sdk)\n  + [PydanticAI](/docs/community/pydantic-ai)\n  + [Vercel AI SDK](/docs/community/vercel-ai-sdk)\n  + [Xcode](/docs/community/xcode)\n  + [Zapier](/docs/community/zapier)\n  + [Discord](https://discord.gg/openrouter)\n\nLight\n\nOn this page\n\n* [Within OpenRouter](#within-openrouter)\n* [Provider Policies](#provider-policies)\n* [Training on Prompts](#training-on-prompts)\n* [Data Retention & Logging](#data-retention--logging)\n* [Enterprise EU in-region routing](#enterprise-eu-in-region-routing)\n\n[Features](/docs/features/privacy-and-logging)\n\n\n\n--- Chunk 6 ---\n[Web Search](/docs/features/web-search)\n  + [Zero Completion Insurance](/docs/features/zero-completion-insurance)\n  + [Provisioning API Keys](/docs/features/provisioning-api-keys)\n  + [App Attribution](/docs/app-attribution)\n* API Reference\n\n  + [Overview](/docs/api-reference/overview)\n  + [Streaming](/docs/api-reference/streaming)\n  + [Embeddings](/docs/api-reference/embeddings)\n  + [Limits](/docs/api-reference/limits)\n  + [Authentication](/docs/api-reference/authentication)\n  + [Parameters](/docs/api-reference/parameters)\n  + [Errors](/docs/api-reference/errors)\n  + Responses API\n  + beta.responses\n  + Analytics\n  + Credits\n  + Embeddings\n  + Generations\n  + Models\n  + Endpoints\n  + Parameters\n  + Providers\n  + API Keys\n  + O Auth\n  + Chat\n  + Completions\n* SDK Reference (BETA)\n\n  + [Python SDK](/docs/sdks/python)\n  + [TypeScript SDK](/docs/sdks/typescript)\n* Use Cases\n\n  + [BYOK](/docs/use-cases/byok)\n  + [Crypto API](/docs/use-cases/crypto-api)\n  + [OAuth PKCE](/docs/use-cases/oauth-pkce)\n  + [MCP Servers](/docs/use-cases/mcp-servers)\n  + [Organization Management](/docs/use-cases/organization-management)\n  + [For Providers](/docs/use-cases/for-providers)\n  + [Reasoning Tokens](/docs/use-cases/reasoning-tokens)\n  + [Usage Accounting](/docs/use-cases/usage-accounting)\n  + [User Tracking](/docs/use-cases/user-tracking)\n* Community\n\n  + [Frameworks and Integrations Overview](/docs/community/frameworks-and-integrations-overview)\n  + [Effect AI SDK](/docs/community/effect-ai-sdk)\n  + [Arize](/docs/community/arize)\n  + [LangChain](/docs/community/lang-chain)\n  + [LiveKit](/docs/community/live-kit)\n  + [Langfuse](/docs/community/langfuse)\n  + [Mastra](/docs/community/mastra)\n  + [OpenAI SDK](/docs/community/open-ai-sdk)\n  + [PydanticAI](/docs/community/pydantic-ai)\n  + [Vercel AI SDK](/docs/community/vercel-ai-sdk)\n  + [Xcode](/docs/community/xcode)\n  + [Zapier](/docs/community/zapier)\n  + [Discord](https://discord.gg/openrouter)\n\nLight\n\nOn this page\n\n* [How OpenRouter Manages Data Policies](#how-openrouter-manages-data-policies)\n* [Per-Request ZDR Enforcement](#per-request-zdr-enforcement)\n* [Usage](#usage)\n* [Caching](#caching)\n* [OpenRouter’s Retention Policy](#openrouters-retention-policy)\n* [Zero Retention Endpoints](#zero-retention-endpoints)\n\n[Features](/docs/features/privacy-and-logging)\n\n\n\n\nCode\n# ragnar_store_inspect(store)\n#ragnar_chunks_view(chunks)\n\n\n\n\nIn Python, we can use LlamaIndex to interact with our DuckDB vector store. In this step, we’ll configure the embedding model and retrieve the top relevant chunks for our query, saving them to a file for inspection. We won’t use an LLM for generation yet, focusing solely on verifying the retrieval quality.\n\n\nCode\nimport os\nimport sys\nprint(f\"Python executable: {sys.executable}\")\n\n\nPython executable: /Library/Frameworks/Python.framework/Versions/3.13/bin/python3\n\n\nCode\nprint(f\"Python path: {sys.path}\")\n\n\nPython path: ['', '/Library/Frameworks/Python.framework/Versions/3.13/bin', '/Library/Frameworks/Python.framework/Versions/3.13/lib/python313.zip', '/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13', '/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/lib-dynload', '/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages', '/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/reticulate/python']\n\n\nCode\nfrom typing import Any, List\nfrom openai import OpenAI\nfrom llama_index.core import VectorStoreIndex, Settings\nfrom llama_index.core.embeddings import BaseEmbedding\nfrom llama_index.vector_stores.duckdb import DuckDBVectorStore\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n\nTrue\n\n\nCode\n# Ensure your API key is available\nopenrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n\n# Custom OpenRouter Embedding class for LlamaIndex\nclass OpenRouterEmbedding(BaseEmbedding):\n    \"\"\"Custom embedding class for OpenRouter API compatible with LlamaIndex.\"\"\"\n    \n    def __init__(\n        self,\n        api_key: str,\n        model: str = \"qwen/qwen3-embedding-8b\",\n        **kwargs: Any\n    ):\n        super().__init__(**kwargs)\n        self._client = OpenAI(\n            base_url=\"https://openrouter.ai/api/v1\",\n            api_key=api_key,\n        )\n        self._model = model\n    \n    def _get_query_embedding(self, query: str) -&gt; List[float]:\n        \"\"\"Get embedding for a query string.\"\"\"\n        response = self._client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self._model,\n            input=query,\n            encoding_format=\"float\"\n        )\n        return response.data[0].embedding\n    \n    def _get_text_embedding(self, text: str) -&gt; List[float]:\n        \"\"\"Get embedding for a text string.\"\"\"\n        return self._get_query_embedding(text)\n    \n    async def _aget_query_embedding(self, query: str) -&gt; List[float]:\n        \"\"\"Async version of get_query_embedding.\"\"\"\n        return self._get_query_embedding(query)\n    \n    async def _aget_text_embedding(self, text: str) -&gt; List[float]:\n        \"\"\"Async version of get_text_embedding.\"\"\"\n        return self._get_text_embedding(text)\n\n# 1. Configure Embedding Model using custom OpenRouter class\nembed_model = OpenRouterEmbedding(\n    api_key=openrouter_api_key,\n    model=\"qwen/qwen3-embedding-8b\"\n)\n\n# 2. Apply Settings\nSettings.embed_model = embed_model\n\n# 3. Load and Retrieve\n# Load the existing DuckDB vector store\nvector_store = DuckDBVectorStore(database_name=\"openrouter.duckdb\", persist_dir=\"./persist/\")\nindex = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n\n# Retrieve top 3 chunks\nretriever = index.as_retriever(similarity_top_k=3)\nnodes = retriever.retrieve(\"What are model variants?\")\n\n# Save retrieved chunks to a markdown file for easy inspection\nwith open(\"retriever.md\", \"w\", encoding=\"utf-8\") as f:\n    f.write(f\"# Retrieved {len(nodes)} chunks\\n\\n\")\n    for i, node in enumerate(nodes, 1):\n        content = f\"## Chunk {i}\\n\\n{node.text}\\n\\n\"\n        print(content)\n        f.write(content)\n\n\n22\n\n\n\n\nNow that our knowledge base is populated, we can test the retrieval system. We can ask a specific question, like “What are model variants?”, and query the Chroma store to see which text chunks are most relevant. This confirms that our embeddings and search are working correctly.\n\n\nCode\nfrom openai import OpenAI\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_chroma import Chroma\nfrom typing import List\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\nTrue\n\n\nCode\n# Custom embeddings class for OpenRouter API\nclass OpenRouterEmbeddings(Embeddings):\n    \"\"\"Custom embeddings class for OpenRouter API.\"\"\"\n    \n    def __init__(self, api_key: str, model: str = \"qwen/qwen3-embedding-8b\"):\n        self.client = OpenAI(\n            base_url=\"https://openrouter.ai/api/v1\",\n            api_key=api_key,\n        )\n        self.model = model\n    \n    def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:\n        \"\"\"Embed a list of documents.\"\"\"\n        response = self.client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self.model,\n            input=texts,\n            encoding_format=\"float\"\n        )\n        return [item.embedding for item in response.data]\n    \n    def embed_query(self, text: str) -&gt; List[float]:\n        \"\"\"Embed a single query.\"\"\"\n        response = self.client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self.model,\n            input=text,\n            encoding_format=\"float\"\n        )\n        return response.data[0].embedding\n\n# Get OpenRouter API key\nopenrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\nif not openrouter_api_key:\n    raise ValueError(\"OPENROUTER_API_KEY not found in environment variables\")\n\n# Create embeddings instance using OpenRouter\nembeddings = OpenRouterEmbeddings(\n    api_key=openrouter_api_key,\n    model=\"qwen/qwen3-embedding-8b\"\n)\n\n# Define vector store location\npersist_directory = \"chroma_db_data\"\n\n# Load existing vector store\nvectorstore = Chroma(\n    persist_directory=persist_directory,\n    embedding_function=embeddings\n)\n\n# Test query\nquery = \"What are model variants?\"\n\n# Perform similarity search\nresults = vectorstore.similarity_search(query, k=5)\n\nprint(f\"\\nQuery: '{query}'\")\n\n\n\nQuery: 'What are model variants?'\n\n\nCode\nprint(f\"Found {len(results)} relevant chunks:\\n\")\n\n\nFound 5 relevant chunks:\n\n\nCode\nfor i, doc in enumerate(results, 1):\n    print(f\"Result {i}:\")\n    print(f\"Source: {doc.metadata.get('source', 'unknown')}\")\n    print(f\"Content preview: {doc.page_content[:800]}...\")\n\n\nResult 1:\nSource: docs_faq_how-are-rate-limits-calculated.md\nContent preview: ###### What are model variants?\n\nVariants are suffixes that can be added to the model slug to change its behavior.\n\nStatic variants can only be used with specific models and these are listed in our [models api](https://openrouter.ai/api/v1/models).\n\n1. `:free` - The model is always provided for free and has low rate limits.\n2. `:beta` - The model is not moderated by OpenRouter.\n3. `:extended` - The model has longer than usual context length.\n4. `:exacto` - The model only uses OpenRouter-curated high-quality endpoints.\n5. `:thinking` - The model supports reasoning by default.\n\nDynamic variants can be used on all models and they change the behavior of how the request is routed or used.\n\n1. `:online` - All requests will run a query to extract web results that are attached to the prompt.\n2. `:...\nResult 2:\nSource: docs_faq.md\nContent preview: ###### What are model variants?\n\nVariants are suffixes that can be added to the model slug to change its behavior.\n\nStatic variants can only be used with specific models and these are listed in our [models api](https://openrouter.ai/api/v1/models).\n\n1. `:free` - The model is always provided for free and has low rate limits.\n2. `:beta` - The model is not moderated by OpenRouter.\n3. `:extended` - The model has longer than usual context length.\n4. `:exacto` - The model only uses OpenRouter-curated high-quality endpoints.\n5. `:thinking` - The model supports reasoning by default.\n\nDynamic variants can be used on all models and they change the behavior of how the request is routed or used.\n\n1. `:online` - All requests will run a query to extract web results that are attached to the prompt.\n2. `:...\nResult 3:\nSource: docs_use-cases_crypto-api.md\nContent preview: [API](/docs/api-reference/overview)[Models](https://openrouter.ai/models)[Chat](https://openrouter.ai/chat)[Ranking](https://openrouter.ai/rankings)...\nResult 4:\nSource: docs_sdks_typescript.md\nContent preview: [API](/docs/api-reference/overview)[Models](https://openrouter.ai/models)[Chat](https://openrouter.ai/chat)[Ranking](https://openrouter.ai/rankings)...\nResult 5:\nSource: docs_features_provider-routing.md\nContent preview: Route requests through OpenRouter-curated providers\n\nNext](/docs/features/exacto-variant)[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=openrouter.ai)\n\n[![Logo](https://files.buildwithfern.com/openrouter.docs.buildwithfern.com/docs/2025-11-21T16:36:36.134Z/content/assets/logo.svg)![Logo](https://files.buildwithfern.com/openrouter.docs.buildwithfern.com/docs/2025-11-21T16:36:36.134Z/content/assets/logo-white.svg)](https://openrouter.ai/)\n\n[API](/docs/api-reference/overview)[Models](https://openrouter.ai/models)[Chat](https://openrouter.ai/chat)[Ranking](https://openrouter.ai/rankings)...\n\n\n\n\n\n\n\nChat with RAG\n\nRPython chatlasPython LangChain\n\n\nThe final piece is to connect this retrieval capability to a chat interface. We use ellmer to create a chat client. Crucially, we register a “retrieval tool” using ragnar_register_tool_retrieve. This gives the LLM the ability to query our vector store whenever it needs information to answer a user’s question.\nWe also provide a system prompt that instructs the model to always check the knowledge base and cite its sources.\n\n\nCode\nlibrary(ellmer)\nlibrary(dotenv)\nload_dot_env(file = \".env\")\n\nchat &lt;- chat_openrouter(\n    api_key = Sys.getenv(\"OPENROUTER_API_KEY\"),\n    model = \"openai/gpt-oss-120b\",\n    system_prompt = glue::trim(\"\n  You are an assistant for question-answering tasks. You are concise.\n\n  Before responding, retrieve relevant material from the knowledge store. Quote or\n  paraphrase passages, clearly marking your own words versus the source. Provide a\n  working link for every source cited, as well as any additional relevant links.\n  Do not answer unless you have retrieved and cited a source.If you do not find\n  relevant information, say 'I could not find anything relevant in the knowledge base\n    \")\n) |&gt;\n    ragnar_register_tool_retrieve(store, top_k = 3)\n\n\n\nCode\nchat$chat(\"What are model variants?\")\n\nModel variants are suffixes you can attach to a model’s slug to modify how that model behaves when you call it through OpenRouter.\nThere are two kinds:\n\n\n\n\n\n\n\n\nVariant\nHow it works\nExample use\n\n\n\n\nStatic variants – only work with certain models (they are listed in\n\n\n\n\n\nthe models API). They change the model’s intrinsic properties. • :free – always provided at no cost, but with low rate limits. • :beta – bypasses OpenRouter‑side moderation. • :extended – gives the model a longer context window than usual. • :exacto – forces use of OpenRouter‑curated high‑quality endpoints only. • :thinking – enables built‑in reasoning support. | | Dynamic variants – can be used with any model. They affect how the request is routed or processed. • :online – runs a web‑search query and appends the results to the prompt. • :nitro – sorts providers by throughput, favoring the fastest response. • :floor – sorts providers by price, favoring the cheapest option. |\nThus, by appending one of these suffixes (e.g., gpt-4o:free or claude-3.5-sonnet:online), you tell OpenRouter to apply the corresponding behavior to that request.\nSource: OpenRouter FAQ – “What are model variants?”【https://openrouter.ai/docs/faq】\n\n\nWe can also use the chatlas library to create a chat interface. Here, we define a custom tool retrieve_trusted_content that queries our DuckDB index. We then register this tool with the chat model, allowing it to pull in relevant information when answering user questions.\n\n\nCode\nimport os\nimport chatlas as ctl\nfrom llama_index.core import VectorStoreIndex, Settings\nfrom llama_index.vector_stores.duckdb import DuckDBVectorStore\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom dotenv import load_dotenv\nload_dotenv()\n\n\nTrue\n\n\nCode\n# Ensure API key\nopenrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n\n# 1. Configure Embedding Model (LlamaIndex)\nembed_model = OpenAIEmbedding(\n    api_key=openrouter_api_key,\n    base_url=\"https://openrouter.ai/api/v1\",\n    model=\"text-embedding-3-small\"  # Use standard OpenAI model name; OpenRouter requires this\n)\nSettings.embed_model = embed_model\n\n# 2. Load Vector Store\nvector_store = DuckDBVectorStore(database_name=\"open.duckdb\", persist_dir=\"./persist/\")\nindex = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n\n# 3. Define Retrieval Tool\ndef retrieve_trusted_content(query: str, top_k: int = 5):\n    \"\"\"\n    Retrieve relevant content from the knowledge store.\n\n    Parameters\n    ----------\n    query\n        The query used to semantically search the knowledge store.\n    top_k\n        The number of results to retrieve from the knowledge store.\n    \"\"\"\n    #print(f\"Retrieving content for query: '{query}'\")\n    retriever = index.as_retriever(similarity_top_k=top_k)\n    nodes = retriever.retrieve(query)\n    return [f\"&lt;excerpt&gt;{x.text}&lt;/excerpt&gt;\" for x in nodes]\n\n# 4. Initialize Chat with System Prompt\nchat = ctl.ChatOpenAI(\n    model=\"openai/gpt-oss-120b\",\n    api_key=openrouter_api_key,\n    base_url=\"https://openrouter.ai/api/v1\",\n    system_prompt=(\n        \"You are an assistant for question-answering tasks. \"\n        \"Use the retrieve_trusted_content tool to find relevant information from the knowledge store. \"\n        \"Answer questions based on the retrieved content. \"\n        \"If you cannot find relevant information, say so clearly.\"\n    )\n)\n\nchat.register_tool(retrieve_trusted_content)\n\n\n\n\nCode\nresponse=chat.chat(\"What are model variants?\", echo=\"none\")\n\n\n\n\nCode\nprint(response)\n\n\n**Model variants** are alternative versions of a base machine‑learning model that share the same overall architecture but differ in one or more of the following attributes:\n\n| Attribute that can differ | What it means for the variant |\n|---------------------------|--------------------------------|\n| **Number of parameters / layers** | Smaller variants (e.g., “base”, “small”, “medium”) have fewer weights, run faster, and cost less to use, while larger variants (e.g., “large”, “XL”) tend to be more accurate but are slower and more expensive. |\n| **Training data** | Some variants are trained on a broader or more recent corpus, while others might be trained on domain‑specific data (e.g., legal texts, code, biomedical literature). |\n| **Fine‑tuning / instruction‑tuning** | A base model can be further fine‑tuned for a particular task (e.g., chat, summarisation, code generation) or to follow user instructions more reliably. The resulting model is a *fine‑tuned variant*. |\n| **Safety/Alignment filters** | Certain variants incorporate additional alignment or content‑filtering steps to reduce harmful or biased outputs. |\n| **Context‑window size** | Some variants are engineered to accept longer input sequences (e.g., 32 k tokens vs. the standard 8 k). |\n| **Output format** | Variants may be specialised for embeddings, token‑level classification, function‑calling APIs, or other downstream formats. |\n| **Hardware optimisation** | A variant might be quantised (e.g., 8‑bit) or otherwise optimised for specific hardware (GPU, CPU, mobile). |\n| **Pricing tier** | In commercial APIs, different variants are often exposed as separate pricing tiers (e.g., *gpt‑3.5‑turbo*, *gpt‑4‑turbo*, *gpt‑4‑32k*). |\n\n### Why create variants?\n\n1. **Trade‑off between cost and performance** – Smaller variants let users run inference cheaply and quickly; larger variants give higher accuracy when that matters.\n2. **Specialisation** – Fine‑tuning on a particular domain or task yields a model that outperforms the generic base model for that use‑case.\n3. **Safety & compliance** – Adding alignment or content‑moderation layers helps meet regulatory or product‑safety requirements.\n4. **Technical constraints** – Longer context windows or quantised weights enable use‑cases (e.g., processing whole documents, running on edge devices) that the original model cannot handle efficiently.\n\n### Common real‑world examples\n\n| Platform | Base model | Notable variants |\n|----------|------------|-------------------|\n| OpenAI   | GPT‑4      | **gpt‑4‑turbo** (faster, cheaper, same quality), **gpt‑4‑32k** (longer context), **gpt‑4‑code‑interpreter** (adds execution capability) |\n| OpenAI   | GPT‑3.5    | **gpt‑3.5‑turbo** (optimised for chat), **gpt‑3.5‑code‑davinci** (code‑focused) |\n| Anthropic| Claude     | **Claude‑2**, **Claude‑2‑Instant** (faster, lower‑cost) |\n| Meta     | LLaMA      | **LLaMA‑7B**, **LLaMA‑13B**, **LLaMA‑30B**, **LLaMA‑70B** (size variants), plus instruction‑tuned “LLaMA‑Chat” variants |\n| Cohere   | Command    | **Command‑R** (retrieval‑augmented), **Command‑Light** (smaller, cheaper) |\n\n### How to think about them\n\n- **All variants stem from the same underlying architecture.** If you think of the base model as a “template,” a variant is a concrete instance of that template with a specific configuration.\n- **Choosing a variant is a matter of matching requirements.** If your application needs sub‑second responses on cheap hardware, you’d pick a small, fast variant. If you need the highest possible reasoning ability on long documents, you’d choose a larger, longer‑context variant, possibly with safety filters turned on or off depending on your risk tolerance.\n- **Variants can be stacked.** A model can first be quantised (hardware optimisation) *and* instruction‑tuned (task specialization), yielding a variant that is both fast and better at following instructions.\n\nIn short, **model variants are purpose‑built versions of a core model, each tuned—by size, data, training objective, safety controls, or hardware optimisation—to serve a particular trade‑off or use‑case.**\n\n\n\n\nThe final piece is to connect this retrieval capability to a chat interface. We use LangChain to create a RAG chain. We create a retriever from our vector store and combine it with a ChatOpenAI model (using OpenRouter) and a prompt template. This gives the LLM the ability to query our vector store whenever it needs information to answer a user’s question.\n\n\nCode\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\nfrom openai import OpenAI\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_chroma import Chroma\nfrom typing import List\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\nTrue\n\n\nCode\n# Custom embeddings class for OpenRouter API\nclass OpenRouterEmbeddings(Embeddings):\n    \"\"\"Custom embeddings class for OpenRouter API.\"\"\"\n    \n    def __init__(self, api_key: str, model: str = \"qwen/qwen3-embedding-8b\"):\n        self.client = OpenAI(\n            base_url=\"https://openrouter.ai/api/v1\",\n            api_key=api_key,\n        )\n        self.model = model\n    \n    def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:\n        \"\"\"Embed a list of documents.\"\"\"\n        response = self.client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self.model,\n            input=texts,\n            encoding_format=\"float\"\n        )\n        return [item.embedding for item in response.data]\n    \n    def embed_query(self, text: str) -&gt; List[float]:\n        \"\"\"Embed a single query.\"\"\"\n        response = self.client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self.model,\n            input=text,\n            encoding_format=\"float\"\n        )\n        return response.data[0].embedding\n\n# Get OpenRouter API key\nopenrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\nif not openrouter_api_key:\n    raise ValueError(\"OPENROUTER_API_KEY not found in environment variables\")\n\n# Create embeddings instance using OpenRouter\nembeddings = OpenRouterEmbeddings(\n    api_key=openrouter_api_key,\n    model=\"qwen/qwen3-embedding-8b\"\n)\n\n# Define vector store location\npersist_directory = \"chroma_db_data\"\n\n# Load existing vector store\nprint(f\"Loading existing vectorstore from {persist_directory}...\")\n\n\nLoading existing vectorstore from chroma_db_data...\n\n\nCode\nvectorstore = Chroma(\n    persist_directory=persist_directory,\n    embedding_function=embeddings\n)\nprint(f\"✓ Loaded vectorstore\")\n\n\n✓ Loaded vectorstore\n\n\nCode\n# Initialize LLM using OpenRouter\nllm = ChatOpenAI(\n    model=\"openai/gpt-oss-120b\",\n    openai_api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n    openai_api_base=\"https://openrouter.ai/api/v1\"\n)\n\n# Create prompt template\nsystem_prompt = (\n    \"You are an assistant for question-answering tasks. \"\n    \"Use the following pieces of retrieved context to answer \"\n    \"the question. If you don't know the answer, say that you \"\n    \"don't know. Use three sentences maximum and keep the \"\n    \"answer concise.\"\n    \"\\n\\n\"\n    \"Context: {context}\"\n    \"\\n\\n\"\n    \"Question: {question}\"\n)\n\nprompt = ChatPromptTemplate.from_template(system_prompt)\n\n# Create retriever\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n\n# Helper function to format documents\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n# Create RAG chain using LCEL\nrag_chain = (\n    {\n        \"context\": retriever | format_docs,\n        \"question\": RunnablePassthrough()\n    }\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\nprint(\"✓ RAG chain created successfully!\")\n\n\n✓ RAG chain created successfully!\n\n\nCode\n# Test the RAG chain\nquestion = \"What are model variants?\"\n\nprint(f\"\\nQuestion: {question}\")\n\n\n\nQuestion: What are model variants?\n\n\nCode\n# Get context documents separately for display\ncontext_docs = retriever.invoke(question)\n\n# Invoke the RAG chain\nanswer = rag_chain.invoke(question)\nimport textwrap\n\nfor line in answer.split('\\n'):\n    print(textwrap.fill(line, width=80))\n\n\nModel variants are suffixes you append to a model’s slug to modify its behavior.\nStatic variants (e.g., `:free`, `:beta`, `:extended`, `:exacto`, `:thinking`)\napply only to specific models, while dynamic variants\n(e.g., `:online`, `:nitro`, `:floor`) work with any model and affect routing or\nusage. These modifiers let you control factors like cost, moderation, context\nlength, reasoning support, web‑search integration, speed, or price."
  },
  {
    "objectID": "posts/openrouter/index.html",
    "href": "posts/openrouter/index.html",
    "title": "OpenRouter: A Unified API for Multiple AI Models",
    "section": "",
    "text": "OpenRouter is a powerful platform that provides a unified API interface for accessing multiple AI models from various providers. Instead of integrating with each AI service separately, developers can use OpenRouter’s single endpoint to access models from OpenAI, Anthropic, Google, and many others.\n\n\nGetting started with OpenRouter is straightforward and can be accomplished in just a few steps. The platform is designed to minimize setup time while maintaining security best practices.\n\n\n\nFirst, visit openrouter.ai and create an account. The registration process is simple:\n\nSign Up: Use your email or social login (Google, GitHub)\nVerify Email: Confirm your email address through the verification link\nAccess Dashboard: Navigate to your dashboard where you’ll find your API key\n\n💡 Pro Tip: Your dashboard provides valuable insights including: - Usage statistics and cost tracking - Model performance metrics - API key management - Billing information\n\n\n\nSecurity is paramount when working with AI APIs. Never hardcode your API keys directly in your code. Instead, use environment variables to keep your credentials safe:\n\n\n\nSecurity: Prevents accidental exposure in version control\nFlexibility: Allows different keys for development, staging, and production\nCollaboration: Team members can use their own keys without sharing\nDeployment: Easy management across different hosting environments\n\n\n\n\nCreate a .env file in your project root:\nOPENROUTER_API_KEY=your_actual_api_key_here\nInstall the python-dotenv library to load environment variables:\npip install python-dotenv\n\n\n\n\nNow that you have your API key set up, let’s make your first API call. OpenRouter cleverly uses the same API format as OpenAI, which means you can use the familiar openai Python library - just with a different base URL.\n\n\n🔧 How It Works: OpenRouter acts as a smart proxy that: 1. Receives your standardized API requests 2. Routes them to the appropriate AI model provider 3. Handles provider-specific authentication and formatting 4. Returns responses in a consistent format 5. Tracks usage and costs across all models\n\n\n\nBefore you start, ensure you have the necessary Python packages:\npip install openai python-dotenv\n\nopenai: The official OpenAI Python client (compatible with OpenRouter)\npython-dotenv: For loading environment variables from .env files\n\n\n\n\n\npython with OpenAI packagepython with chatlas packageR with ellmer package\n\n\n\n\nCode\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize the OpenAI client with OpenRouter\nclient = OpenAI(\n  base_url=\"https://openrouter.ai/api/v1\",  # OpenRouter's API endpoint\n  api_key=os.getenv(\"OPENROUTER_API_KEY\"),  # Your secure API key\n)\n\n# Create a chat completion with OpenRouter\ncompletion = client.chat.completions.create(\n  extra_headers={\n    \"HTTP-Referer\": \"https://your-site.com\",  # Optional: Helps OpenRouter improve their service\n    \"X-Title\": \"Your Site Name\",             # Optional: Your site name for OpenRouter rankings\n  },\n  model=\"openai/gpt-oss-20b:free\",          # Free model for testing/development\n  messages=[\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello! Can you explain what OpenRouter is in simple terms?\"\n    }\n  ],\n  temperature=0.7  # Controls creativity (0.0 = deterministic, 1.0 = very creative)\n)\n\n# Extract and print the response\nprint(completion.choices[0].message.content)\n\n\n\n\n\n\nCode\nfrom chatlas import ChatOpenRouter\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize the OpenAI client with OpenRouter\nclient = ChatOpenRouter(api_key=os.getenv(\"OPENROUTER_API_KEY\")\n,base_url='https://openrouter.ai/api/v1'\n ,system_prompt=None\n ,model=\"openai/gpt-oss-20b:free\")\n\n\n\n\nCode\nresponse=client.chat(\"What is the capital of France?\")\n#str(response)\n\n\n\n\n\n\nCode\nlibrary(ellmer)\nlibrary(dotenv)\nload_dot_env(file = \".env\")\n\nchat &lt;- chat_openrouter(\n  system_prompt = NULL,\n  api_key = Sys.getenv(\"OPENROUTER_API_KEY\"),\n \n  model = \"openai/gpt-oss-20b:free\",\n  echo = \"none\"\n)\n\n\n\n\nCode\nchat$chat(\"Tell me three jokes about statisticians\")\n\n\n\n\n\nKey Components Explained:\n\nbase_url: Points to OpenRouter’s API instead of OpenAI’s\nmodel: Uses OpenRouter’s model naming format (provider/model-name)\nextra_headers: Optional but recommended for OpenRouter’s analytics\ntemperature: Controls response creativity (0.0-2.0 range)\nmessages: Standard chat format with role-based conversation structure\n\n💡 Model Selection Tips: - Free Models: Great for development (*free suffix) - Budget Models: Cost-effective for production (*:budget suffix) - Premium Models: Best performance (*, *:pro, *:latest) - Specialized: Task-optimized models (coding, math, creative writing)\n\n\n\n\nSystem prompts are powerful tools that shape the AI’s behavior, personality, and response style. They set the context and rules for the entire conversation, appearing before user messages in the conversation flow.\n\n\nSystem prompts act as meta-instructions that guide how the AI should respond throughout the conversation. They’re processed first and influence all subsequent interactions.\n\n\n\n\nConsistent Behavior: Ensures AI maintains the desired personality throughout\nOutput Format: Dictates response structure (JSON, markdown, code blocks)\nSafety Constraints: Sets boundaries and restrictions on responses\nContext Setting: Provides background information for better responses\nTask Specialization: Optimizes AI for specific use cases\n\n\n\n\n\n\nCode\n# Example with system prompt\ncompletion = client.chat.completions.create(\n  model=\"openai/gpt-oss-20b:free\",\n  messages=[\n    {\n      \"role\": \"system\",\n      \"content\": \"You are a helpful AI assistant that explains technical concepts in simple terms. Always be friendly and use analogies when possible and be simple\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"How does temperature in LLM model work?\"\n    }\n  ],\n  temperature=0.7\n)\n\nprint(completion.choices[0].message.content)\n\n\nCommon system prompt patterns:\n\n\nCode\n# Different system prompt examples\nsystem_prompts = {\n    \"coding_assistant\": \"You are an expert programmer. Provide clean, well-commented code solutions and explain your reasoning.\",\n    \"creative_writer\": \"You are a creative storyteller. Write engaging narratives with vivid descriptions and compelling characters.\",\n    \"data_analyst\": \"You are a data analyst. Provide insights based on data, suggest visualizations, and explain statistical concepts clearly.\",\n    \"tutor\": \"You are a patient tutor. Break down complex topics into simple steps and provide encouraging feedback.\"\n}\n\ndef chat_with_persona(persona, user_message):\n    completion = client.chat.completions.create(\n        model=\"openai/gpt-oss-20b:free\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompts[persona]},\n            {\"role\": \"user\", \"content\": user_message}\n        ],\n        temperature=0.7\n    )\n    return completion.choices[0].message.content\n\n# Example usage\n#response = chat_with_persona(\"coding_assistant\", \"How do I reverse a string in Python?\")\n#print(response)\n\n\n\n\n\n\nStreaming is a game-changer for user experience, especially in chat applications and interactive tools. Instead of waiting for complete responses, streaming delivers content as it’s being generated, creating natural and engaging conversations.\n\n\n🚀 User Experience Benefits: - Immediate Feedback: Users see responses starting immediately - Reduced Perceived Latency: Content appears as it’s generated - Natural Conversation Flow: Mimics human speech patterns - Progress Indication: Users know the AI is working - Early Termination: Users can stop lengthy responses if needed\n⚡ Technical Advantages: - Lower Memory Usage: No need to buffer complete responses - Faster Time-to-First-Byte: Content starts flowing immediately - Better Error Handling: Issues detected earlier in process - Resource Efficiency: Processes data incrementally\n\n\n\n\n\nCode\n# Initialize the OpenAI client (if not already done)\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nclient = OpenAI(\n  base_url=\"https://openrouter.ai/api/v1\",\n  api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n)\n\ndef stream_response(model, message):\n    stream = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": message}],\n        stream=True\n    )\n\n    for chunk in stream:\n        if chunk.choices[0].delta.content is not None:\n            print(chunk.choices[0].delta.content, end='', flush=True)\n\n# Stream example\nprint(\"Streaming response:\")\nstream_response(\"openai/gpt-oss-20b:free\", \"Tell me a short story about AI\")\nprint()  # Add newline after streaming\n\n\n\n\n\n\n\nCode\nfrom openai import OpenAI\nimport base64\nimport datetime\n\n\n\n\nCode\n# | eval: false\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize the OpenAI client with OpenRouter\nclient = OpenAI(\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n)\n\n\n\n\nCode\n# Image generation request\nresponse = client.chat.completions.create(\n    extra_headers={\n        \"HTTP-Referer\": \"www.tonydotdev.com\",\n        \"X-Title\": \"TT_AI_blog\",\n    },\n    model=\"google/gemini-2.5-flash-image\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Generate a serene and realistic snowy mountain landscape at sunrise.\",\n        }\n    ],\n    modalities=[\"image\", \"text\"],\n    #image_size=\"1024x1024\",\n)\n\n\n\n\nCode\nimport base64\nimport datetime\n# Extract message\nmessage = response.choices[0].message\n\n# Handle image output (base64 string inside \"data:image/png;base64,...\")\nif message.images:\n    data_url = message.images[0][\"image_url\"][\"url\"]\n\n    # Strip prefix if present\n    if data_url.startswith(\"data:image\"):\n        _, base64_data = data_url.split(\",\", 1)\n    else:\n        base64_data = data_url\n\n    # Decode and save as PNG\n    image_bytes = base64.b64decode(base64_data)\n    current_time = datetime.datetime.now().strftime(\"%H%M\")\n    output_file = f\"text_image_gemini_25_openrouter_{current_time}.png\"\n    with open(output_file, \"wb\") as f:\n        f.write(image_bytes)\n\n    print(f\"✅ Image saved as {output_file}\")\nelse:\n    print(\"❌ No image returned in response\")\n\n\n\n\nCode\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"text_image_gemini_25_openrouter_1352.png\"))\n\n\n&lt;IPython.core.display.Image object&gt;\n\n\n\n\n\n\n\nCode\nimport base64\n\n# Convert local image to data URL\nwith open(\"text_image_gemini_25_openrouter_1352.png\", \"rb\") as image_file:\n    base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n    data_url = f\"data:image/png;base64,{base64_image}\"\n\ncompletion = client.chat.completions.create(\n  extra_headers={\n    \"HTTP-Referer\": \"&lt;YOUR_SITE_URL&gt;\", # Optional. Site URL for rankings on openrouter.ai.\n    \"X-Title\": \"&lt;YOUR_SITE_NAME&gt;\", # Optional. Site title for rankings on openrouter.ai.\n  },\n  extra_body={},\n  model=\"nvidia/nemotron-nano-12b-v2-vl:free\",\n  messages=[\n              {\n                \"role\": \"user\",\n                \"content\": [\n                  {\n                    \"type\": \"text\",\n                    \"text\": \"What is in this image?\"\n                  },\n                  {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                      \"url\": data_url\n                    }\n                  }\n                ]\n              }\n            ]\n)\nprint(completion.choices[0].message.content)\n\n\n\n\n\n\n\nCode\n# | eval: false\nimport base64\n\n# Convert the previously generated image to base64\nwith open(\"text_image_gemini_25_openrouter_1352.png\", \"rb\") as image_file:\n    base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n\nresponse = client.chat.completions.create(\n    extra_headers={\n        \"HTTP-Referer\": \"www.tonydotdev.com\",\n        \"X-Title\": \"TT_AI_blog\",\n    },\n    model=\"google/gemini-2.5-flash-image\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Transform this image into a sunset version with warmer colors and golden light.add one person skiing in the foreground.\",\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"},\n                },\n            ],\n        }\n    ],\n    modalities=[\"image\", \"text\"],\n    # image_size=\"1024x1024\",\n)\n\n\n\n\nCode\nimport base64\nimport datetime\n# Extract message\nmessage = response.choices[0].message\n\n# Handle image output (base64 string inside \"data:image/png;base64,...\")\nif message.images:\n    data_url = message.images[0][\"image_url\"][\"url\"]\n\n    # Strip prefix if present\n    if data_url.startswith(\"data:image\"):\n        _, base64_data = data_url.split(\",\", 1)\n    else:\n        base64_data = data_url\n\n    # Decode and save as PNG\n    image_bytes = base64.b64decode(base64_data)\n    current_time = datetime.datetime.now().strftime(\"%H%M\")\n    output_file = f\"text_image_gemini_25_openrouter_{current_time}.png\"\n    with open(output_file, \"wb\") as f:\n        f.write(image_bytes)\n\n    print(f\"✅ Image saved as {output_file}\")\nelse:\n    print(\"❌ No image returned in response\")\n\n\n\n\nCode\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"text_image_gemini_25_openrouter_1405.png\"))\n\n\n\n\n\n\n\nCode\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize the OpenAI client with OpenRouter\nclient = OpenAI(\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n)\n\n\nembedding = client.embeddings.create(\n  extra_headers={\n    \"HTTP-Referer\": \"&lt;YOUR_SITE_URL&gt;\", # Optional. Site URL for rankings on openrouter.ai.\n    \"X-Title\": \"&lt;YOUR_SITE_NAME&gt;\", # Optional. Site title for rankings on openrouter.ai.\n  },\n  model=\"thenlper/gte-base\",\n  input=\"I can\",\n  encoding_format=\"float\"\n)\n\n#print(embedding.data[0].embedding) \n\n\n\n\nCode\nlen(embedding.data[0].embedding)\n\n\n\n\nCode\nprint(embedding.data[0].embedding[:5])  # Print first 5 dimensions\n\n\n\n\n\n\nOne of OpenRouter’s most powerful features is its transparent pricing model and cost management capabilities. Understanding and managing costs is crucial for production AI applications.\n\n\n\n💰 Financial Planning: - Predictable monthly expenses - Budget allocation across different use cases - ROI analysis for AI features - Cost per user tracking\n🔍 Technical Optimization: - Model selection based on cost/performance ratio - Prompt engineering to reduce token usage - Caching strategies for repeated requests - Batch processing for efficiency\n\n\n\nOpenRouter provides programmatic access to current pricing for all models:\n\n\nCode\n# Initialize the OpenAI client (if not already done)\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\nimport pandas as pd\n\nload_dotenv()\n\n\nTrue\n\n\nCode\nclient = OpenAI(\n  base_url=\"https://openrouter.ai/api/v1\",\n  api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n)\n\ndef get_model_pricing():\n    models_list = client.models.list()\n    pricing_data = []\n\n    for model in models_list.data:\n        name = model.id\n        pricing = model.pricing\n\n        # Get model metadata\n        context_length = getattr(model, 'context_length', None)\n        description = getattr(model, 'description', '')\n\n        # Get creation date and convert to readable format\n        created_timestamp = getattr(model, 'created', None)\n        if created_timestamp:\n            import datetime\n            created_date = datetime.datetime.fromtimestamp(created_timestamp).strftime('%Y-%m-%d')\n        else:\n            created_date = None\n\n        # Extract company from model name (first part before '/')\n        company = name.split('/')[0] if '/' in name else 'Unknown'\n\n        # Convert per-token prices to cost per 1M tokens\n        prompt_cost = float(pricing.get('prompt', 0)) * 1000000 if pricing and pricing.get('prompt') else 0\n        completion_cost = float(pricing.get('completion', 0)) * 1000000 if pricing and pricing.get('completion') else 0\n        request_cost = float(pricing.get('request', 0)) * 1000000 if pricing and pricing.get('request') else 0\n        image_cost = float(pricing.get('image', 0)) * 1000000 if pricing and pricing.get('image') else 0\n\n        pricing_data.append({\n            'Model': name,\n            'Company': company,\n            'Description': description,\n            'Context_Length': context_length,\n            'Created_Date': created_date,\n            'Prompt_Cost_per_1M': prompt_cost,\n            'Completion_Cost_per_1M': completion_cost,\n            'Request_Cost_per_1M': request_cost,\n            'Image_Cost_per_1M': image_cost\n        })\n\n    # Create pandas DataFrame\n    df = pd.DataFrame(pricing_data)\n\n    # Sort by company then by model name for better organization\n    df = df.sort_values(['Company', 'Model']).reset_index(drop=True)\n\n    return df\n\n# Get pricing DataFrames (all models and paid models only)\nall_models_df = get_model_pricing()\n\n\n\n\n\n\nCode\nimport panel as pn\n\n\ndf = all_models_df[\n    [\n        \"Model\",\n        \"Context_Length\",\n        \"Created_Date\",\n        \"Prompt_Cost_per_1M\",\n        \"Completion_Cost_per_1M\",\n    ]\n].sort_values(\"Prompt_Cost_per_1M\", ascending=False)\n\n# Create a paginated table\npn.extension(\"tabulator\")\ntable = pn.widgets.Tabulator(df, pagination=\"local\", page_size=10, show_index=False)\ntable\n\n\nTabulator(page_size=10, pagination='local', show_index=False, value=              ...)\n\n\n\n\n\n\nFollowing these best practices will help you build robust, secure, and efficient AI applications with OpenRouter.\n\n\n\n\n\n\nNever hardcode API keys in source code or configuration files\nUse environment variables or secret management systems (AWS Secrets Manager, Azure Key Vault)\nRotate API keys regularly and implement key rotation policies\nUse different keys for development, staging, and production environments\nAdd .env to .gitignore - Never commit credentials to version control\n\n\n\n\n\nValidate user inputs before sending to AI models\nSanitize prompts to prevent prompt injection attacks\nImplement rate limiting to prevent abuse\nLog and monitor for suspicious activity patterns\n\n\n\n\n\n\n\n\nChoose the right model for your use case - Not all tasks need the most expensive model\nBenchmark models for your specific use cases\nUse free models for development and testing\nConsider specialized models for specific tasks (coding, math, creative writing)\n\n\n\n\n\nImplement caching for repeated requests to reduce costs\nUse streaming for better user experience\nBatch requests when appropriate for efficiency\nOptimize prompts - Well-crafted prompts reduce token usage and improve results\n\n\n\n\n\n\n\n\nImplement comprehensive error handling - Models can be unavailable or rate-limited\nUse fallback models - Ensure your application remains functional even if one model is down\nImplement retry logic with exponential backoff\nMonitor response times and set appropriate timeouts\n\n\n\n\n\nMonitor usage - Keep track of costs and set limits\nTrack performance metrics (latency, success rates, error rates)\nSet up alerts for unusual activity patterns\nCreate dashboards for real-time monitoring\n\n\n\n\n\n\n\n\nSet spending limits and alerts in your OpenRouter dashboard\nUse cost-effective models for non-critical tasks\nImplement token counting to estimate costs before requests\nReview usage reports regularly to identify optimization opportunities\n\n\n\n\n\nOpenRouter represents a paradigm shift in how developers interact with AI models. By providing a unified, reliable, and cost-effective gateway to the world’s most advanced AI models, OpenRouter enables developers to focus on creating value rather than managing infrastructure complexities."
  },
  {
    "objectID": "posts/openrouter/index.html#getting-started-with-openrouter",
    "href": "posts/openrouter/index.html#getting-started-with-openrouter",
    "title": "OpenRouter: A Unified API for Multiple AI Models",
    "section": "",
    "text": "Getting started with OpenRouter is straightforward and can be accomplished in just a few steps. The platform is designed to minimize setup time while maintaining security best practices."
  },
  {
    "objectID": "posts/openrouter/index.html#create-your-openrouter-account",
    "href": "posts/openrouter/index.html#create-your-openrouter-account",
    "title": "OpenRouter: A Unified API for Multiple AI Models",
    "section": "",
    "text": "First, visit openrouter.ai and create an account. The registration process is simple:\n\nSign Up: Use your email or social login (Google, GitHub)\nVerify Email: Confirm your email address through the verification link\nAccess Dashboard: Navigate to your dashboard where you’ll find your API key\n\n💡 Pro Tip: Your dashboard provides valuable insights including: - Usage statistics and cost tracking - Model performance metrics - API key management - Billing information"
  },
  {
    "objectID": "posts/openrouter/index.html#secure-api-key-management",
    "href": "posts/openrouter/index.html#secure-api-key-management",
    "title": "OpenRouter: A Unified API for Multiple AI Models",
    "section": "",
    "text": "Security is paramount when working with AI APIs. Never hardcode your API keys directly in your code. Instead, use environment variables to keep your credentials safe:\n\n\n\nSecurity: Prevents accidental exposure in version control\nFlexibility: Allows different keys for development, staging, and production\nCollaboration: Team members can use their own keys without sharing\nDeployment: Easy management across different hosting environments\n\n\n\n\nCreate a .env file in your project root:\nOPENROUTER_API_KEY=your_actual_api_key_here\nInstall the python-dotenv library to load environment variables:\npip install python-dotenv"
  },
  {
    "objectID": "posts/openrouter/index.html#your-first-api-call",
    "href": "posts/openrouter/index.html#your-first-api-call",
    "title": "OpenRouter: A Unified API for Multiple AI Models",
    "section": "",
    "text": "Now that you have your API key set up, let’s make your first API call. OpenRouter cleverly uses the same API format as OpenAI, which means you can use the familiar openai Python library - just with a different base URL.\n\n\n🔧 How It Works: OpenRouter acts as a smart proxy that: 1. Receives your standardized API requests 2. Routes them to the appropriate AI model provider 3. Handles provider-specific authentication and formatting 4. Returns responses in a consistent format 5. Tracks usage and costs across all models\n\n\n\nBefore you start, ensure you have the necessary Python packages:\npip install openai python-dotenv\n\nopenai: The official OpenAI Python client (compatible with OpenRouter)\npython-dotenv: For loading environment variables from .env files\n\n\n\n\n\npython with OpenAI packagepython with chatlas packageR with ellmer package\n\n\n\n\nCode\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize the OpenAI client with OpenRouter\nclient = OpenAI(\n  base_url=\"https://openrouter.ai/api/v1\",  # OpenRouter's API endpoint\n  api_key=os.getenv(\"OPENROUTER_API_KEY\"),  # Your secure API key\n)\n\n# Create a chat completion with OpenRouter\ncompletion = client.chat.completions.create(\n  extra_headers={\n    \"HTTP-Referer\": \"https://your-site.com\",  # Optional: Helps OpenRouter improve their service\n    \"X-Title\": \"Your Site Name\",             # Optional: Your site name for OpenRouter rankings\n  },\n  model=\"openai/gpt-oss-20b:free\",          # Free model for testing/development\n  messages=[\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello! Can you explain what OpenRouter is in simple terms?\"\n    }\n  ],\n  temperature=0.7  # Controls creativity (0.0 = deterministic, 1.0 = very creative)\n)\n\n# Extract and print the response\nprint(completion.choices[0].message.content)\n\n\n\n\n\n\nCode\nfrom chatlas import ChatOpenRouter\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize the OpenAI client with OpenRouter\nclient = ChatOpenRouter(api_key=os.getenv(\"OPENROUTER_API_KEY\")\n,base_url='https://openrouter.ai/api/v1'\n ,system_prompt=None\n ,model=\"openai/gpt-oss-20b:free\")\n\n\n\n\nCode\nresponse=client.chat(\"What is the capital of France?\")\n#str(response)\n\n\n\n\n\n\nCode\nlibrary(ellmer)\nlibrary(dotenv)\nload_dot_env(file = \".env\")\n\nchat &lt;- chat_openrouter(\n  system_prompt = NULL,\n  api_key = Sys.getenv(\"OPENROUTER_API_KEY\"),\n \n  model = \"openai/gpt-oss-20b:free\",\n  echo = \"none\"\n)\n\n\n\n\nCode\nchat$chat(\"Tell me three jokes about statisticians\")\n\n\n\n\n\nKey Components Explained:\n\nbase_url: Points to OpenRouter’s API instead of OpenAI’s\nmodel: Uses OpenRouter’s model naming format (provider/model-name)\nextra_headers: Optional but recommended for OpenRouter’s analytics\ntemperature: Controls response creativity (0.0-2.0 range)\nmessages: Standard chat format with role-based conversation structure\n\n💡 Model Selection Tips: - Free Models: Great for development (*free suffix) - Budget Models: Cost-effective for production (*:budget suffix) - Premium Models: Best performance (*, *:pro, *:latest) - Specialized: Task-optimized models (coding, math, creative writing)"
  },
  {
    "objectID": "posts/openrouter/index.html#mastering-system-prompts",
    "href": "posts/openrouter/index.html#mastering-system-prompts",
    "title": "OpenRouter: A Unified API for Multiple AI Models",
    "section": "",
    "text": "System prompts are powerful tools that shape the AI’s behavior, personality, and response style. They set the context and rules for the entire conversation, appearing before user messages in the conversation flow.\n\n\nSystem prompts act as meta-instructions that guide how the AI should respond throughout the conversation. They’re processed first and influence all subsequent interactions.\n\n\n\n\nConsistent Behavior: Ensures AI maintains the desired personality throughout\nOutput Format: Dictates response structure (JSON, markdown, code blocks)\nSafety Constraints: Sets boundaries and restrictions on responses\nContext Setting: Provides background information for better responses\nTask Specialization: Optimizes AI for specific use cases\n\n\n\n\n\n\nCode\n# Example with system prompt\ncompletion = client.chat.completions.create(\n  model=\"openai/gpt-oss-20b:free\",\n  messages=[\n    {\n      \"role\": \"system\",\n      \"content\": \"You are a helpful AI assistant that explains technical concepts in simple terms. Always be friendly and use analogies when possible and be simple\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"How does temperature in LLM model work?\"\n    }\n  ],\n  temperature=0.7\n)\n\nprint(completion.choices[0].message.content)\n\n\nCommon system prompt patterns:\n\n\nCode\n# Different system prompt examples\nsystem_prompts = {\n    \"coding_assistant\": \"You are an expert programmer. Provide clean, well-commented code solutions and explain your reasoning.\",\n    \"creative_writer\": \"You are a creative storyteller. Write engaging narratives with vivid descriptions and compelling characters.\",\n    \"data_analyst\": \"You are a data analyst. Provide insights based on data, suggest visualizations, and explain statistical concepts clearly.\",\n    \"tutor\": \"You are a patient tutor. Break down complex topics into simple steps and provide encouraging feedback.\"\n}\n\ndef chat_with_persona(persona, user_message):\n    completion = client.chat.completions.create(\n        model=\"openai/gpt-oss-20b:free\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompts[persona]},\n            {\"role\": \"user\", \"content\": user_message}\n        ],\n        temperature=0.7\n    )\n    return completion.choices[0].message.content\n\n# Example usage\n#response = chat_with_persona(\"coding_assistant\", \"How do I reverse a string in Python?\")\n#print(response)"
  },
  {
    "objectID": "posts/openrouter/index.html#streaming-responses-real-time-ai-interaction",
    "href": "posts/openrouter/index.html#streaming-responses-real-time-ai-interaction",
    "title": "OpenRouter: A Unified API for Multiple AI Models",
    "section": "",
    "text": "Streaming is a game-changer for user experience, especially in chat applications and interactive tools. Instead of waiting for complete responses, streaming delivers content as it’s being generated, creating natural and engaging conversations.\n\n\n🚀 User Experience Benefits: - Immediate Feedback: Users see responses starting immediately - Reduced Perceived Latency: Content appears as it’s generated - Natural Conversation Flow: Mimics human speech patterns - Progress Indication: Users know the AI is working - Early Termination: Users can stop lengthy responses if needed\n⚡ Technical Advantages: - Lower Memory Usage: No need to buffer complete responses - Faster Time-to-First-Byte: Content starts flowing immediately - Better Error Handling: Issues detected earlier in process - Resource Efficiency: Processes data incrementally\n\n\n\n\n\nCode\n# Initialize the OpenAI client (if not already done)\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nclient = OpenAI(\n  base_url=\"https://openrouter.ai/api/v1\",\n  api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n)\n\ndef stream_response(model, message):\n    stream = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": message}],\n        stream=True\n    )\n\n    for chunk in stream:\n        if chunk.choices[0].delta.content is not None:\n            print(chunk.choices[0].delta.content, end='', flush=True)\n\n# Stream example\nprint(\"Streaming response:\")\nstream_response(\"openai/gpt-oss-20b:free\", \"Tell me a short story about AI\")\nprint()  # Add newline after streaming\n\n\n\n\n\n\n\nCode\nfrom openai import OpenAI\nimport base64\nimport datetime\n\n\n\n\nCode\n# | eval: false\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize the OpenAI client with OpenRouter\nclient = OpenAI(\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n)\n\n\n\n\nCode\n# Image generation request\nresponse = client.chat.completions.create(\n    extra_headers={\n        \"HTTP-Referer\": \"www.tonydotdev.com\",\n        \"X-Title\": \"TT_AI_blog\",\n    },\n    model=\"google/gemini-2.5-flash-image\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Generate a serene and realistic snowy mountain landscape at sunrise.\",\n        }\n    ],\n    modalities=[\"image\", \"text\"],\n    #image_size=\"1024x1024\",\n)\n\n\n\n\nCode\nimport base64\nimport datetime\n# Extract message\nmessage = response.choices[0].message\n\n# Handle image output (base64 string inside \"data:image/png;base64,...\")\nif message.images:\n    data_url = message.images[0][\"image_url\"][\"url\"]\n\n    # Strip prefix if present\n    if data_url.startswith(\"data:image\"):\n        _, base64_data = data_url.split(\",\", 1)\n    else:\n        base64_data = data_url\n\n    # Decode and save as PNG\n    image_bytes = base64.b64decode(base64_data)\n    current_time = datetime.datetime.now().strftime(\"%H%M\")\n    output_file = f\"text_image_gemini_25_openrouter_{current_time}.png\"\n    with open(output_file, \"wb\") as f:\n        f.write(image_bytes)\n\n    print(f\"✅ Image saved as {output_file}\")\nelse:\n    print(\"❌ No image returned in response\")\n\n\n\n\nCode\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"text_image_gemini_25_openrouter_1352.png\"))\n\n\n&lt;IPython.core.display.Image object&gt;\n\n\n\n\n\n\n\nCode\nimport base64\n\n# Convert local image to data URL\nwith open(\"text_image_gemini_25_openrouter_1352.png\", \"rb\") as image_file:\n    base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n    data_url = f\"data:image/png;base64,{base64_image}\"\n\ncompletion = client.chat.completions.create(\n  extra_headers={\n    \"HTTP-Referer\": \"&lt;YOUR_SITE_URL&gt;\", # Optional. Site URL for rankings on openrouter.ai.\n    \"X-Title\": \"&lt;YOUR_SITE_NAME&gt;\", # Optional. Site title for rankings on openrouter.ai.\n  },\n  extra_body={},\n  model=\"nvidia/nemotron-nano-12b-v2-vl:free\",\n  messages=[\n              {\n                \"role\": \"user\",\n                \"content\": [\n                  {\n                    \"type\": \"text\",\n                    \"text\": \"What is in this image?\"\n                  },\n                  {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                      \"url\": data_url\n                    }\n                  }\n                ]\n              }\n            ]\n)\nprint(completion.choices[0].message.content)\n\n\n\n\n\n\n\nCode\n# | eval: false\nimport base64\n\n# Convert the previously generated image to base64\nwith open(\"text_image_gemini_25_openrouter_1352.png\", \"rb\") as image_file:\n    base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n\nresponse = client.chat.completions.create(\n    extra_headers={\n        \"HTTP-Referer\": \"www.tonydotdev.com\",\n        \"X-Title\": \"TT_AI_blog\",\n    },\n    model=\"google/gemini-2.5-flash-image\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Transform this image into a sunset version with warmer colors and golden light.add one person skiing in the foreground.\",\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"},\n                },\n            ],\n        }\n    ],\n    modalities=[\"image\", \"text\"],\n    # image_size=\"1024x1024\",\n)\n\n\n\n\nCode\nimport base64\nimport datetime\n# Extract message\nmessage = response.choices[0].message\n\n# Handle image output (base64 string inside \"data:image/png;base64,...\")\nif message.images:\n    data_url = message.images[0][\"image_url\"][\"url\"]\n\n    # Strip prefix if present\n    if data_url.startswith(\"data:image\"):\n        _, base64_data = data_url.split(\",\", 1)\n    else:\n        base64_data = data_url\n\n    # Decode and save as PNG\n    image_bytes = base64.b64decode(base64_data)\n    current_time = datetime.datetime.now().strftime(\"%H%M\")\n    output_file = f\"text_image_gemini_25_openrouter_{current_time}.png\"\n    with open(output_file, \"wb\") as f:\n        f.write(image_bytes)\n\n    print(f\"✅ Image saved as {output_file}\")\nelse:\n    print(\"❌ No image returned in response\")\n\n\n\n\nCode\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"text_image_gemini_25_openrouter_1405.png\"))\n\n\n\n\n\n\n\nCode\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize the OpenAI client with OpenRouter\nclient = OpenAI(\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n)\n\n\nembedding = client.embeddings.create(\n  extra_headers={\n    \"HTTP-Referer\": \"&lt;YOUR_SITE_URL&gt;\", # Optional. Site URL for rankings on openrouter.ai.\n    \"X-Title\": \"&lt;YOUR_SITE_NAME&gt;\", # Optional. Site title for rankings on openrouter.ai.\n  },\n  model=\"thenlper/gte-base\",\n  input=\"I can\",\n  encoding_format=\"float\"\n)\n\n#print(embedding.data[0].embedding) \n\n\n\n\nCode\nlen(embedding.data[0].embedding)\n\n\n\n\nCode\nprint(embedding.data[0].embedding[:5])  # Print first 5 dimensions"
  },
  {
    "objectID": "posts/openrouter/index.html#cost-management-optimizing-your-ai-spending",
    "href": "posts/openrouter/index.html#cost-management-optimizing-your-ai-spending",
    "title": "OpenRouter: A Unified API for Multiple AI Models",
    "section": "",
    "text": "One of OpenRouter’s most powerful features is its transparent pricing model and cost management capabilities. Understanding and managing costs is crucial for production AI applications."
  },
  {
    "objectID": "posts/openrouter/index.html#why-cost-management-matters",
    "href": "posts/openrouter/index.html#why-cost-management-matters",
    "title": "OpenRouter: A Unified API for Multiple AI Models",
    "section": "",
    "text": "💰 Financial Planning: - Predictable monthly expenses - Budget allocation across different use cases - ROI analysis for AI features - Cost per user tracking\n🔍 Technical Optimization: - Model selection based on cost/performance ratio - Prompt engineering to reduce token usage - Caching strategies for repeated requests - Batch processing for efficiency"
  },
  {
    "objectID": "posts/openrouter/index.html#real-time-cost-tracking",
    "href": "posts/openrouter/index.html#real-time-cost-tracking",
    "title": "OpenRouter: A Unified API for Multiple AI Models",
    "section": "",
    "text": "OpenRouter provides programmatic access to current pricing for all models:\n\n\nCode\n# Initialize the OpenAI client (if not already done)\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\nimport pandas as pd\n\nload_dotenv()\n\n\nTrue\n\n\nCode\nclient = OpenAI(\n  base_url=\"https://openrouter.ai/api/v1\",\n  api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n)\n\ndef get_model_pricing():\n    models_list = client.models.list()\n    pricing_data = []\n\n    for model in models_list.data:\n        name = model.id\n        pricing = model.pricing\n\n        # Get model metadata\n        context_length = getattr(model, 'context_length', None)\n        description = getattr(model, 'description', '')\n\n        # Get creation date and convert to readable format\n        created_timestamp = getattr(model, 'created', None)\n        if created_timestamp:\n            import datetime\n            created_date = datetime.datetime.fromtimestamp(created_timestamp).strftime('%Y-%m-%d')\n        else:\n            created_date = None\n\n        # Extract company from model name (first part before '/')\n        company = name.split('/')[0] if '/' in name else 'Unknown'\n\n        # Convert per-token prices to cost per 1M tokens\n        prompt_cost = float(pricing.get('prompt', 0)) * 1000000 if pricing and pricing.get('prompt') else 0\n        completion_cost = float(pricing.get('completion', 0)) * 1000000 if pricing and pricing.get('completion') else 0\n        request_cost = float(pricing.get('request', 0)) * 1000000 if pricing and pricing.get('request') else 0\n        image_cost = float(pricing.get('image', 0)) * 1000000 if pricing and pricing.get('image') else 0\n\n        pricing_data.append({\n            'Model': name,\n            'Company': company,\n            'Description': description,\n            'Context_Length': context_length,\n            'Created_Date': created_date,\n            'Prompt_Cost_per_1M': prompt_cost,\n            'Completion_Cost_per_1M': completion_cost,\n            'Request_Cost_per_1M': request_cost,\n            'Image_Cost_per_1M': image_cost\n        })\n\n    # Create pandas DataFrame\n    df = pd.DataFrame(pricing_data)\n\n    # Sort by company then by model name for better organization\n    df = df.sort_values(['Company', 'Model']).reset_index(drop=True)\n\n    return df\n\n# Get pricing DataFrames (all models and paid models only)\nall_models_df = get_model_pricing()\n\n\n\n\n\n\nCode\nimport panel as pn\n\n\ndf = all_models_df[\n    [\n        \"Model\",\n        \"Context_Length\",\n        \"Created_Date\",\n        \"Prompt_Cost_per_1M\",\n        \"Completion_Cost_per_1M\",\n    ]\n].sort_values(\"Prompt_Cost_per_1M\", ascending=False)\n\n# Create a paginated table\npn.extension(\"tabulator\")\ntable = pn.widgets.Tabulator(df, pagination=\"local\", page_size=10, show_index=False)\ntable\n\n\nTabulator(page_size=10, pagination='local', show_index=False, value=              ...)"
  },
  {
    "objectID": "posts/openrouter/index.html#best-practices-production-ready-ai-development",
    "href": "posts/openrouter/index.html#best-practices-production-ready-ai-development",
    "title": "OpenRouter: A Unified API for Multiple AI Models",
    "section": "",
    "text": "Following these best practices will help you build robust, secure, and efficient AI applications with OpenRouter."
  },
  {
    "objectID": "posts/openrouter/index.html#security-best-practices",
    "href": "posts/openrouter/index.html#security-best-practices",
    "title": "OpenRouter: A Unified API for Multiple AI Models",
    "section": "",
    "text": "Never hardcode API keys in source code or configuration files\nUse environment variables or secret management systems (AWS Secrets Manager, Azure Key Vault)\nRotate API keys regularly and implement key rotation policies\nUse different keys for development, staging, and production environments\nAdd .env to .gitignore - Never commit credentials to version control\n\n\n\n\n\nValidate user inputs before sending to AI models\nSanitize prompts to prevent prompt injection attacks\nImplement rate limiting to prevent abuse\nLog and monitor for suspicious activity patterns"
  },
  {
    "objectID": "posts/openrouter/index.html#performance-best-practices",
    "href": "posts/openrouter/index.html#performance-best-practices",
    "title": "OpenRouter: A Unified API for Multiple AI Models",
    "section": "",
    "text": "Choose the right model for your use case - Not all tasks need the most expensive model\nBenchmark models for your specific use cases\nUse free models for development and testing\nConsider specialized models for specific tasks (coding, math, creative writing)\n\n\n\n\n\nImplement caching for repeated requests to reduce costs\nUse streaming for better user experience\nBatch requests when appropriate for efficiency\nOptimize prompts - Well-crafted prompts reduce token usage and improve results"
  },
  {
    "objectID": "posts/openrouter/index.html#reliability-best-practices",
    "href": "posts/openrouter/index.html#reliability-best-practices",
    "title": "OpenRouter: A Unified API for Multiple AI Models",
    "section": "",
    "text": "Implement comprehensive error handling - Models can be unavailable or rate-limited\nUse fallback models - Ensure your application remains functional even if one model is down\nImplement retry logic with exponential backoff\nMonitor response times and set appropriate timeouts\n\n\n\n\n\nMonitor usage - Keep track of costs and set limits\nTrack performance metrics (latency, success rates, error rates)\nSet up alerts for unusual activity patterns\nCreate dashboards for real-time monitoring"
  },
  {
    "objectID": "posts/openrouter/index.html#cost-management-best-practices",
    "href": "posts/openrouter/index.html#cost-management-best-practices",
    "title": "OpenRouter: A Unified API for Multiple AI Models",
    "section": "",
    "text": "Set spending limits and alerts in your OpenRouter dashboard\nUse cost-effective models for non-critical tasks\nImplement token counting to estimate costs before requests\nReview usage reports regularly to identify optimization opportunities"
  },
  {
    "objectID": "posts/openrouter/index.html#conclusion-building-the-future-of-ai-applications",
    "href": "posts/openrouter/index.html#conclusion-building-the-future-of-ai-applications",
    "title": "OpenRouter: A Unified API for Multiple AI Models",
    "section": "",
    "text": "OpenRouter represents a paradigm shift in how developers interact with AI models. By providing a unified, reliable, and cost-effective gateway to the world’s most advanced AI models, OpenRouter enables developers to focus on creating value rather than managing infrastructure complexities."
  },
  {
    "objectID": "posts/weather-trend/index.html",
    "href": "posts/weather-trend/index.html",
    "title": "Weather Forecast App with Streamlit and AI Integration",
    "section": "",
    "text": "In this comprehensive tutorial, I’ll guide you through creating a sophisticated weather forecast application that combines modern web development with artificial intelligence. This project demonstrates how to build a production-ready weather app with interactive maps, bilingual support, real-time data visualization, and AI-powered weather insights.\nLive Demo: https://weather-trend.streamlit.app/\nGithub: https://github.com/JCwinning/weather_trend\n\n\n\nWeather Forecast App Main Interface\n\n\nThis weather forecast application goes beyond basic weather data by integrating multiple advanced features:\n\nInteractive Location Selection: Click anywhere on the map or search by city name\nBilingual Interface: Full English/Chinese language support with toggle functionality\nAI-Powered Insights: Weather analysis and recommendations using DeepSeek AI\nAdvanced Visualization: Temperature trends, air quality monitoring, and rain probability\nReal-time Data: 7-day historical and 5-day forecast data\nResponsive Design: Works seamlessly across desktop, tablet, and mobile devices\n\n\n\n\n\n\n\n\nflowchart TD\n    A[User Interface&lt;br/&gt;Streamlit Web App] --&gt; B[Location Services]\n    A --&gt; C[Weather Data API]\n    A --&gt; D[AI Integration Layer]\n    A --&gt; E[Visualization Engine]\n\n    B --&gt; F[Interactive Map&lt;br/&gt;Folium + OpenStreetMap]\n    B --&gt; G[City Search&lt;br/&gt;Nominatim Geocoding]\n\n    C --&gt; H[Open-Meteo API&lt;br/&gt;Weather + Air Quality]\n    C --&gt; I[Data Processing&lt;br/&gt;Pandas DataFrame]\n\n    D --&gt; J[ModelScope API&lt;br/&gt;DeepSeek-V3.2 AI]\n    D --&gt; K[Intelligent Analysis&lt;br/&gt;Weather Recommendations]\n\n    E --&gt; L[Altair Charts&lt;br/&gt;Temperature Trends]\n    E --&gt; M[Data Tables&lt;br/&gt;Weather Details]\n\n    F --&gt; A\n    G --&gt; A\n    I --&gt; E\n    K --&gt; A\n\n\n Weather App Architecture \n\n\n\n\n\n\nFrontend Framework: Streamlit for rapid web application development\nMapping: Folium with OpenStreetMap tiles for interactive location selection\nData Visualization: Altair for professional charts and graphs\nAPI Integration: Open-Meteo for weather and air quality data\nAI Services: ModelScope API with DeepSeek-V3.2 for intelligent analysis\nGeocoding: Nominatim for address-to-coordinate conversion\nInternationalization: Custom language system for English/Chinese support\n\n\n\n\n\n\n\nBefore we dive into the code, let’s set up our development environment:\n# 1. Clone the repository\ngit clone &lt;your-repo-url&gt;\ncd weather_trend\n\n# 2. Install required packages\npip install streamlit pandas requests altair folium streamlit-folium geopy python-dotenv openai\n\n# 3. Set up environment variables for AI features\necho \"modelscope=your_api_key_here\" &gt; .env\n\n\n\n\nstreamlit: Web application framework with reactive UI components\npandas: Data manipulation and analysis for weather datasets\nrequests: HTTP client for API communication\naltair: Declarative statistical visualization library\nfolium + streamlit-folium: Interactive map integration\ngeopy: Geocoding services for location lookup\npython-dotenv: Environment variable management\nopenai: AI model integration (compatible with ModelScope)\n\n\n\n\n\n\n\nThe map functionality allows users to click anywhere and get weather data for that exact location:\n\n\nCode\nimport folium\nfrom streamlit_folium import st_folium\n\n# Create interactive map centered on default location\ndef create_interactive_map(lat=40.7128, lon=-74.0060, zoom=10):\n    \"\"\"Create an interactive Folium map\"\"\"\n    m = folium.Map(\n        location=[lat, lon],\n        zoom_start=zoom,\n        tiles=\"OpenStreetMap\"\n    )\n\n    # Add click event handler to capture coordinates\n    m.add_child(folium.LatLngPopup())\n\n    return m\n\n# Display map in Streamlit\nif 'map_data' not in st.session_state:\n    st.session_state.map_data = None\n\nmap_object = create_interactive_map()\nst_data = st_folium(map_object, width=700, height=500)\n\n# Capture clicked coordinates\nif st_data['last_clicked']:\n    lat = st_data['last_clicked']['lat']\n    lon = st_data['last_clicked']['lng']\n    st.session_state.clicked_location = (lat, lon)\n    get_weather_for_coordinates(lat, lon)\n\n\nKey Features: - Click-to-Select: Users can click anywhere on the map - Zoom Controls: Standard map navigation - Responsive Design: Adapts to different screen sizes - Coordinate Capture: Automatic extraction of clicked locations\n\n\n\nThe app supports both English and Chinese city names with intelligent fallback:\n\n\nCode\nimport re\nfrom geopy.geocoders import Nominatim\n\n# Extended Chinese character detection\ndef contains_chinese(text):\n    \"\"\"Check if text contains Chinese characters including CJK Unified Ideographs\"\"\"\n    chinese_pattern = re.compile(\n        r\"[\\u4e00-\\u9fff\\u3400-\\u4dbf\\U00020000-\\U0002a6df\\U0002a700-\\U0002b73f]\"\n    )\n    return bool(chinese_pattern.search(text))\n\n# Chinese city mapping for better geocoding\nCHINESE_CITY_MAPPING = {\n    \"纽约\": \"New York\",\n    \"东京\": \"Tokyo\",\n    \"伦敦\": \"London\",\n    \"巴黎\": \"Paris\",\n    \"洛杉矶\": \"Los Angeles\",\n    # ... more mappings\n}\n\ndef get_coordinates_for_city(city_name):\n    \"\"\"Get coordinates for city with multilingual support\"\"\"\n    # Check for Chinese city name mapping\n    if contains_chinese(city_name) and city_name in CHINESE_CITY_MAPPING:\n        city_name = CHINESE_CITY_MAPPING[city_name]\n\n    # Geocode the city\n    geolocator = Nominatim(user_agent=\"weather_app\")\n    try:\n        location = geolocator.geocode(city_name)\n        return (location.latitude, location.longitude) if location else None\n    except:\n        return None\n\n\nBilingual Features: - Chinese Character Detection: Advanced regex pattern for CJK characters - City Name Mapping: Translation database for major cities - Fallback Handling: Graceful degradation when geocoding fails - Unicode Support: Full international character support\n\n\n\nThe application fetches comprehensive weather data from Open-Meteo API:\n\n\nCode\nimport requests\nimport pandas as pd\n\ndef get_weather_data(latitude, longitude):\n    \"\"\"Fetch 12-day weather data (7 historical + 5 forecast)\"\"\"\n\n    # API endpoint configuration\n    url = \"https://api.open-meteo.com/v1/forecast\"\n    params = {\n        'latitude': latitude,\n        'longitude': longitude,\n        'daily': [\n            'temperature_2m_max', 'temperature_2m_min', 'temperature_2m_mean',\n            'weathercode', 'windspeed_10m_max', 'precipitation_probability_max',\n            'pm10', 'pm2_5'\n        ],\n        'timezone': 'auto',\n        'past_days': 7,  # Get historical data\n        'forecast_days': 5  # Get future forecast\n    }\n\n    try:\n        response = requests.get(url, params=params, timeout=10)\n        response.raise_for_status()\n\n        # Process response data\n        data = response.json()\n        df = process_weather_data(data)\n\n        return df\n\n    except requests.RequestException as e:\n        st.error(f\"API request failed: {e}\")\n        return None\n\ndef process_weather_data(data):\n    \"\"\"Convert API response to structured DataFrame\"\"\"\n    daily_data = data['daily']\n\n    # Create date range (historical + future)\n    dates = pd.date_range(\n        start=pd.to_datetime(daily_data['time'][0]),\n        periods=len(daily_data['time']),\n        freq='D'\n    )\n\n    # Build DataFrame\n    df = pd.DataFrame({\n        'date': dates,\n        'temperature_max': daily_data['temperature_2m_max'],\n        'temperature_min': daily_data['temperature_2m_min'],\n        'temperature_mean': daily_data['temperature_2m_mean'],\n        'weather_code': daily_data['weathercode'],\n        'wind_speed_max': daily_data['windspeed_10m_max'],\n        'rain_probability': daily_data['precipitation_probability_max'],\n        'pm2_5': daily_data.get('pm2_5', [0] * len(dates))\n    })\n\n    # Add derived columns\n    df['is_today'] = df['date'].dt.date == pd.Timestamp.now().date()\n    df['is_future'] = df['date'] &gt; pd.Timestamp.now()\n\n    return df\n\n\nData Processing Features: - Historical + Forecast: 7 days past + 5 days future - Air Quality Integration: PM2.5 and PM10 data - Derived Metrics: Today detection and future projection - Error Handling: Robust API error management\n\n\n\nTemperature trends with clear visual distinction between historical and forecast data:\n\n\nCode\nimport altair as alt\n\ndef create_temperature_chart(weather_df):\n    \"\"\"Create interactive temperature trend chart\"\"\"\n\n    # Base chart with temperature line\n    base_chart = alt.Chart(weather_df).mark_line(\n        point=True,\n        strokeWidth=3,\n        opacity=0.8\n    ).encode(\n        x=alt.X('date:T', title='Date'),\n        y=alt.Y('temperature_mean:Q', title='Temperature (°C)', scale=alt.Scale(domain=[weather_df['temperature_min'].min()-5, weather_df['temperature_max'].max()+5]))\n    ).properties(\n        width=800,\n        height=400,\n        title='Temperature Trends'\n    )\n\n    # Historical data (solid line)\n    historical_data = weather_df[weather_df['is_future'] == False]\n    historical_line = base_chart.transform_filter(\n        'datum.is_future == false'\n    ).encode(\n        color=alt.value('blue'),\n        strokeDash=alt.value([0])  # Solid line\n    )\n\n    # Future forecast (dotted line)\n    future_data = weather_df[weather_df['is_future'] == True]\n    future_line = base_chart.transform_filter(\n        'datum.is_future == true'\n    ).encode(\n        color=alt.value('red'),\n        strokeDash=alt.value([5, 5])  # Dotted line\n    )\n\n    # Today indicator\n    today_line = alt.Chart(pd.DataFrame({'x': [pd.Timestamp.now()]})).mark_rule(\n        strokeDash=[2, 2],\n        stroke='green',\n        strokeWidth=2\n    ).encode(x='x:T')\n\n    return (historical_line + future_line + today_line).resolve_scale(color='independent')\n\n\nVisualization Features: - Line Style Differentiation: Solid (historical) vs Dotted (forecast) - Today Indicator: Clear visual marker for current day - Color Coding: Blue for past, red for future - Interactive Tooltips: Hover information for data points\n\n\n\nPM2.5 levels with EPA-compliant color coding:\n\n\nCode\ndef get_air_quality_level(pm25_value):\n    \"\"\"Get air quality level based on US EPA PM2.5 standards\"\"\"\n\n    if pm25_value &lt;= 12:\n        return {\n            'level': 'Good',\n            'color': '#00e400',  # Deeper green\n            'text_color': 'white',\n            'icon': '🟢'\n        }\n    elif pm25_value &lt;= 35.4:\n        return {\n            'level': 'Moderate',\n            'color': '#ffff00',  # Light green\n            'text_color': 'black',\n            'icon': '🟢'\n        }\n    elif pm25_value &lt;= 55.4:\n        return {\n            'level': 'Unhealthy for Sensitive Groups',\n            'color': '#ff7e00',  # Darker yellow\n            'text_color': 'black',\n            'icon': '🟡'\n        }\n    elif pm25_value &lt;= 150.4:\n        return {\n            'level': 'Unhealthy',\n            'color': '#ff0000',  # Darker orange\n            'text_color': 'white',\n            'icon': '🟠'\n        }\n    elif pm25_value &lt;= 250.4:\n        return {\n            'level': 'Very Unhealthy',\n            'color': '#8f3f97',  # Darker red\n            'text_color': 'white',\n            'icon': '🔴'\n        }\n    else:\n        return {\n            'level': 'Hazardous',\n            'color': '#7e0023',  # Very dark red\n            'text_color': 'white',\n            'icon': '⚫'\n        }\n\ndef display_air_quality_badge(pm25_value):\n    \"\"\"Display air quality with visual indicators\"\"\"\n    aq_info = get_air_quality_level(pm25_value)\n\n    st.markdown(f\"\"\"\n    &lt;div style=\"background-color: {aq_info['color']}; color: {aq_info['text_color']};\n                padding: 10px; border-radius: 5px; text-align: center; margin: 5px 0;\"&gt;\n        &lt;strong&gt;{aq_info['icon']} PM2.5: {pm25_value} μg/m³&lt;/strong&gt;&lt;br&gt;\n        &lt;small&gt;{aq_info['level']}&lt;/small&gt;\n    &lt;/div&gt;\n    \"\"\", unsafe_allow_html=True)\n\n\nAir Quality Features: - EPA Standards: Based on US Environmental Protection Agency - Visual Indicators: Color-coded badges with icons - Accessibility: High contrast colors for readability - Educational: Level descriptions for user understanding\n\n\n\n\n\n\nThe most innovative feature is AI-powered weather analysis using ModelScope’s DeepSeek-V3.2 model:\n\n\n\nAI Weather Recommendations\n\n\n\n\nCode\nfrom openai import OpenAI\nimport os\n\n# Initialize AI client with ModelScope\ndef init_ai_client():\n    \"\"\"Initialize OpenAI client for ModelScope API\"\"\"\n    return OpenAI(\n        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n        api_key=os.getenv(\"modelscope\"),\n    )\n\ndef generate_weather_insights(weather_df, location_name, language=\"en\"):\n    \"\"\"Generate AI-powered weather recommendations\"\"\"\n\n    # Prepare weather data for AI analysis\n    today_index = weather_df[weather_df['is_today']].index[0] if any(weather_df['is_today']) else 0\n    historical_data = weather_df.iloc[:today_index+1]  # Up to today\n    future_data = weather_df.iloc[today_index:]  # Today onwards\n\n    # Create weather summary for AI\n    weather_summary = f\"\"\"\n    Location: {location_name}\n\n    Historical Weather (Last {len(historical_data)} days):\n    - Temperature Range: {historical_data['temperature_min'].min():.1f}°C to {historical_data['temperature_max'].max():.1f}°C\n    - Average Temperature: {historical_data['temperature_mean'].mean():.1f}°C\n    - Air Quality Range: {historical_data['pm2_5'].min():.1f} to {historical_data['pm2_5'].max():.1f} PM2.5\n\n    Forecast (Next {len(future_data)} days):\n    - Temperature Range: {future_data['temperature_min'].min():.1f}°C to {future_data['temperature_max'].max():.1f}°C\n    - Average Rain Probability: {future_data['rain_probability'].mean():.0f}%\n    \"\"\"\n\n    # Generate prompt based on language\n    if language == \"zh\":\n        prompt = f\"\"\"\n        基于以下天气数据，请提供简洁实用的天气建议（100-200字）：\n\n        {weather_summary}\n\n        请包括：\n        1. 天气模式分析\n        2. 穿衣建议\n        3. 户外活动建议\n        4. 健康注意事项（如空气质量相关）\n\n        请用中文回复，语气友好实用。\n        \"\"\"\n    else:\n        prompt = f\"\"\"\n        Based on the following weather data, please provide concise and practical weather advice (100-200 words):\n\n        {weather_summary}\n\n        Please include:\n        1. Weather pattern analysis\n        2. Clothing recommendations\n        3. Outdoor activity suggestions\n        4. Health considerations (related to air quality if applicable)\n\n        Please respond in {language} with a friendly and practical tone.\n        \"\"\"\n\n    try:\n        client = init_ai_client()\n        response = client.chat.completions.create(\n            model=\"deepseek-ai/DeepSeek-V3\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=300,\n            temperature=0.7\n        )\n\n        return response.choices[0].message.content\n\n    except Exception as e:\n        return f\"AI service temporarily unavailable. Error: {str(e)}\"\n\n# In Streamlit app\nif st.button(get_text(\"ai_button\", language)):\n    with st.spinner(\"Getting AI weather advice...\"):\n        if 'weather_df' in st.session_state and 'location_name' in st.session_state:\n            ai_insights = generate_weather_insights(\n                st.session_state.weather_df,\n                st.session_state.location_name,\n                language\n            )\n            st.markdown(\"### 🤖 AI Weather Analysis\")\n            st.write(ai_insights)\n        else:\n            st.warning(\"Please get weather data first.\")\n\n\nAI Features: - Context-Aware Analysis: Processes both historical and forecast data - Multilingual Support: AI responds in user’s selected language - Practical Recommendations: Clothing, activities, and health advice - Error Handling: Graceful fallback when AI service unavailable\n\n\n\nThe AI system uses carefully crafted prompts to generate useful insights:\nPrompt Structure: 1. Data Context: Comprehensive weather statistics 2. Task Definition: Clear requirements for analysis 3. Output Format: Structured response categories 4. Language Adaptation: Matches user interface language\nResponse Categories: - Weather Pattern Analysis: Trends and anomalies - Clothing Recommendations: Practical dress suggestions - Activity Advice: Outdoor planning recommendations - Health Considerations: Air quality and weather impacts\n\n\n\n\n\n\nThe app implements a comprehensive bilingual system:\n\n\nCode\n# language.py - Translation management\nTRANSLATIONS = {\n    \"en\": {\n        \"app_title\": \"Weather Forecast App\",\n        \"sidebar_header\": \"Weather Query\",\n        \"city_input_placeholder\": \"Enter a city name\",\n        \"get_weather_button\": \"Get Weather\",\n        \"weather_trends_title\": \"Weather Trends for\",\n        \"ai_button\": \"AI Weather Advice\",\n        # ... more translations\n    },\n    \"zh\": {\n        \"app_title\": \"天气预报应用\",\n        \"sidebar_header\": \"天气查询\",\n        \"city_input_placeholder\": \"输入城市名称\",\n        \"get_weather_button\": \"获取天气\",\n        \"weather_trends_title\": \"天气趋势\",\n        \"ai_button\": \"AI天气建议\",\n        # ... more translations\n    }\n}\n\ndef get_text(key, language=\"en\"):\n    \"\"\"Get translated text for given key and language\"\"\"\n    return TRANSLATIONS.get(language, {}).get(key, key)\n\ndef get_available_languages():\n    \"\"\"Get available language options\"\"\"\n    return {\"en\": \"English\", \"zh\": \"中文\"}\n\n# In main app (app.py)\ndef main():\n    # Language state management\n    if 'language' not in st.session_state:\n        st.session_state.language = \"en\"\n\n    # Language toggle button\n    current_lang = st.session_state.language\n    available_langs = get_available_languages()\n\n    col1, col2, col3 = st.columns([1,1,6])\n    with col1:\n        if st.button(\"EN\", disabled=current_lang==\"en\"):\n            st.session_state.language = \"en\"\n            st.rerun()\n\n    with col2:\n        if st.button(\"中文\", disabled=current_lang==\"zh\"):\n            st.session_state.language = \"zh\"\n            st.rerun()\n\n    # Use current language for all UI elements\n    language = st.session_state.language\n    st.title(get_text(\"app_title\", language))\n    st.sidebar.header(get_text(\"sidebar_header\", language))\n\n\nInternationalization Features: - Complete UI Translation: All interface elements localized - Dynamic Language Switching: Instant UI updates on language change - Chinese Character Support: Full Unicode and CJK support - Consistent Language: AI responses match UI language\n\n\n\n\n\n\nThe weather table combines data with visual elements for quick understanding:\n\n\nCode\ndef create_weather_table(weather_df, language=\"en\"):\n    \"\"\"Create enhanced weather table with icons and colors\"\"\"\n\n    # Weather code to emoji mapping\n    WEATHER_ICONS = {\n        0: \"☀️\",   # Clear sky\n        1: \"⛅\",   # Mainly clear\n        2: \"☁️\",   # Partly cloudy\n        3: \"☁️\",   # Overcast\n        45: \"🌫️\",  # Fog\n        48: \"🌦️\",  # Drizzle\n        51: \"🌧️\",  # Rain\n        53: \"❄️\",  # Snow\n        95: \"⛈️\",  # Thunderstorm\n    }\n\n    def format_weather_row(row):\n        \"\"\"Format individual weather row with styling\"\"\"\n        date_str = row['date'].strftime('%Y-%m-%d')\n        temp_range = f\"{row['temperature_min']:.1f}° ~ {row['temperature_max']:.1f}°\"\n        weather_icon = WEATHER_ICONS.get(row['weather_code'], \"🌡️\")\n\n        # Air quality badge\n        aq_info = get_air_quality_level(row['pm2_5'])\n        aq_badge = f'&lt;span style=\"background-color: {aq_info[\"color\"]}; color: {aq_info[\"text_color\"]}; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;\"&gt;{aq_info[\"icon\"]} {row[\"pm2_5\"]:.0f}&lt;/span&gt;'\n\n        # Rain probability indicator\n        rain_color = 'red' if row['rain_probability'] &gt;= 80 else 'orange' if row['rain_probability'] &gt;= 50 else 'gray'\n        rain_indicator = f'&lt;span style=\"color: {rain_color};\"&gt;🔴 {row[\"rain_probability\"]:.0f}%&lt;/span&gt;' if row['rain_probability'] &gt;= 50 else f'&lt;span style=\"color: {rain_color};\"&gt;🟢 {row[\"rain_probability\"]:.0f}%&lt;/span&gt;'\n\n        # Today highlighting\n        row_style = 'font-size: 1.2em; font-weight: bold;' if row['is_today'] else ''\n\n        return {\n            'Date': f'&lt;span style=\"{row_style}\"&gt;{date_str}&lt;/span&gt;',\n            'Weather': f'&lt;span style=\"{row_style}\"&gt;{weather_icon}&lt;/span&gt;',\n            'Temperature': f'&lt;span style=\"{row_style}\"&gt;{temp_range}&lt;/span&gt;',\n            'Wind': f'&lt;span style=\"{row_style}\"&gt;💨 {row[\"wind_speed_max\"]:.1f} km/h&lt;/span&gt;',\n            'Rain': rain_indicator,\n            'Air Quality': aq_badge\n        }\n\n    # Apply formatting to all rows\n    formatted_rows = [format_weather_row(row) for _, row in weather_df.iterrows()]\n\n    return pd.DataFrame(formatted_rows)\n\n# Display in Streamlit\nst.markdown(\"### 📊 Weather Details\")\nst.dataframe(\n    create_weather_table(weather_df, language),\n    width=1200,\n    hide_index=True,\n    unsafe_allow_html=True\n)\n\n\nTable Features: - Weather Icons: Emoji representation for quick visual understanding - Today Highlighting: Larger, bold text for current day - Air Quality Badges: Color-coded PM2.5 indicators - Rain Probability: Visual indicators with Material Design colors - Responsive Layout: Adapts to different screen sizes\n\n\n\n\n\n\nFor production deployment, configure environment variables:\n# .env file\nmodelscope=your_modelscope_api_key_here\n\n# Additional production settings\n# Consider rate limiting, caching, and monitoring\n\n\n\n# 1. Install Streamlit CLI\npip install streamlit\n\n# 2. Login to Streamlit\nstreamlit login\n\n# 3. Deploy to Streamlit Cloud\nstreamlit run app.py  # Test locally first\n# Then deploy through cloud.streamlit.io or using CLI\n\n\n\nFROM python:3.9-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nEXPOSE 8501\n\nCMD [\"streamlit\", \"run\", \"app.py\", \"--server.address\", \"0.0.0.0\"]\n\n\n\n\n\n\n@st.cache_data(ttl=3600)  # Cache for 1 hour\ndef get_weather_data_cached(lat, lon):\n    \"\"\"Cached weather data fetching\"\"\"\n    return get_weather_data(lat, lon)\n\n@st.cache_resource\ndef get_ai_client():\n    \"\"\"Cached AI client initialization\"\"\"\n    return init_ai_client()\n\n# Cache map generation\n@st.cache_data(ttl=3600)\ndef create_map_cached(lat, lon):\n    \"\"\"Cached map creation\"\"\"\n    return create_interactive_map(lat, lon)\n\n\n\n\n\nCode\ndef robust_api_call(func, *args, max_retries=3, **kwargs):\n    \"\"\"Robust API calling with retry logic\"\"\"\n    for attempt in range(max_retries):\n        try:\n            return func(*args, **kwargs)\n        except requests.exceptions.Timeout:\n            if attempt == max_retries - 1:\n                st.error(\"Weather service temporarily unavailable\")\n                return None\n            time.sleep(2 ** attempt)  # Exponential backoff\n        except requests.exceptions.RequestException as e:\n            st.error(f\"API error: {e}\")\n            return None\n\n\n\n\n\n\n\n\n\nModular Architecture: Separate concerns (API, UI, AI, Visualization)\nError Resilience: Comprehensive error handling and fallbacks\nUser Experience: Fast loading with caching and progress indicators\nInternationalization: Plan for multilingual support from the start\nAPI Management: Rate limiting and request optimization\nResponsive Design: Test across different devices and screen sizes\nSecurity: Environment variables for sensitive data\n\n\n\n\n\nData Caching: 1-hour TTL for weather data\nLazy Loading: Load components only when needed\nAsync Operations: Non-blocking API calls where possible\nOptimization: Minimize re-renders and state updates\n\n\n\n\n\n\n\n\nExtended AI Capabilities:\n\nMulti-day activity planning\nPersonalized recommendations based on user preferences\nIntegration with calendar applications\n\nAdvanced Visualizations:\n\nWind direction and speed maps\nPrecipitation intensity charts\nHistorical weather comparisons\n\nData Sources:\n\nMultiple weather provider integration\nReal-time radar integration\nWeather alert systems\n\nUser Features:\n\nSaved locations and favorites\nWeather notifications\nHistorical data analysis\nExport functionality\n\nTechnical Enhancements:\n\nWebSocket real-time updates\nProgressive Web App (PWA) features\nOffline functionality\n\n\n\n\n\n\nThis weather forecast application demonstrates how modern web development technologies can be combined to create a comprehensive, intelligent weather service. The integration of AI-powered recommendations elevates it from a simple data display tool to a practical weather assistant that helps users make informed decisions about their daily activities.\nThe project showcases:\n\nModern Web Development: Streamlit for rapid prototyping\nData Visualization: Professional charts and interactive maps\nAI Integration: Practical use of language models for data analysis\nInternationalization: Complete bilingual support\nProduction Readiness: Error handling, caching, and optimization\n\nWhether you’re building weather applications, data dashboards, or AI-powered tools, this project provides an excellent foundation for creating sophisticated, user-friendly applications."
  },
  {
    "objectID": "posts/weather-trend/index.html#technical-architecture",
    "href": "posts/weather-trend/index.html#technical-architecture",
    "title": "Weather Forecast App with Streamlit and AI Integration",
    "section": "",
    "text": "flowchart TD\n    A[User Interface&lt;br/&gt;Streamlit Web App] --&gt; B[Location Services]\n    A --&gt; C[Weather Data API]\n    A --&gt; D[AI Integration Layer]\n    A --&gt; E[Visualization Engine]\n\n    B --&gt; F[Interactive Map&lt;br/&gt;Folium + OpenStreetMap]\n    B --&gt; G[City Search&lt;br/&gt;Nominatim Geocoding]\n\n    C --&gt; H[Open-Meteo API&lt;br/&gt;Weather + Air Quality]\n    C --&gt; I[Data Processing&lt;br/&gt;Pandas DataFrame]\n\n    D --&gt; J[ModelScope API&lt;br/&gt;DeepSeek-V3.2 AI]\n    D --&gt; K[Intelligent Analysis&lt;br/&gt;Weather Recommendations]\n\n    E --&gt; L[Altair Charts&lt;br/&gt;Temperature Trends]\n    E --&gt; M[Data Tables&lt;br/&gt;Weather Details]\n\n    F --&gt; A\n    G --&gt; A\n    I --&gt; E\n    K --&gt; A\n\n\n Weather App Architecture \n\n\n\n\n\n\nFrontend Framework: Streamlit for rapid web application development\nMapping: Folium with OpenStreetMap tiles for interactive location selection\nData Visualization: Altair for professional charts and graphs\nAPI Integration: Open-Meteo for weather and air quality data\nAI Services: ModelScope API with DeepSeek-V3.2 for intelligent analysis\nGeocoding: Nominatim for address-to-coordinate conversion\nInternationalization: Custom language system for English/Chinese support"
  },
  {
    "objectID": "posts/weather-trend/index.html#getting-started",
    "href": "posts/weather-trend/index.html#getting-started",
    "title": "Weather Forecast App with Streamlit and AI Integration",
    "section": "",
    "text": "Before we dive into the code, let’s set up our development environment:\n# 1. Clone the repository\ngit clone &lt;your-repo-url&gt;\ncd weather_trend\n\n# 2. Install required packages\npip install streamlit pandas requests altair folium streamlit-folium geopy python-dotenv openai\n\n# 3. Set up environment variables for AI features\necho \"modelscope=your_api_key_here\" &gt; .env\n\n\n\n\nstreamlit: Web application framework with reactive UI components\npandas: Data manipulation and analysis for weather datasets\nrequests: HTTP client for API communication\naltair: Declarative statistical visualization library\nfolium + streamlit-folium: Interactive map integration\ngeopy: Geocoding services for location lookup\npython-dotenv: Environment variable management\nopenai: AI model integration (compatible with ModelScope)"
  },
  {
    "objectID": "posts/weather-trend/index.html#core-features-implementation",
    "href": "posts/weather-trend/index.html#core-features-implementation",
    "title": "Weather Forecast App with Streamlit and AI Integration",
    "section": "",
    "text": "The map functionality allows users to click anywhere and get weather data for that exact location:\n\n\nCode\nimport folium\nfrom streamlit_folium import st_folium\n\n# Create interactive map centered on default location\ndef create_interactive_map(lat=40.7128, lon=-74.0060, zoom=10):\n    \"\"\"Create an interactive Folium map\"\"\"\n    m = folium.Map(\n        location=[lat, lon],\n        zoom_start=zoom,\n        tiles=\"OpenStreetMap\"\n    )\n\n    # Add click event handler to capture coordinates\n    m.add_child(folium.LatLngPopup())\n\n    return m\n\n# Display map in Streamlit\nif 'map_data' not in st.session_state:\n    st.session_state.map_data = None\n\nmap_object = create_interactive_map()\nst_data = st_folium(map_object, width=700, height=500)\n\n# Capture clicked coordinates\nif st_data['last_clicked']:\n    lat = st_data['last_clicked']['lat']\n    lon = st_data['last_clicked']['lng']\n    st.session_state.clicked_location = (lat, lon)\n    get_weather_for_coordinates(lat, lon)\n\n\nKey Features: - Click-to-Select: Users can click anywhere on the map - Zoom Controls: Standard map navigation - Responsive Design: Adapts to different screen sizes - Coordinate Capture: Automatic extraction of clicked locations\n\n\n\nThe app supports both English and Chinese city names with intelligent fallback:\n\n\nCode\nimport re\nfrom geopy.geocoders import Nominatim\n\n# Extended Chinese character detection\ndef contains_chinese(text):\n    \"\"\"Check if text contains Chinese characters including CJK Unified Ideographs\"\"\"\n    chinese_pattern = re.compile(\n        r\"[\\u4e00-\\u9fff\\u3400-\\u4dbf\\U00020000-\\U0002a6df\\U0002a700-\\U0002b73f]\"\n    )\n    return bool(chinese_pattern.search(text))\n\n# Chinese city mapping for better geocoding\nCHINESE_CITY_MAPPING = {\n    \"纽约\": \"New York\",\n    \"东京\": \"Tokyo\",\n    \"伦敦\": \"London\",\n    \"巴黎\": \"Paris\",\n    \"洛杉矶\": \"Los Angeles\",\n    # ... more mappings\n}\n\ndef get_coordinates_for_city(city_name):\n    \"\"\"Get coordinates for city with multilingual support\"\"\"\n    # Check for Chinese city name mapping\n    if contains_chinese(city_name) and city_name in CHINESE_CITY_MAPPING:\n        city_name = CHINESE_CITY_MAPPING[city_name]\n\n    # Geocode the city\n    geolocator = Nominatim(user_agent=\"weather_app\")\n    try:\n        location = geolocator.geocode(city_name)\n        return (location.latitude, location.longitude) if location else None\n    except:\n        return None\n\n\nBilingual Features: - Chinese Character Detection: Advanced regex pattern for CJK characters - City Name Mapping: Translation database for major cities - Fallback Handling: Graceful degradation when geocoding fails - Unicode Support: Full international character support\n\n\n\nThe application fetches comprehensive weather data from Open-Meteo API:\n\n\nCode\nimport requests\nimport pandas as pd\n\ndef get_weather_data(latitude, longitude):\n    \"\"\"Fetch 12-day weather data (7 historical + 5 forecast)\"\"\"\n\n    # API endpoint configuration\n    url = \"https://api.open-meteo.com/v1/forecast\"\n    params = {\n        'latitude': latitude,\n        'longitude': longitude,\n        'daily': [\n            'temperature_2m_max', 'temperature_2m_min', 'temperature_2m_mean',\n            'weathercode', 'windspeed_10m_max', 'precipitation_probability_max',\n            'pm10', 'pm2_5'\n        ],\n        'timezone': 'auto',\n        'past_days': 7,  # Get historical data\n        'forecast_days': 5  # Get future forecast\n    }\n\n    try:\n        response = requests.get(url, params=params, timeout=10)\n        response.raise_for_status()\n\n        # Process response data\n        data = response.json()\n        df = process_weather_data(data)\n\n        return df\n\n    except requests.RequestException as e:\n        st.error(f\"API request failed: {e}\")\n        return None\n\ndef process_weather_data(data):\n    \"\"\"Convert API response to structured DataFrame\"\"\"\n    daily_data = data['daily']\n\n    # Create date range (historical + future)\n    dates = pd.date_range(\n        start=pd.to_datetime(daily_data['time'][0]),\n        periods=len(daily_data['time']),\n        freq='D'\n    )\n\n    # Build DataFrame\n    df = pd.DataFrame({\n        'date': dates,\n        'temperature_max': daily_data['temperature_2m_max'],\n        'temperature_min': daily_data['temperature_2m_min'],\n        'temperature_mean': daily_data['temperature_2m_mean'],\n        'weather_code': daily_data['weathercode'],\n        'wind_speed_max': daily_data['windspeed_10m_max'],\n        'rain_probability': daily_data['precipitation_probability_max'],\n        'pm2_5': daily_data.get('pm2_5', [0] * len(dates))\n    })\n\n    # Add derived columns\n    df['is_today'] = df['date'].dt.date == pd.Timestamp.now().date()\n    df['is_future'] = df['date'] &gt; pd.Timestamp.now()\n\n    return df\n\n\nData Processing Features: - Historical + Forecast: 7 days past + 5 days future - Air Quality Integration: PM2.5 and PM10 data - Derived Metrics: Today detection and future projection - Error Handling: Robust API error management\n\n\n\nTemperature trends with clear visual distinction between historical and forecast data:\n\n\nCode\nimport altair as alt\n\ndef create_temperature_chart(weather_df):\n    \"\"\"Create interactive temperature trend chart\"\"\"\n\n    # Base chart with temperature line\n    base_chart = alt.Chart(weather_df).mark_line(\n        point=True,\n        strokeWidth=3,\n        opacity=0.8\n    ).encode(\n        x=alt.X('date:T', title='Date'),\n        y=alt.Y('temperature_mean:Q', title='Temperature (°C)', scale=alt.Scale(domain=[weather_df['temperature_min'].min()-5, weather_df['temperature_max'].max()+5]))\n    ).properties(\n        width=800,\n        height=400,\n        title='Temperature Trends'\n    )\n\n    # Historical data (solid line)\n    historical_data = weather_df[weather_df['is_future'] == False]\n    historical_line = base_chart.transform_filter(\n        'datum.is_future == false'\n    ).encode(\n        color=alt.value('blue'),\n        strokeDash=alt.value([0])  # Solid line\n    )\n\n    # Future forecast (dotted line)\n    future_data = weather_df[weather_df['is_future'] == True]\n    future_line = base_chart.transform_filter(\n        'datum.is_future == true'\n    ).encode(\n        color=alt.value('red'),\n        strokeDash=alt.value([5, 5])  # Dotted line\n    )\n\n    # Today indicator\n    today_line = alt.Chart(pd.DataFrame({'x': [pd.Timestamp.now()]})).mark_rule(\n        strokeDash=[2, 2],\n        stroke='green',\n        strokeWidth=2\n    ).encode(x='x:T')\n\n    return (historical_line + future_line + today_line).resolve_scale(color='independent')\n\n\nVisualization Features: - Line Style Differentiation: Solid (historical) vs Dotted (forecast) - Today Indicator: Clear visual marker for current day - Color Coding: Blue for past, red for future - Interactive Tooltips: Hover information for data points\n\n\n\nPM2.5 levels with EPA-compliant color coding:\n\n\nCode\ndef get_air_quality_level(pm25_value):\n    \"\"\"Get air quality level based on US EPA PM2.5 standards\"\"\"\n\n    if pm25_value &lt;= 12:\n        return {\n            'level': 'Good',\n            'color': '#00e400',  # Deeper green\n            'text_color': 'white',\n            'icon': '🟢'\n        }\n    elif pm25_value &lt;= 35.4:\n        return {\n            'level': 'Moderate',\n            'color': '#ffff00',  # Light green\n            'text_color': 'black',\n            'icon': '🟢'\n        }\n    elif pm25_value &lt;= 55.4:\n        return {\n            'level': 'Unhealthy for Sensitive Groups',\n            'color': '#ff7e00',  # Darker yellow\n            'text_color': 'black',\n            'icon': '🟡'\n        }\n    elif pm25_value &lt;= 150.4:\n        return {\n            'level': 'Unhealthy',\n            'color': '#ff0000',  # Darker orange\n            'text_color': 'white',\n            'icon': '🟠'\n        }\n    elif pm25_value &lt;= 250.4:\n        return {\n            'level': 'Very Unhealthy',\n            'color': '#8f3f97',  # Darker red\n            'text_color': 'white',\n            'icon': '🔴'\n        }\n    else:\n        return {\n            'level': 'Hazardous',\n            'color': '#7e0023',  # Very dark red\n            'text_color': 'white',\n            'icon': '⚫'\n        }\n\ndef display_air_quality_badge(pm25_value):\n    \"\"\"Display air quality with visual indicators\"\"\"\n    aq_info = get_air_quality_level(pm25_value)\n\n    st.markdown(f\"\"\"\n    &lt;div style=\"background-color: {aq_info['color']}; color: {aq_info['text_color']};\n                padding: 10px; border-radius: 5px; text-align: center; margin: 5px 0;\"&gt;\n        &lt;strong&gt;{aq_info['icon']} PM2.5: {pm25_value} μg/m³&lt;/strong&gt;&lt;br&gt;\n        &lt;small&gt;{aq_info['level']}&lt;/small&gt;\n    &lt;/div&gt;\n    \"\"\", unsafe_allow_html=True)\n\n\nAir Quality Features: - EPA Standards: Based on US Environmental Protection Agency - Visual Indicators: Color-coded badges with icons - Accessibility: High contrast colors for readability - Educational: Level descriptions for user understanding"
  },
  {
    "objectID": "posts/weather-trend/index.html#ai-powered-weather-intelligence",
    "href": "posts/weather-trend/index.html#ai-powered-weather-intelligence",
    "title": "Weather Forecast App with Streamlit and AI Integration",
    "section": "",
    "text": "The most innovative feature is AI-powered weather analysis using ModelScope’s DeepSeek-V3.2 model:\n\n\n\nAI Weather Recommendations\n\n\n\n\nCode\nfrom openai import OpenAI\nimport os\n\n# Initialize AI client with ModelScope\ndef init_ai_client():\n    \"\"\"Initialize OpenAI client for ModelScope API\"\"\"\n    return OpenAI(\n        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n        api_key=os.getenv(\"modelscope\"),\n    )\n\ndef generate_weather_insights(weather_df, location_name, language=\"en\"):\n    \"\"\"Generate AI-powered weather recommendations\"\"\"\n\n    # Prepare weather data for AI analysis\n    today_index = weather_df[weather_df['is_today']].index[0] if any(weather_df['is_today']) else 0\n    historical_data = weather_df.iloc[:today_index+1]  # Up to today\n    future_data = weather_df.iloc[today_index:]  # Today onwards\n\n    # Create weather summary for AI\n    weather_summary = f\"\"\"\n    Location: {location_name}\n\n    Historical Weather (Last {len(historical_data)} days):\n    - Temperature Range: {historical_data['temperature_min'].min():.1f}°C to {historical_data['temperature_max'].max():.1f}°C\n    - Average Temperature: {historical_data['temperature_mean'].mean():.1f}°C\n    - Air Quality Range: {historical_data['pm2_5'].min():.1f} to {historical_data['pm2_5'].max():.1f} PM2.5\n\n    Forecast (Next {len(future_data)} days):\n    - Temperature Range: {future_data['temperature_min'].min():.1f}°C to {future_data['temperature_max'].max():.1f}°C\n    - Average Rain Probability: {future_data['rain_probability'].mean():.0f}%\n    \"\"\"\n\n    # Generate prompt based on language\n    if language == \"zh\":\n        prompt = f\"\"\"\n        基于以下天气数据，请提供简洁实用的天气建议（100-200字）：\n\n        {weather_summary}\n\n        请包括：\n        1. 天气模式分析\n        2. 穿衣建议\n        3. 户外活动建议\n        4. 健康注意事项（如空气质量相关）\n\n        请用中文回复，语气友好实用。\n        \"\"\"\n    else:\n        prompt = f\"\"\"\n        Based on the following weather data, please provide concise and practical weather advice (100-200 words):\n\n        {weather_summary}\n\n        Please include:\n        1. Weather pattern analysis\n        2. Clothing recommendations\n        3. Outdoor activity suggestions\n        4. Health considerations (related to air quality if applicable)\n\n        Please respond in {language} with a friendly and practical tone.\n        \"\"\"\n\n    try:\n        client = init_ai_client()\n        response = client.chat.completions.create(\n            model=\"deepseek-ai/DeepSeek-V3\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=300,\n            temperature=0.7\n        )\n\n        return response.choices[0].message.content\n\n    except Exception as e:\n        return f\"AI service temporarily unavailable. Error: {str(e)}\"\n\n# In Streamlit app\nif st.button(get_text(\"ai_button\", language)):\n    with st.spinner(\"Getting AI weather advice...\"):\n        if 'weather_df' in st.session_state and 'location_name' in st.session_state:\n            ai_insights = generate_weather_insights(\n                st.session_state.weather_df,\n                st.session_state.location_name,\n                language\n            )\n            st.markdown(\"### 🤖 AI Weather Analysis\")\n            st.write(ai_insights)\n        else:\n            st.warning(\"Please get weather data first.\")\n\n\nAI Features: - Context-Aware Analysis: Processes both historical and forecast data - Multilingual Support: AI responds in user’s selected language - Practical Recommendations: Clothing, activities, and health advice - Error Handling: Graceful fallback when AI service unavailable\n\n\n\nThe AI system uses carefully crafted prompts to generate useful insights:\nPrompt Structure: 1. Data Context: Comprehensive weather statistics 2. Task Definition: Clear requirements for analysis 3. Output Format: Structured response categories 4. Language Adaptation: Matches user interface language\nResponse Categories: - Weather Pattern Analysis: Trends and anomalies - Clothing Recommendations: Practical dress suggestions - Activity Advice: Outdoor planning recommendations - Health Considerations: Air quality and weather impacts"
  },
  {
    "objectID": "posts/weather-trend/index.html#internationalization-system",
    "href": "posts/weather-trend/index.html#internationalization-system",
    "title": "Weather Forecast App with Streamlit and AI Integration",
    "section": "",
    "text": "The app implements a comprehensive bilingual system:\n\n\nCode\n# language.py - Translation management\nTRANSLATIONS = {\n    \"en\": {\n        \"app_title\": \"Weather Forecast App\",\n        \"sidebar_header\": \"Weather Query\",\n        \"city_input_placeholder\": \"Enter a city name\",\n        \"get_weather_button\": \"Get Weather\",\n        \"weather_trends_title\": \"Weather Trends for\",\n        \"ai_button\": \"AI Weather Advice\",\n        # ... more translations\n    },\n    \"zh\": {\n        \"app_title\": \"天气预报应用\",\n        \"sidebar_header\": \"天气查询\",\n        \"city_input_placeholder\": \"输入城市名称\",\n        \"get_weather_button\": \"获取天气\",\n        \"weather_trends_title\": \"天气趋势\",\n        \"ai_button\": \"AI天气建议\",\n        # ... more translations\n    }\n}\n\ndef get_text(key, language=\"en\"):\n    \"\"\"Get translated text for given key and language\"\"\"\n    return TRANSLATIONS.get(language, {}).get(key, key)\n\ndef get_available_languages():\n    \"\"\"Get available language options\"\"\"\n    return {\"en\": \"English\", \"zh\": \"中文\"}\n\n# In main app (app.py)\ndef main():\n    # Language state management\n    if 'language' not in st.session_state:\n        st.session_state.language = \"en\"\n\n    # Language toggle button\n    current_lang = st.session_state.language\n    available_langs = get_available_languages()\n\n    col1, col2, col3 = st.columns([1,1,6])\n    with col1:\n        if st.button(\"EN\", disabled=current_lang==\"en\"):\n            st.session_state.language = \"en\"\n            st.rerun()\n\n    with col2:\n        if st.button(\"中文\", disabled=current_lang==\"zh\"):\n            st.session_state.language = \"zh\"\n            st.rerun()\n\n    # Use current language for all UI elements\n    language = st.session_state.language\n    st.title(get_text(\"app_title\", language))\n    st.sidebar.header(get_text(\"sidebar_header\", language))\n\n\nInternationalization Features: - Complete UI Translation: All interface elements localized - Dynamic Language Switching: Instant UI updates on language change - Chinese Character Support: Full Unicode and CJK support - Consistent Language: AI responses match UI language"
  },
  {
    "objectID": "posts/weather-trend/index.html#advanced-ui-components",
    "href": "posts/weather-trend/index.html#advanced-ui-components",
    "title": "Weather Forecast App with Streamlit and AI Integration",
    "section": "",
    "text": "The weather table combines data with visual elements for quick understanding:\n\n\nCode\ndef create_weather_table(weather_df, language=\"en\"):\n    \"\"\"Create enhanced weather table with icons and colors\"\"\"\n\n    # Weather code to emoji mapping\n    WEATHER_ICONS = {\n        0: \"☀️\",   # Clear sky\n        1: \"⛅\",   # Mainly clear\n        2: \"☁️\",   # Partly cloudy\n        3: \"☁️\",   # Overcast\n        45: \"🌫️\",  # Fog\n        48: \"🌦️\",  # Drizzle\n        51: \"🌧️\",  # Rain\n        53: \"❄️\",  # Snow\n        95: \"⛈️\",  # Thunderstorm\n    }\n\n    def format_weather_row(row):\n        \"\"\"Format individual weather row with styling\"\"\"\n        date_str = row['date'].strftime('%Y-%m-%d')\n        temp_range = f\"{row['temperature_min']:.1f}° ~ {row['temperature_max']:.1f}°\"\n        weather_icon = WEATHER_ICONS.get(row['weather_code'], \"🌡️\")\n\n        # Air quality badge\n        aq_info = get_air_quality_level(row['pm2_5'])\n        aq_badge = f'&lt;span style=\"background-color: {aq_info[\"color\"]}; color: {aq_info[\"text_color\"]}; padding: 2px 6px; border-radius: 3px; font-size: 0.8em;\"&gt;{aq_info[\"icon\"]} {row[\"pm2_5\"]:.0f}&lt;/span&gt;'\n\n        # Rain probability indicator\n        rain_color = 'red' if row['rain_probability'] &gt;= 80 else 'orange' if row['rain_probability'] &gt;= 50 else 'gray'\n        rain_indicator = f'&lt;span style=\"color: {rain_color};\"&gt;🔴 {row[\"rain_probability\"]:.0f}%&lt;/span&gt;' if row['rain_probability'] &gt;= 50 else f'&lt;span style=\"color: {rain_color};\"&gt;🟢 {row[\"rain_probability\"]:.0f}%&lt;/span&gt;'\n\n        # Today highlighting\n        row_style = 'font-size: 1.2em; font-weight: bold;' if row['is_today'] else ''\n\n        return {\n            'Date': f'&lt;span style=\"{row_style}\"&gt;{date_str}&lt;/span&gt;',\n            'Weather': f'&lt;span style=\"{row_style}\"&gt;{weather_icon}&lt;/span&gt;',\n            'Temperature': f'&lt;span style=\"{row_style}\"&gt;{temp_range}&lt;/span&gt;',\n            'Wind': f'&lt;span style=\"{row_style}\"&gt;💨 {row[\"wind_speed_max\"]:.1f} km/h&lt;/span&gt;',\n            'Rain': rain_indicator,\n            'Air Quality': aq_badge\n        }\n\n    # Apply formatting to all rows\n    formatted_rows = [format_weather_row(row) for _, row in weather_df.iterrows()]\n\n    return pd.DataFrame(formatted_rows)\n\n# Display in Streamlit\nst.markdown(\"### 📊 Weather Details\")\nst.dataframe(\n    create_weather_table(weather_df, language),\n    width=1200,\n    hide_index=True,\n    unsafe_allow_html=True\n)\n\n\nTable Features: - Weather Icons: Emoji representation for quick visual understanding - Today Highlighting: Larger, bold text for current day - Air Quality Badges: Color-coded PM2.5 indicators - Rain Probability: Visual indicators with Material Design colors - Responsive Layout: Adapts to different screen sizes"
  },
  {
    "objectID": "posts/weather-trend/index.html#deployment-and-production",
    "href": "posts/weather-trend/index.html#deployment-and-production",
    "title": "Weather Forecast App with Streamlit and AI Integration",
    "section": "",
    "text": "For production deployment, configure environment variables:\n# .env file\nmodelscope=your_modelscope_api_key_here\n\n# Additional production settings\n# Consider rate limiting, caching, and monitoring\n\n\n\n# 1. Install Streamlit CLI\npip install streamlit\n\n# 2. Login to Streamlit\nstreamlit login\n\n# 3. Deploy to Streamlit Cloud\nstreamlit run app.py  # Test locally first\n# Then deploy through cloud.streamlit.io or using CLI\n\n\n\nFROM python:3.9-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nEXPOSE 8501\n\nCMD [\"streamlit\", \"run\", \"app.py\", \"--server.address\", \"0.0.0.0\"]"
  },
  {
    "objectID": "posts/weather-trend/index.html#performance-optimization",
    "href": "posts/weather-trend/index.html#performance-optimization",
    "title": "Weather Forecast App with Streamlit and AI Integration",
    "section": "",
    "text": "@st.cache_data(ttl=3600)  # Cache for 1 hour\ndef get_weather_data_cached(lat, lon):\n    \"\"\"Cached weather data fetching\"\"\"\n    return get_weather_data(lat, lon)\n\n@st.cache_resource\ndef get_ai_client():\n    \"\"\"Cached AI client initialization\"\"\"\n    return init_ai_client()\n\n# Cache map generation\n@st.cache_data(ttl=3600)\ndef create_map_cached(lat, lon):\n    \"\"\"Cached map creation\"\"\"\n    return create_interactive_map(lat, lon)\n\n\n\n\n\nCode\ndef robust_api_call(func, *args, max_retries=3, **kwargs):\n    \"\"\"Robust API calling with retry logic\"\"\"\n    for attempt in range(max_retries):\n        try:\n            return func(*args, **kwargs)\n        except requests.exceptions.Timeout:\n            if attempt == max_retries - 1:\n                st.error(\"Weather service temporarily unavailable\")\n                return None\n            time.sleep(2 ** attempt)  # Exponential backoff\n        except requests.exceptions.RequestException as e:\n            st.error(f\"API error: {e}\")\n            return None"
  },
  {
    "objectID": "posts/weather-trend/index.html#best-practices-and-lessons-learned",
    "href": "posts/weather-trend/index.html#best-practices-and-lessons-learned",
    "title": "Weather Forecast App with Streamlit and AI Integration",
    "section": "",
    "text": "Modular Architecture: Separate concerns (API, UI, AI, Visualization)\nError Resilience: Comprehensive error handling and fallbacks\nUser Experience: Fast loading with caching and progress indicators\nInternationalization: Plan for multilingual support from the start\nAPI Management: Rate limiting and request optimization\nResponsive Design: Test across different devices and screen sizes\nSecurity: Environment variables for sensitive data\n\n\n\n\n\nData Caching: 1-hour TTL for weather data\nLazy Loading: Load components only when needed\nAsync Operations: Non-blocking API calls where possible\nOptimization: Minimize re-renders and state updates"
  },
  {
    "objectID": "posts/weather-trend/index.html#future-enhancements",
    "href": "posts/weather-trend/index.html#future-enhancements",
    "title": "Weather Forecast App with Streamlit and AI Integration",
    "section": "",
    "text": "Extended AI Capabilities:\n\nMulti-day activity planning\nPersonalized recommendations based on user preferences\nIntegration with calendar applications\n\nAdvanced Visualizations:\n\nWind direction and speed maps\nPrecipitation intensity charts\nHistorical weather comparisons\n\nData Sources:\n\nMultiple weather provider integration\nReal-time radar integration\nWeather alert systems\n\nUser Features:\n\nSaved locations and favorites\nWeather notifications\nHistorical data analysis\nExport functionality\n\nTechnical Enhancements:\n\nWebSocket real-time updates\nProgressive Web App (PWA) features\nOffline functionality"
  },
  {
    "objectID": "posts/weather-trend/index.html#conclusion",
    "href": "posts/weather-trend/index.html#conclusion",
    "title": "Weather Forecast App with Streamlit and AI Integration",
    "section": "",
    "text": "This weather forecast application demonstrates how modern web development technologies can be combined to create a comprehensive, intelligent weather service. The integration of AI-powered recommendations elevates it from a simple data display tool to a practical weather assistant that helps users make informed decisions about their daily activities.\nThe project showcases:\n\nModern Web Development: Streamlit for rapid prototyping\nData Visualization: Professional charts and interactive maps\nAI Integration: Practical use of language models for data analysis\nInternationalization: Complete bilingual support\nProduction Readiness: Error handling, caching, and optimization\n\nWhether you’re building weather applications, data dashboards, or AI-powered tools, this project provides an excellent foundation for creating sophisticated, user-friendly applications."
  },
  {
    "objectID": "posts/whisky-tasting/index.html",
    "href": "posts/whisky-tasting/index.html",
    "title": "Whisky Tasting note with RAG",
    "section": "",
    "text": "The whisky tasting application is an advanced AI-powered web application that generates detailed, professional whisky tasting notes and recommendations. What makes this system special is its multi-agent architecture with specialized AI personalities, each trained on different whisky review sources and linguistic styles.\nLive Demo(modelscope): https://modelscope.cn/studios/ttflying/whisky_AI_tasting\nLive Demo(shinyapp): https://jcflyingco.shinyapps.io/ai-whisky-tasting/\nGithub: https://github.com/JCwinning/whisky_tasting\n\nAI tasting noteAI recommendationAI品鉴词AI推荐\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe application features three specialized AI agents, each with unique characteristics and data sources:\n\nDrunkTony (dt) - Chinese-language agent specializing in Chinese whisky reviews\nWhiskyFunny (wf) - English-language agent with data from whiskyfun.com\nWhiskyNotebook (wn) - English-language agent with data from whiskynotes.be\n\n\n\n\n\nPrimary Language: Python 3.13+\nWeb Framework: Streamlit (primary) + Shiny for Python (alternative)\nDatabase: DuckDB (376MB, optimized for vector operations)\nAI/ML: OpenAI API, Vector embeddings, RAG system\nData Sources: Web scraping with BeautifulSoup4\n\n\n\n\n\n\n\nThe system uses DuckDB as its primary database, chosen for its excellent performance with vector operations:\n-- Database structure\nCREATE TABLE drinktony_embed (\n    full TEXT,\n    bottle_embedding FLOAT[1024]  -- 1024-dimensional vectors\n);\n\nCREATE TABLE whiskyfun_embed (\n    full TEXT,\n    bottle_embedding FLOAT[1024]\n);\n\nCREATE TABLE whiskynote_embed (\n    full TEXT,\n    bottle_embedding FLOAT[1024]\n);\n\n\n\n\n\nCode\n# Web scraping example from get_data_dt.py\ndef scrape_drinktony_reviews():\n    \"\"\"Scrape Chinese whisky reviews from drinktony.netlify.app\"\"\"\n    url = \"https://drinktony.netlify.app/\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, \"html.parser\")\n\n    # Find all review links\n    post_links = [urljoin(url, a.get(\"href\"))\n                 for a in soup.select(\"a.quarto-grid-link\")]\n\n    all_reviews = []\n    for post_url in post_links:\n        response = requests.get(post_url)\n        soup = BeautifulSoup(response.content, \"html.parser\")\n\n        # Extract review sections\n        review_sections = soup.select(\"section.level1\")\n        for section in review_sections:\n            # Parse whisky name and review content\n            whisky_name = extract_whisky_name(section)\n            review_text = extract_review_text(section)\n            all_reviews.append({\n                'whisky': whisky_name,\n                'review': review_text\n            })\n\n    return all_reviews\n\n\n\n\n\n\n\n\nThe application uses state-of-the-art embedding technology to convert text into high-dimensional vectors for semantic similarity search.\n\n\nModel: BAAI/bge-large-zh-v1.5 - Dimensions: 1024 - Provider: SiliconFlow API - Purpose: Cross-lingual text understanding (works well for both Chinese and English)\n\n\n\n\nCross-lingual Capability: Excels at understanding both Chinese and English whisky terminology\nHigh Performance: Superior semantic understanding compared to generic embeddings\nEfficient Size: 1024 dimensions balance between performance and storage efficiency\nAPI Access: Stable service via SiliconFlow\n\n\n\n\nThe application uses BGE-Large-ZH-v1.5 model for generating embeddings:\n\n\nCode\ndef get_embedding(text: str, api_key: str):\n    \"\"\"Generate text embeddings using SiliconFlow API\"\"\"\n    client = OpenAI(\n        api_key=api_key,\n        base_url=\"https://api.siliconflow.cn/v1\"\n    )\n    response = client.embeddings.create(\n        model=\"BAAI/bge-large-zh-v1.5\",\n        input=[text]\n    )\n    return np.array(response.data[0].embedding)\n\ndef cosine_similarity(v1, v2):\n    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n    if v1 is None or v2 is None:\n        return 0\n    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n\n\n\n\n\n\nThe Retrieval-Augmented Generation (RAG) system is the core innovation of this application. Let’s explore its components in detail.\n\n\n\nThe Retrieval-Augmented Generation process follows these steps:\n%%| fig-cap: \"RAG Workflow for Whisky Tasting\"\nflowchart TD\n    A[User Input&lt;br/&gt;Whisky Name] --&gt; B[Generate Embedding]\n    B --&gt; C[Vector Similarity Search&lt;br/&gt;in DuckDB]\n    C --&gt; D[Retrieve Top 10&lt;br/&gt;Similar Reviews]\n    D --&gt; E[Format Context for LLM]\n    E --&gt; F[Generate Tasting Notes&lt;br/&gt;with Primary LLM]\n    F --&gt; G[Generate Recommendations&lt;br/&gt;with Secondary LLM]\n    G --&gt; H[Display Results&lt;br/&gt;with Formatting]\n\n    C --&gt; I[Databases]\n    I --&gt; J[drinktony_embed&lt;br/&gt;1.2K Chinese reviews]\n    I --&gt; K[whiskyfun_embed&lt;br/&gt;20K English reviews]\n    I --&gt; L[whiskynote_embed&lt;br/&gt;5K English reviews]\n\n\n\n\n\nCode\ndef find_similar_chunks(\n    query_embedding,\n    db_path,\n    table_name,\n    text_col,\n    embedding_col,\n    top_n=10,\n):\n    \"\"\"Find most similar chunks in database using cosine similarity\"\"\"\n    try:\n        with duckdb.connect(database=db_path, read_only=True) as con:\n            df = con.execute(\n                f'SELECT \"{text_col}\", \"{embedding_col}\" FROM \"{table_name}\"'\n            ).fetchdf()\n\n            # Calculate similarities\n            similarities = []\n            for _, row in df.iterrows():\n                text = row[text_col]\n                embedding = np.array(row[embedding_col])\n                similarity = cosine_similarity(query_embedding, embedding)\n                similarities.append((text, similarity))\n\n            # Sort by similarity and return top N\n            similarities.sort(key=lambda x: x[1], reverse=True)\n            return similarities[:top_n]\n\n    except Exception as e:\n        print(f\"Error searching similar chunks: {e}\")\n        return []\n\n\n\n\n\n\n\n\n\n\nCode\ndef run_conversation(query, api_key, model):\n    \"\"\"Generate whisky tasting notes in Chinese format\"\"\"\n\n    # Step 1: Generate embedding and find similar reviews\n    query_embedding = get_embedding(query, api_key)\n    similar_reviews = find_similar_chunks(\n        query_embedding=query_embedding,\n        db_path=\"data/whisky_database.duckdb\",\n        table_name=\"drinktony_embed\",\n        text_col=\"full\",\n        embedding_col=\"bottle_embedding\"\n    )\n\n    # Step 2: Format context for LLM\n    context = \"\\n---\\n\".join([review[0] for review in similar_reviews])\n\n    # Step 3: Generate tasting notes\n    prompt = f\"\"\"作为威士忌专家，基于以下威士忌品鉴笔记，为\"{query}\"生成专业的品鉴报告：\n\n参考品鉴笔记：\n{context}\n\n请按以下格式输出：\n{query}\n闻香: [详细描述]\n品味: [详细描述]\n打分: [90-100分]\n\"\"\"\n\n    response = llm_client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": \"你是一位专业的威士忌品鉴师，擅长生成详细准确的品鉴笔记。\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0.7,\n        max_tokens=1000\n    )\n\n    return response.choices[0].message.content\n\n\n\n\n\n\n\nCode\ndef run_conversation(query, api_key, model):\n    \"\"\"Generate whisky tasting notes in English format\"\"\"\n\n    query_embedding = get_embedding(query, api_key)\n    similar_reviews = find_similar_chunks(\n        query_embedding=query_embedding,\n        db_path=\"data/whisky_database.duckdb\",\n        table_name=\"whiskyfun_embed\",\n        text_col=\"full\",\n        embedding_col=\"bottle_embedding\"\n    )\n\n    context = \"\\n---\\n\".join([review[0] for review in similar_reviews])\n\n    prompt = f\"\"\"As a whisky expert, generate professional tasting notes for \"{query}\" based on these reference reviews:\n\nReference reviews:\n{context}\n\nPlease output in this format:\nColour: [detailed description]\nNose: [detailed aroma description]\nMouth: [detailed taste description]\nFinish: [detailed finish description]\nComments: [overall impression]\nSGP: xxx - xx points\n\"\"\"\n\n    response = llm_client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a professional whisky taster specializing in detailed sensory analysis.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0.7,\n        max_tokens=1200\n    )\n\n    return response.choices[0].message.content\n\n\n\n\n\n\n\n\nThe application uses a separate LLM for generating whisky recommendations:\n\n\nCode\ndef recommend_whiskies_by_profile(tasting_notes, api_key, model):\n    \"\"\"Generate whisky recommendations based on tasting profile\"\"\"\n\n    prompt = f\"\"\"Based on these tasting notes, recommend 2 similar whiskies that the user might enjoy:\n\n{Tasting Notes:}\n{tasting_notes}\n\nPlease provide:\n1. Whisky name with brief description\n2. Why it matches the user's preference\n3. Price range and availability\n\nFormat each recommendation as:\n**Recommendation [1/2]:** [Whisky Name]\n**Why it matches:** [detailed reasoning]\n**Details:** [price, availability, tasting profile]\n\"\"\"\n\n    response = recommendation_client.chat.completions.create(\n        model=model,  # Different model for recommendations\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a whisky recommendation expert with deep knowledge of global whisky brands and profiles.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0.8,\n        max_tokens=800\n    )\n\n    return response.choices[0].message.content\n\n\n\n\n\n\n\n\n\n\nCode\n# Main application interface\ndef main():\n    st.set_page_config(\n        page_title=\"AI Whisky Tasting System\",\n        page_icon=\"🥃\",\n        layout=\"wide\"\n    )\n\n    # Sidebar for agent selection\n    with st.sidebar:\n        st.header(\"🥃 Whisky AI Tasting\")\n\n        # Agent selection\n        agent_type = st.selectbox(\n            \"Select Tasting Agent:\",\n            [\"DrunkTony (中文)\", \"WhiskyFunny (English)\", \"WhiskyNotebook (English)\"],\n            help=\"Each agent has different personality and data sources\"\n        )\n\n        # Model selection\n        model_options = get_model_options(agent_type)\n        selected_model = st.selectbox(\"Model:\", model_options)\n\n    # Main content area\n    st.header(\"Professional Whisky Tasting Notes Generator\")\n\n    # Input section\n    col1, col2 = st.columns([2, 1])\n    with col1:\n        whisky_name = st.text_input(\n            \"Enter whisky name:\",\n            placeholder=\"e.g., Macallan 18 Year Old\",\n            help=\"Enter the full whisky name including age and cask type\"\n        )\n\n    with col2:\n        st.write(\"\")  # Spacer\n        generate_btn = st.button(\"🍷 Generate Tasting Notes\", type=\"primary\")\n        recommend_btn = st.button(\"🎯 Get Recommendations\")\n\n    # Output sections\n    if generate_btn:\n        if not whisky_name:\n            st.error(\"Please enter a whisky name\")\n        else:\n            with st.spinner(\"Analyzing whisky...\"):\n                tasting_notes = generate_tasting_notes(whisky_name, agent_type, selected_model)\n                st.markdown(\"### 🥃 Tasting Notes\")\n                st.markdown(tasting_notes)\n\n    if recommend_btn:\n        with st.spinner(\"Finding recommendations...\"):\n            recommendations = get_recommendations(tasting_notes, agent_type)\n            st.markdown(\"### 🎯 Personalized Recommendations\")\n            st.markdown(recommendations)\n\n\n\n\n\n\n\n\n\n\nCode\n# Rate limiting implementation\nRATE_LIMIT_FILE = \"rate_limit.json\"\nMAX_RUNS_PER_DAY = 80\n\ndef get_rate_limit_data():\n    \"\"\"Get current usage data\"\"\"\n    try:\n        with open(RATE_LIMIT_FILE, \"r\") as f:\n            data = json.load(f)\n            # Ensure keys exist\n            if \"date\" not in data or \"count\" not in data:\n                return {\"date\": str(datetime.date.today()), \"count\": 0}\n            return data\n    except (FileNotFoundError, json.JSONDecodeError):\n        return {\"date\": str(datetime.date.today()), \"count\": 0}\n\ndef check_rate_limit():\n    \"\"\"Check if user has exceeded daily limit\"\"\"\n    data = get_rate_limit_data()\n    today = str(datetime.date.today())\n\n    if data[\"date\"] != today:\n        # Reset counter for new day\n        return {\"date\": today, \"count\": 0, \"can_run\": True}\n\n    if data[\"count\"] &gt;= MAX_RUNS_PER_DAY:\n        return {\"date\": today, \"count\": data[\"count\"], \"can_run\": False}\n\n    return {\"date\": today, \"count\": data[\"count\"], \"can_run\": True}\n\n\n\n\n\n\n\n\n\n\nCode\n# Optimized similarity search with batching\ndef batch_similarity_search(query_embedding, db_path, table_name, batch_size=1000):\n    \"\"\"Optimized similarity search with batching for large datasets\"\"\"\n\n    with duckdb.connect(database=db_path, read_only=True) as con:\n        # Get total row count\n        total_rows = con.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()[0]\n\n        all_similarities = []\n\n        # Process in batches to manage memory\n        for offset in range(0, total_rows, batch_size):\n            batch_df = con.execute(\n                f'SELECT full, bottle_embedding FROM {table_name} LIMIT {batch_size} OFFSET {offset}'\n            ).fetchdf()\n\n            # Calculate similarities for batch\n            for _, row in batch_df.iterrows():\n                embedding = np.array(row['bottle_embedding'])\n                similarity = cosine_similarity(query_embedding, embedding)\n                all_similarities.append((row['full'], similarity))\n\n        # Sort and return top results\n        all_similarities.sort(key=lambda x: x[1], reverse=True)\n        return all_similarities[:10]\n\n\n\n\n\n\n\n\n# Environment variables in .env file\nSILICONFLOW_API_KEY=your_siliconflow_key\nMODELSCOPE_API_KEY=your_modelscope_key\nGEMINI_API_KEY=your_gemini_key  # Optional\n\n# Database initialization\npython -c \"\nimport duckdb\nconn = duckdb.connect('data/whisky_database.duckdb')\n# Create tables with vector support\n\"\n\n\n\n\n\nCode\n# Alternative UI implementation using Shiny\nfrom shiny import App, ui, render, reactive, req\n\napp_ui = ui.page_fluid(\n    ui.h2(\"🥃 AI Whisky Tasting System\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_select(\"agent\", \"Select Agent:\", [\n                \"DrunkTony (中文)\", \"WhiskyFunny (English)\", \"WhiskyNotebook (English)\"\n            ]),\n            ui.input_text(\"whisky_name\", \"Whisky Name:\", \"\"),\n            ui.input_action_button(\"generate\", \"Generate Notes\")\n        ),\n        ui.output_text_verbatim(\"tasting_notes\"),\n        ui.output_text_verbatim(\"recommendations\")\n    )\n)\n\ndef server(input, output, session):\n    @output\n    @render.text\n    def tasting_notes():\n        req(input.generate())\n        # Generate tasting notes logic\n        return generate_tasting_notes(input.whisky_name(), input.agent())\n\napp = App(app_ui, server)\n\n\n\n\n\n\n\n\n\nMulti-Agent Architecture: Different AI personalities and data sources\nRAG Implementation: Retrieval-augmented generation with real whisky reviews\nVector Database: Efficient similarity search with 376MB database\nBilingual Support: Chinese and English language agents\nCost Management: Built-in rate limiting and API optimization\nProfessional Tasting Formats: Industry-standard note structures\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nDatabase Size\n376MB (26K+ reviews)\n\n\nEmbedding Dimension\n1024\n\n\nSearch Latency\n&lt;2 seconds\n\n\nDaily Request Limit\n80 requests\n\n\nSupport Languages\n2 (EN/ZH)\n\n\nData Sources\n3 (drinktony, whiskyfun, whiskynotes)\n\n\n\n\n\n\n\nPotential improvements for next versions:\n\nAdvanced Similarity Algorithms: Implement hybrid search with text + metadata\nUser Personalization: Learn from user preferences over time\nMobile App: Native iOS/Android applications\nReal-time Data Updates: Automated web scraping pipeline\nTasting Profile Analysis: Generate user taste profiles from preferences\nSocial Features: Share tasting notes and recommendations with community\n\n\n\n\nThis whisky tasting application demonstrates the power of combining RAG technology with specialized AI agents to create a sophisticated domain-specific system. The project showcases:\n\nAdvanced AI Architecture: Multi-agent system with different personalities\nData Engineering: Large-scale vector database management\nProfessional Knowledge: Industry-standard tasting note formats\nUser Experience: Clean, intuitive interfaces across platforms\nCost Efficiency: Smart rate limiting and optimization\n\nWhether you’re a whisky enthusiast, AI developer, or data scientist, this project provides an excellent example of building production-grade AI applications that combine real-world data with advanced machine learning techniques.\n\nTechnology Stack: Python, Streamlit, DuckDB, OpenAI API, Vector Embeddings\nDatabase: 26K+ real whisky reviews across 3 specialized sources"
  },
  {
    "objectID": "posts/whisky-tasting/index.html#core-architecture",
    "href": "posts/whisky-tasting/index.html#core-architecture",
    "title": "Whisky Tasting note with RAG",
    "section": "",
    "text": "The application features three specialized AI agents, each with unique characteristics and data sources:\n\nDrunkTony (dt) - Chinese-language agent specializing in Chinese whisky reviews\nWhiskyFunny (wf) - English-language agent with data from whiskyfun.com\nWhiskyNotebook (wn) - English-language agent with data from whiskynotes.be\n\n\n\n\n\nPrimary Language: Python 3.13+\nWeb Framework: Streamlit (primary) + Shiny for Python (alternative)\nDatabase: DuckDB (376MB, optimized for vector operations)\nAI/ML: OpenAI API, Vector embeddings, RAG system\nData Sources: Web scraping with BeautifulSoup4"
  },
  {
    "objectID": "posts/whisky-tasting/index.html#database-architecture",
    "href": "posts/whisky-tasting/index.html#database-architecture",
    "title": "Whisky Tasting note with RAG",
    "section": "",
    "text": "The system uses DuckDB as its primary database, chosen for its excellent performance with vector operations:\n-- Database structure\nCREATE TABLE drinktony_embed (\n    full TEXT,\n    bottle_embedding FLOAT[1024]  -- 1024-dimensional vectors\n);\n\nCREATE TABLE whiskyfun_embed (\n    full TEXT,\n    bottle_embedding FLOAT[1024]\n);\n\nCREATE TABLE whiskynote_embed (\n    full TEXT,\n    bottle_embedding FLOAT[1024]\n);\n\n\n\n\n\nCode\n# Web scraping example from get_data_dt.py\ndef scrape_drinktony_reviews():\n    \"\"\"Scrape Chinese whisky reviews from drinktony.netlify.app\"\"\"\n    url = \"https://drinktony.netlify.app/\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, \"html.parser\")\n\n    # Find all review links\n    post_links = [urljoin(url, a.get(\"href\"))\n                 for a in soup.select(\"a.quarto-grid-link\")]\n\n    all_reviews = []\n    for post_url in post_links:\n        response = requests.get(post_url)\n        soup = BeautifulSoup(response.content, \"html.parser\")\n\n        # Extract review sections\n        review_sections = soup.select(\"section.level1\")\n        for section in review_sections:\n            # Parse whisky name and review content\n            whisky_name = extract_whisky_name(section)\n            review_text = extract_review_text(section)\n            all_reviews.append({\n                'whisky': whisky_name,\n                'review': review_text\n            })\n\n    return all_reviews"
  },
  {
    "objectID": "posts/whisky-tasting/index.html#aiml-implementation",
    "href": "posts/whisky-tasting/index.html#aiml-implementation",
    "title": "Whisky Tasting note with RAG",
    "section": "",
    "text": "The application uses state-of-the-art embedding technology to convert text into high-dimensional vectors for semantic similarity search.\n\n\nModel: BAAI/bge-large-zh-v1.5 - Dimensions: 1024 - Provider: SiliconFlow API - Purpose: Cross-lingual text understanding (works well for both Chinese and English)\n\n\n\n\nCross-lingual Capability: Excels at understanding both Chinese and English whisky terminology\nHigh Performance: Superior semantic understanding compared to generic embeddings\nEfficient Size: 1024 dimensions balance between performance and storage efficiency\nAPI Access: Stable service via SiliconFlow\n\n\n\n\nThe application uses BGE-Large-ZH-v1.5 model for generating embeddings:\n\n\nCode\ndef get_embedding(text: str, api_key: str):\n    \"\"\"Generate text embeddings using SiliconFlow API\"\"\"\n    client = OpenAI(\n        api_key=api_key,\n        base_url=\"https://api.siliconflow.cn/v1\"\n    )\n    response = client.embeddings.create(\n        model=\"BAAI/bge-large-zh-v1.5\",\n        input=[text]\n    )\n    return np.array(response.data[0].embedding)\n\ndef cosine_similarity(v1, v2):\n    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n    if v1 is None or v2 is None:\n        return 0\n    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n\n\n\n\n\n\nThe Retrieval-Augmented Generation (RAG) system is the core innovation of this application. Let’s explore its components in detail.\n\n\n\nThe Retrieval-Augmented Generation process follows these steps:\n%%| fig-cap: \"RAG Workflow for Whisky Tasting\"\nflowchart TD\n    A[User Input&lt;br/&gt;Whisky Name] --&gt; B[Generate Embedding]\n    B --&gt; C[Vector Similarity Search&lt;br/&gt;in DuckDB]\n    C --&gt; D[Retrieve Top 10&lt;br/&gt;Similar Reviews]\n    D --&gt; E[Format Context for LLM]\n    E --&gt; F[Generate Tasting Notes&lt;br/&gt;with Primary LLM]\n    F --&gt; G[Generate Recommendations&lt;br/&gt;with Secondary LLM]\n    G --&gt; H[Display Results&lt;br/&gt;with Formatting]\n\n    C --&gt; I[Databases]\n    I --&gt; J[drinktony_embed&lt;br/&gt;1.2K Chinese reviews]\n    I --&gt; K[whiskyfun_embed&lt;br/&gt;20K English reviews]\n    I --&gt; L[whiskynote_embed&lt;br/&gt;5K English reviews]\n\n\n\n\n\nCode\ndef find_similar_chunks(\n    query_embedding,\n    db_path,\n    table_name,\n    text_col,\n    embedding_col,\n    top_n=10,\n):\n    \"\"\"Find most similar chunks in database using cosine similarity\"\"\"\n    try:\n        with duckdb.connect(database=db_path, read_only=True) as con:\n            df = con.execute(\n                f'SELECT \"{text_col}\", \"{embedding_col}\" FROM \"{table_name}\"'\n            ).fetchdf()\n\n            # Calculate similarities\n            similarities = []\n            for _, row in df.iterrows():\n                text = row[text_col]\n                embedding = np.array(row[embedding_col])\n                similarity = cosine_similarity(query_embedding, embedding)\n                similarities.append((text, similarity))\n\n            # Sort by similarity and return top N\n            similarities.sort(key=lambda x: x[1], reverse=True)\n            return similarities[:top_n]\n\n    except Exception as e:\n        print(f\"Error searching similar chunks: {e}\")\n        return []"
  },
  {
    "objectID": "posts/whisky-tasting/index.html#specialized-agent-implementation",
    "href": "posts/whisky-tasting/index.html#specialized-agent-implementation",
    "title": "Whisky Tasting note with RAG",
    "section": "",
    "text": "Code\ndef run_conversation(query, api_key, model):\n    \"\"\"Generate whisky tasting notes in Chinese format\"\"\"\n\n    # Step 1: Generate embedding and find similar reviews\n    query_embedding = get_embedding(query, api_key)\n    similar_reviews = find_similar_chunks(\n        query_embedding=query_embedding,\n        db_path=\"data/whisky_database.duckdb\",\n        table_name=\"drinktony_embed\",\n        text_col=\"full\",\n        embedding_col=\"bottle_embedding\"\n    )\n\n    # Step 2: Format context for LLM\n    context = \"\\n---\\n\".join([review[0] for review in similar_reviews])\n\n    # Step 3: Generate tasting notes\n    prompt = f\"\"\"作为威士忌专家，基于以下威士忌品鉴笔记，为\"{query}\"生成专业的品鉴报告：\n\n参考品鉴笔记：\n{context}\n\n请按以下格式输出：\n{query}\n闻香: [详细描述]\n品味: [详细描述]\n打分: [90-100分]\n\"\"\"\n\n    response = llm_client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": \"你是一位专业的威士忌品鉴师，擅长生成详细准确的品鉴笔记。\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0.7,\n        max_tokens=1000\n    )\n\n    return response.choices[0].message.content\n\n\n\n\n\n\n\nCode\ndef run_conversation(query, api_key, model):\n    \"\"\"Generate whisky tasting notes in English format\"\"\"\n\n    query_embedding = get_embedding(query, api_key)\n    similar_reviews = find_similar_chunks(\n        query_embedding=query_embedding,\n        db_path=\"data/whisky_database.duckdb\",\n        table_name=\"whiskyfun_embed\",\n        text_col=\"full\",\n        embedding_col=\"bottle_embedding\"\n    )\n\n    context = \"\\n---\\n\".join([review[0] for review in similar_reviews])\n\n    prompt = f\"\"\"As a whisky expert, generate professional tasting notes for \"{query}\" based on these reference reviews:\n\nReference reviews:\n{context}\n\nPlease output in this format:\nColour: [detailed description]\nNose: [detailed aroma description]\nMouth: [detailed taste description]\nFinish: [detailed finish description]\nComments: [overall impression]\nSGP: xxx - xx points\n\"\"\"\n\n    response = llm_client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a professional whisky taster specializing in detailed sensory analysis.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0.7,\n        max_tokens=1200\n    )\n\n    return response.choices[0].message.content"
  },
  {
    "objectID": "posts/whisky-tasting/index.html#recommendation-engine",
    "href": "posts/whisky-tasting/index.html#recommendation-engine",
    "title": "Whisky Tasting note with RAG",
    "section": "",
    "text": "The application uses a separate LLM for generating whisky recommendations:\n\n\nCode\ndef recommend_whiskies_by_profile(tasting_notes, api_key, model):\n    \"\"\"Generate whisky recommendations based on tasting profile\"\"\"\n\n    prompt = f\"\"\"Based on these tasting notes, recommend 2 similar whiskies that the user might enjoy:\n\n{Tasting Notes:}\n{tasting_notes}\n\nPlease provide:\n1. Whisky name with brief description\n2. Why it matches the user's preference\n3. Price range and availability\n\nFormat each recommendation as:\n**Recommendation [1/2]:** [Whisky Name]\n**Why it matches:** [detailed reasoning]\n**Details:** [price, availability, tasting profile]\n\"\"\"\n\n    response = recommendation_client.chat.completions.create(\n        model=model,  # Different model for recommendations\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a whisky recommendation expert with deep knowledge of global whisky brands and profiles.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0.8,\n        max_tokens=800\n    )\n\n    return response.choices[0].message.content"
  },
  {
    "objectID": "posts/whisky-tasting/index.html#user-interface-design",
    "href": "posts/whisky-tasting/index.html#user-interface-design",
    "title": "Whisky Tasting note with RAG",
    "section": "",
    "text": "Code\n# Main application interface\ndef main():\n    st.set_page_config(\n        page_title=\"AI Whisky Tasting System\",\n        page_icon=\"🥃\",\n        layout=\"wide\"\n    )\n\n    # Sidebar for agent selection\n    with st.sidebar:\n        st.header(\"🥃 Whisky AI Tasting\")\n\n        # Agent selection\n        agent_type = st.selectbox(\n            \"Select Tasting Agent:\",\n            [\"DrunkTony (中文)\", \"WhiskyFunny (English)\", \"WhiskyNotebook (English)\"],\n            help=\"Each agent has different personality and data sources\"\n        )\n\n        # Model selection\n        model_options = get_model_options(agent_type)\n        selected_model = st.selectbox(\"Model:\", model_options)\n\n    # Main content area\n    st.header(\"Professional Whisky Tasting Notes Generator\")\n\n    # Input section\n    col1, col2 = st.columns([2, 1])\n    with col1:\n        whisky_name = st.text_input(\n            \"Enter whisky name:\",\n            placeholder=\"e.g., Macallan 18 Year Old\",\n            help=\"Enter the full whisky name including age and cask type\"\n        )\n\n    with col2:\n        st.write(\"\")  # Spacer\n        generate_btn = st.button(\"🍷 Generate Tasting Notes\", type=\"primary\")\n        recommend_btn = st.button(\"🎯 Get Recommendations\")\n\n    # Output sections\n    if generate_btn:\n        if not whisky_name:\n            st.error(\"Please enter a whisky name\")\n        else:\n            with st.spinner(\"Analyzing whisky...\"):\n                tasting_notes = generate_tasting_notes(whisky_name, agent_type, selected_model)\n                st.markdown(\"### 🥃 Tasting Notes\")\n                st.markdown(tasting_notes)\n\n    if recommend_btn:\n        with st.spinner(\"Finding recommendations...\"):\n            recommendations = get_recommendations(tasting_notes, agent_type)\n            st.markdown(\"### 🎯 Personalized Recommendations\")\n            st.markdown(recommendations)"
  },
  {
    "objectID": "posts/whisky-tasting/index.html#rate-limiting-and-cost-management",
    "href": "posts/whisky-tasting/index.html#rate-limiting-and-cost-management",
    "title": "Whisky Tasting note with RAG",
    "section": "",
    "text": "Code\n# Rate limiting implementation\nRATE_LIMIT_FILE = \"rate_limit.json\"\nMAX_RUNS_PER_DAY = 80\n\ndef get_rate_limit_data():\n    \"\"\"Get current usage data\"\"\"\n    try:\n        with open(RATE_LIMIT_FILE, \"r\") as f:\n            data = json.load(f)\n            # Ensure keys exist\n            if \"date\" not in data or \"count\" not in data:\n                return {\"date\": str(datetime.date.today()), \"count\": 0}\n            return data\n    except (FileNotFoundError, json.JSONDecodeError):\n        return {\"date\": str(datetime.date.today()), \"count\": 0}\n\ndef check_rate_limit():\n    \"\"\"Check if user has exceeded daily limit\"\"\"\n    data = get_rate_limit_data()\n    today = str(datetime.date.today())\n\n    if data[\"date\"] != today:\n        # Reset counter for new day\n        return {\"date\": today, \"count\": 0, \"can_run\": True}\n\n    if data[\"count\"] &gt;= MAX_RUNS_PER_DAY:\n        return {\"date\": today, \"count\": data[\"count\"], \"can_run\": False}\n\n    return {\"date\": today, \"count\": data[\"count\"], \"can_run\": True}"
  },
  {
    "objectID": "posts/whisky-tasting/index.html#performance-optimization",
    "href": "posts/whisky-tasting/index.html#performance-optimization",
    "title": "Whisky Tasting note with RAG",
    "section": "",
    "text": "Code\n# Optimized similarity search with batching\ndef batch_similarity_search(query_embedding, db_path, table_name, batch_size=1000):\n    \"\"\"Optimized similarity search with batching for large datasets\"\"\"\n\n    with duckdb.connect(database=db_path, read_only=True) as con:\n        # Get total row count\n        total_rows = con.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()[0]\n\n        all_similarities = []\n\n        # Process in batches to manage memory\n        for offset in range(0, total_rows, batch_size):\n            batch_df = con.execute(\n                f'SELECT full, bottle_embedding FROM {table_name} LIMIT {batch_size} OFFSET {offset}'\n            ).fetchdf()\n\n            # Calculate similarities for batch\n            for _, row in batch_df.iterrows():\n                embedding = np.array(row['bottle_embedding'])\n                similarity = cosine_similarity(query_embedding, embedding)\n                all_similarities.append((row['full'], similarity))\n\n        # Sort and return top results\n        all_similarities.sort(key=lambda x: x[1], reverse=True)\n        return all_similarities[:10]"
  },
  {
    "objectID": "posts/whisky-tasting/index.html#deployment-and-configuration",
    "href": "posts/whisky-tasting/index.html#deployment-and-configuration",
    "title": "Whisky Tasting note with RAG",
    "section": "",
    "text": "# Environment variables in .env file\nSILICONFLOW_API_KEY=your_siliconflow_key\nMODELSCOPE_API_KEY=your_modelscope_key\nGEMINI_API_KEY=your_gemini_key  # Optional\n\n# Database initialization\npython -c \"\nimport duckdb\nconn = duckdb.connect('data/whisky_database.duckdb')\n# Create tables with vector support\n\"\n\n\n\n\n\nCode\n# Alternative UI implementation using Shiny\nfrom shiny import App, ui, render, reactive, req\n\napp_ui = ui.page_fluid(\n    ui.h2(\"🥃 AI Whisky Tasting System\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_select(\"agent\", \"Select Agent:\", [\n                \"DrunkTony (中文)\", \"WhiskyFunny (English)\", \"WhiskyNotebook (English)\"\n            ]),\n            ui.input_text(\"whisky_name\", \"Whisky Name:\", \"\"),\n            ui.input_action_button(\"generate\", \"Generate Notes\")\n        ),\n        ui.output_text_verbatim(\"tasting_notes\"),\n        ui.output_text_verbatim(\"recommendations\")\n    )\n)\n\ndef server(input, output, session):\n    @output\n    @render.text\n    def tasting_notes():\n        req(input.generate())\n        # Generate tasting notes logic\n        return generate_tasting_notes(input.whisky_name(), input.agent())\n\napp = App(app_ui, server)"
  },
  {
    "objectID": "posts/whisky-tasting/index.html#technical-achievements",
    "href": "posts/whisky-tasting/index.html#technical-achievements",
    "title": "Whisky Tasting note with RAG",
    "section": "",
    "text": "Multi-Agent Architecture: Different AI personalities and data sources\nRAG Implementation: Retrieval-augmented generation with real whisky reviews\nVector Database: Efficient similarity search with 376MB database\nBilingual Support: Chinese and English language agents\nCost Management: Built-in rate limiting and API optimization\nProfessional Tasting Formats: Industry-standard note structures\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nDatabase Size\n376MB (26K+ reviews)\n\n\nEmbedding Dimension\n1024\n\n\nSearch Latency\n&lt;2 seconds\n\n\nDaily Request Limit\n80 requests\n\n\nSupport Languages\n2 (EN/ZH)\n\n\nData Sources\n3 (drinktony, whiskyfun, whiskynotes)"
  },
  {
    "objectID": "posts/whisky-tasting/index.html#future-enhancements",
    "href": "posts/whisky-tasting/index.html#future-enhancements",
    "title": "Whisky Tasting note with RAG",
    "section": "",
    "text": "Potential improvements for next versions:\n\nAdvanced Similarity Algorithms: Implement hybrid search with text + metadata\nUser Personalization: Learn from user preferences over time\nMobile App: Native iOS/Android applications\nReal-time Data Updates: Automated web scraping pipeline\nTasting Profile Analysis: Generate user taste profiles from preferences\nSocial Features: Share tasting notes and recommendations with community"
  },
  {
    "objectID": "posts/whisky-tasting/index.html#conclusion",
    "href": "posts/whisky-tasting/index.html#conclusion",
    "title": "Whisky Tasting note with RAG",
    "section": "",
    "text": "This whisky tasting application demonstrates the power of combining RAG technology with specialized AI agents to create a sophisticated domain-specific system. The project showcases:\n\nAdvanced AI Architecture: Multi-agent system with different personalities\nData Engineering: Large-scale vector database management\nProfessional Knowledge: Industry-standard tasting note formats\nUser Experience: Clean, intuitive interfaces across platforms\nCost Efficiency: Smart rate limiting and optimization\n\nWhether you’re a whisky enthusiast, AI developer, or data scientist, this project provides an excellent example of building production-grade AI applications that combine real-world data with advanced machine learning techniques.\n\nTechnology Stack: Python, Streamlit, DuckDB, OpenAI API, Vector Embeddings\nDatabase: 26K+ real whisky reviews across 3 specialized sources"
  },
  {
    "objectID": "posts/AI-Chat/index.html",
    "href": "posts/AI-Chat/index.html",
    "title": "AI Chat: A Multilingual Multi-Model application",
    "section": "",
    "text": "Introduction\nIn the rapidly evolving landscape of Artificial Intelligence, having a unified interface to interact with multiple models is invaluable. AI Chat is a multilingual, multi-model AI chat application built with Streamlit. It supports streaming responses, image inputs, concurrent model querying, web search, and comprehensive file processing.\nWhether you’re comparing responses from different LLMs or generating stunning visuals, AI Chat provides a seamless and powerful experience.\n\nText answerCreate imagesummary upload documentpre define prompt\n\n\n\n\n\n\n\n\n\nAI summary paper:DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning (https://arxiv.org/pdf/2501.12948)\n\n\n\n\n\n\n\n\n✨ Key Features\n\n🌐 Multilingual Support: Instant switching between English and Chinese interfaces.\n🤖 Multi-Model Chat: Query multiple AI models simultaneously with a side-by-side comparison view.\n🎨 Image Generation & Editing: Create and edit images using models like FLUX, Qwen Image, and Gemini.\n📚 Prompt Bay: Access a library of 100+ searchable system prompts for specialized interactions.\n🔍 Web Search: Integrated Tavily AI search for real-time information retrieval.\n🖼️ Comprehensive File Support: Upload and process PDFs, Word docs, Excel, CSV, and images.\n⚡ Streaming Responses: Real-time feedback from multiple models concurrently.\n\n\n\n🚀 Quick Start\nTo get started with AI Chat locally, follow these steps:\n\nPrerequisites\n\nPython 3.7 or higher\npip package manager\n\n\n\nInstallation\n\nClone the repository:\ngit clone https://github.com/JCwinning/AI_Chat.git\ncd AI_Chat\nInstall dependencies:\npip install -r requirements.txt\nConfigure API keys: Create a .env file in the project root and add your keys:\nmodelscope=your-modelscope-api-key\nopenrouter=your-openrouter-api-key\ndashscope=your-dashscope-api-key\nbigmodel=your-bigmodel-api-key\ntavily_api_key=your-tavily-api-key\nRun the application:\nstreamlit run app.py\n\n\n\n\n🏗️ Architecture & Components\nThe project is structured for modularity and performance:\n\napp.py: The main entry point using Streamlit for the UI and session management.\nconfig.py: Centralized model definitions and provider settings.\nsearch_providers.py: Handles web search integration with caching.\nMulti-threading: Uses Python’s threading and Queue to handle parallel model requests and streaming.\nFile Processing: Leverages markitdown for document conversion and Pillow for image handling.\n\n\n\n📖 Usage Tips\n\nSide-by-Side Comparison\nOne of the most powerful features is the ability to select multiple models in the “🤖 Models” tab. When you send a message, the app queries all selected models in parallel and displays their responses side-by-side, making it easy to compare performance and accuracy.\n\n\nUsing the Prompt Bay\nDon’t know how to start? Use the “📚 Prompt Bay” to find the perfect system prompt. You can search by category or keyword and apply it to your current session with a single click.\n\n\nAdvanced File Analysis\nUpload a PDF or Excel file, and the app will automatically convert it to markdown or a table, including it in the conversation context. This allows you to ask the AI questions directly about your documents.\n\nCheck out the full source code on GitHub and start building your own AI-powered workflows!"
  },
  {
    "objectID": "posts/llm-summary/index.html",
    "href": "posts/llm-summary/index.html",
    "title": "LLM Summary System: A Multi-Platform AI Summarization Tool",
    "section": "",
    "text": "Introduction\nIn the era of information explosion, keeping up with content across multiple platforms like YouTube, Bilibili, Spotify, and Xiaohongshu can be overwhelming. The LLM Summary System is a powerful, unified tool designed to solve this by automating the process of downloading, transcribing, and summarizing content using state-of-the-art Large Language Models (LLMs).\nThis project provides a seamless experience for converting long-form audio/video content into concise, actionable summaries.\n\n\n\nKey Features\nThe system is packed with features that make it a robust choice for content consumption:\n\nMulti-Platform Support: Unified interface for YouTube, Bilibili, Spotify, Xiaohongshu, and Xiaoyuzhou FM.\nIntelligent Processing: Automatic chunking for large files and transcripts to handle long-form content (up to 2M tokens).\nHigh-Quality Transcription: Uses MLX Whisper (optimized for Apple Silicon) with an OpenAI Whisper fallback.\nAdvanced Summarization: Support for 15+ LLM models (including Qwen, GLM, DeepSeek, GPT-4, Gemini, and Grok).\nText-to-Speech (TTS): Convert summaries back to audio using Google Cloud TTS, Qwen, or Gemini.\nSearch Integration: Automatically finds related reading materials using AI-extracted keywords.\nReal-time Progress Tracking: A user-friendly Streamlit web interface with live updates.\n\n\n\nArchitecture Overview\nThe system follows a streamlined pipeline:\ngraph LR\n    A[URL Input] --&gt; B[Platform Detection]\n    B --&gt; C[Content Download]\n    C --&gt; D[Audio Extraction]\n    D --&gt; E[Transcription - Whisper]\n    E --&gt; F[AI Summarization]\n    F --&gt; G[TTS Generation]\n    G --&gt; H[File Output + Web Display]\n\nCore Components\n\napp.py: The main Streamlit web interface handle concurrent URL submission and dynamic queuing.\ndownload.py: A unified downloader with comprehensive retry logic for all supported platforms.\nprocess.py: Handles audio transcription and AI summarization with intelligent chunking.\nconfig.py: Manages LLM model configurations and environment variables.\ntts.py: Multi-provider text-to-speech with hash-based caching.\n\n\n\n\nAdvanced Intelligence\nWhat sets this system apart is its handling of complex tasks:\n\nIntelligent Chunking\nTo prevent RAM exhaustion and handle extremely long content: - Audio: Automatically splits files longer than 60 minutes for transcription. - Text: Transcripts exceeding 240K characters are split into 100K character chunks.\n\n\nSearch Integration\nAfter generating a summary, the system extracts key terms and searches for supplemental materials via DuckDuckGo (or Baidu as a fallback). This ensures you get a well-rounded understanding of the topic.\n\n\n\nGetting Started\nThe easiest way to run the system is via the Web Interface:\n\nClone the repository:\ngit clone https://github.com/JCwinning/llm_summary.git\ncd llm_summary\nConfigure your environment: Create a .env file with your API keys (OpenAI, DashScope, etc.).\nInstall dependencies:\npip install -r requirements.txt\nRun the app:\nstreamlit run app.py\n\n\n\nConclusion\nThe LLM Summary System is more than just a downloader; it’s a personalized AI research assistant. Whether you’re a student, researcher, or just someone who wants to stay informed, this tool can significantly boost your productivity by distilling hours of content into minutes of reading.\nCheck out the full source code and contribute on GitHub."
  }
]