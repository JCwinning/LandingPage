{
  "hash": "50b484d76ffee6ad33fde73e657bf8a5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Retrieval-Augmented Generation(RAG) in R & Python\"\nauthor: \"Tony D\"\ndate: \"2025-11-02\"\ncategories: [AI, API, tutorial]\nimage: \"images.png\"\n\nformat:\n  html:\n    code-fold: true\n    code-tools: true\n    code-copy: true\n\nexecute:\n\n  warning: false\n---\n\n\n\n\n\n\n# Introduction\n\nRetrieval-Augmented Generation (RAG) is a powerful technique that combines the generative capabilities of Large Language Models (LLMs) with the precision of information retrieval. By grounding LLM responses in external, verifiable data, RAG reduces hallucinations and enables the model to answer questions about specific, private, or up-to-date information.\n\nIn this tutorial, we will build a RAG system using both R and Python. \n\nIn R, we'll leverage the `ragnar` package for RAG workflows and `ellmer` for chat interfaces. \n\nIn Python, we'll use `LangChain` for the RAG pipeline, `ChromaDB` for the vector store, and `OpenAI` for model interaction.\n\nOur goal is to create a system that can answer questions about the OpenRouter API by scraping its documentation.\n\n# Data Collection\n\n::: {.panel-tabset}\n\n\n## R\n\nFirst, we need to gather the data for our knowledge base. We'll use the `rvest` package to scrape URLs from the OpenRouter documentation. This will give us a list of pages to ingest.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ragnar)\nlibrary(ellmer)\nlibrary(dotenv)\nload_dot_env(file = \".env\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rvest)\n\n# URL to scrape\nurl <- \"https://openrouter.ai/docs/quickstart\"\n\n# Read the HTML content of the page\npage <- read_html(url)\n\n# Extract all <a> tags with href\nlinks <- page %>%\n    html_nodes(\"a\") %>%\n    html_attr(\"href\")\n\n# Remove NAs and duplicates\nlinks <- unique(na.omit(links))\n\n# Optional: keep only full URLs\nlinks_full <- paste0(\"https://openrouter.ai\", links[grepl(\"^/docs/\", links)])\n\n# Print all links\nprint(links_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"https://openrouter.ai/docs/api-reference/overview\"                               \n [2] \"https://openrouter.ai/docs/quickstart\"                                           \n [3] \"https://openrouter.ai/docs/api/reference/overview\"                               \n [4] \"https://openrouter.ai/docs/sdks/call-model/overview\"                             \n [5] \"https://openrouter.ai/docs/guides/overview/principles\"                           \n [6] \"https://openrouter.ai/docs/guides/overview/models\"                               \n [7] \"https://openrouter.ai/docs/faq\"                                                  \n [8] \"https://openrouter.ai/docs/guides/overview/report-feedback\"                      \n [9] \"https://openrouter.ai/docs/guides/routing/model-fallbacks\"                       \n[10] \"https://openrouter.ai/docs/guides/routing/provider-selection\"                    \n[11] \"https://openrouter.ai/docs/guides/features/presets\"                              \n[12] \"https://openrouter.ai/docs/guides/features/tool-calling\"                         \n[13] \"https://openrouter.ai/docs/guides/features/structured-outputs\"                   \n[14] \"https://openrouter.ai/docs/guides/features/message-transforms\"                   \n[15] \"https://openrouter.ai/docs/guides/features/zero-completion-insurance\"            \n[16] \"https://openrouter.ai/docs/guides/features/zdr\"                                  \n[17] \"https://openrouter.ai/docs/app-attribution\"                                      \n[18] \"https://openrouter.ai/docs/faq#how-are-rate-limits-calculated\"                   \n[19] \"https://openrouter.ai/docs/api/reference/streaming\"                              \n[20] \"https://openrouter.ai/docs/guides/community/frameworks-and-integrations-overview\"\n```\n\n\n:::\n:::\n\n\n\n## Python \n\nFirst, we need to gather the data for our knowledge base. We'll use `requests` and `BeautifulSoup` to scrape URLs from the OpenRouter documentation. This will give us a list of pages to ingest.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport sys\nprint(sys.executable)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n/Library/Frameworks/Python.framework/Versions/3.13/bin/python3\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport requests\nfrom bs4 import BeautifulSoup\nfrom dotenv import load_dotenv\nimport os\nfrom markitdown import MarkItDown\nfrom io import BytesIO\nimport re\n\n# Load environment variables\nload_dotenv()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue\n```\n\n\n:::\n\n```{.python .cell-code}\n# Helper functions\ndef fetch_html(url: str) -> bytes:\n    \"\"\"Fetch HTML content from URL and return as bytes.\"\"\"\n    resp = requests.get(url)\n    resp.raise_for_status()\n    return resp.content\n\ndef html_to_markdown(html_bytes: bytes) -> str:\n    \"\"\"Convert HTML bytes to markdown using MarkItDown.\"\"\"\n    md = MarkItDown()\n    stream = BytesIO(html_bytes)\n    result = md.convert_stream(stream, mime_type=\"text/html\")\n    return result.markdown\n\ndef save_markdown(md_content: str, output_path: str):\n    \"\"\"Save markdown content to file.\"\"\"\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(md_content)\n\ndef sanitize_filename(filename: str) -> str:\n    \"\"\"Sanitize URL to create a valid filename.\"\"\"\n    filename = re.sub(r'^https?://[^/]+', '', filename)\n    filename = re.sub(r'[^\\w\\-_.]', '_', filename)\n    filename = filename.strip('_')\n    if not filename.endswith('.md'):\n        filename += '.md'\n    return filename\n\n# URL to scrape\nurl = \"https://openrouter.ai/docs/quickstart\"\n\n# Read the HTML content of the page\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.content, 'html.parser')\n\n# Extract all <a> tags with href\nlinks = [a['href'] for a in soup.find_all('a', href=True)]\n\n# Remove duplicates\nlinks = list(set(links))\n\n# Keep only full URLs for docs\nlinks_full = [f\"https://openrouter.ai{link}\" for link in links if link.startswith(\"/docs/\")]\n\n# Explicitly add FAQ\nlinks_full.append(\"https://openrouter.ai/docs/faq\")\nlinks_full = list(set(links_full))\n\n# Print all links\nprint(f\"Found {len(links_full)} documentation URLs\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFound 20 documentation URLs\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(links_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['https://openrouter.ai/docs/guides/overview/report-feedback', 'https://openrouter.ai/docs/guides/features/zdr', 'https://openrouter.ai/docs/faq#how-are-rate-limits-calculated', 'https://openrouter.ai/docs/sdks/call-model/overview', 'https://openrouter.ai/docs/guides/features/presets', 'https://openrouter.ai/docs/guides/routing/provider-selection', 'https://openrouter.ai/docs/guides/overview/principles', 'https://openrouter.ai/docs/guides/overview/models', 'https://openrouter.ai/docs/guides/routing/model-fallbacks', 'https://openrouter.ai/docs/faq', 'https://openrouter.ai/docs/api-reference/overview', 'https://openrouter.ai/docs/guides/community/frameworks-and-integrations-overview', 'https://openrouter.ai/docs/app-attribution', 'https://openrouter.ai/docs/guides/features/message-transforms', 'https://openrouter.ai/docs/guides/features/tool-calling', 'https://openrouter.ai/docs/quickstart', 'https://openrouter.ai/docs/guides/features/structured-outputs', 'https://openrouter.ai/docs/guides/features/zero-completion-insurance', 'https://openrouter.ai/docs/api/reference/overview', 'https://openrouter.ai/docs/api/reference/streaming']\n```\n\n\n:::\n:::\n\n\n\n:::\n\n# Save web to local data\n\n::: {.panel-tabset}\n\n## R\n\nTo perform semantic search, we need to store our text data as vectors (embeddings). We'll use `DuckDB` as our local vector store. We also need an embedding model to convert text into vectors. Here, we configure `ragnar` to use a specific embedding model via an OpenAI-compatible API (SiliconFlow).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# pages <- ragnar_find_links(base_url)\npages <- links_full\nstore_location <- \"openrouter.duckdb\"\n\nstore <- ragnar_store_create(\n    store_location,\n    overwrite = TRUE,\n    embed = \\(x) ragnar::embed_openai(x,\n        model = \"BAAI/bge-m3\",\n        base_url = \"https://api.siliconflow.cn/v1\",\n        api_key = Sys.getenv(\"siliconflow\")\n    )\n)\n```\n:::\n\n\n\nWith our store initialized, we can now ingest the data. We iterate through the list of pages we scraped earlier. For each page, we:\n1.  Read the content as markdown.\n2.  Split the content into smaller chunks (approx. 600 characters).\n3.  Insert these chunks into our vector store.\n\nThis process builds the index that we'll search against.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# page=\"https://openrouter.ai/docs/faq\"\n# chunks <- page |>read_as_markdown() |>markdown_chunk(target_size = 2000)\n# ragnar_chunks_view(chunks)\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (page in pages) {\n    message(\"ingesting: \", page)\n    print(page)\n    chunks <- page |>\n        read_as_markdown() |>\n        markdown_chunk(target_size = 2000)\n    # print(chunks)\n    # print('chunks done')\n    ragnar_store_insert(store, chunks)\n    print(\"insrt done\")\n}\n```\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nragnar_store_build_index(store)\n```\n:::\n\n\n\n\n## Python DuckDB\n\n::: {.cell}\n\n```{.python .cell-code}\nimport os\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, Settings\nfrom llama_index.vector_stores.duckdb import DuckDBVectorStore\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\n# --- 1. Configuration ---\n\n# Ensure your API key is available\nopenrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\") # or paste string directly\n\n# Initialize the embedding model pointing to OpenRouter\n# We use the OpenAI class because OpenRouter uses an OpenAI-compatible API structure\nembed_model = OpenAIEmbedding(\n    api_key=openrouter_api_key,\n    base_url=\"https://openrouter.ai/api/v1\",\n    model=\"qwen/qwen3-embedding-8b\"  \n)\n\n# Update the global settings so LlamaIndex knows to use this model\nSettings.embed_model = embed_model\nSettings.chunk_size = 2000\nSettings.chunk_overlap = 200\n# --- 2. Ingestion and Indexing ---\n\n# Load data\ndocuments = SimpleDirectoryReader(\"markdown_docs\").load_data()\n\n# Initialize DuckDB Vector Store\nvector_store = DuckDBVectorStore(\"openrouter.duckdb\", persist_dir=\"./persist/\")\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n# Create the index\n# This will now automatically use the Qwen embeddings defined in Settings\nindex = VectorStoreIndex.from_documents(\n    documents, \n    storage_context=storage_context\n)\n```\n:::\n\n\n\n\n\n\n## Python Chroma\n\nTo perform semantic search, we need to store our text data as vectors (embeddings). We'll use `ChromaDB` as our local vector store. We also need an embedding model to convert text into vectors. Here, we configure a custom `OpenRouterEmbeddings` class to use the `qwen/qwen3-embedding-8b` model via the OpenRouter API.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom openai import OpenAI\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_chroma import Chroma\nfrom typing import List\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Custom embeddings class for OpenRouter API\nclass OpenRouterEmbeddings(Embeddings):\n    \"\"\"Custom embeddings class for OpenRouter API.\"\"\"\n    \n    def __init__(self, api_key: str, model: str = \"text-embedding-3-small\"):\n        self.client = OpenAI(\n            base_url=\"https://openrouter.ai/api/v1\",\n            api_key=api_key,\n        )\n        self.model = model\n    \n    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"Embed a list of documents.\"\"\"\n        response = self.client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self.model,\n            input=texts,\n            encoding_format=\"float\"\n        )\n        return [item.embedding for item in response.data]\n    \n    def embed_query(self, text: str) -> List[float]:\n        \"\"\"Embed a single query.\"\"\"\n        response = self.client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self.model,\n            input=text,\n            encoding_format=\"float\"\n        )\n        return response.data[0].embedding\n\n# Get OpenRouter API key\nopenrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\nif not openrouter_api_key:\n    raise ValueError(\"OPENROUTER_API_KEY not found in environment variables\")\n\n# Create embeddings instance using OpenRouter\nembeddings = OpenRouterEmbeddings(\n    api_key=openrouter_api_key,\n    model=\"qwen/qwen3-embedding-8b\"\n)\n\n# Define vector store location\npersist_directory = \"chroma_db_data\"\n```\n:::\n\n\n\n\n\nWith our store initialized, we can now ingest the data. We iterate through the markdown files we saved earlier. For each file, we:\n1.  Load the content.\n2.  Split the content into smaller chunks (approx. 2000 characters) using `RecursiveCharacterTextSplitter`.\n3.  Create a new `Chroma` vector store from these chunks.\n\nThis process builds the index that we'll search against.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom langchain_core.documents import Document\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nimport shutil\n\n# Helper function to load markdown files\ndef load_markdown_files(directory: str) -> list[Document]:\n    \"\"\"Load all markdown files from directory and create Document objects.\"\"\"\n    documents = []\n    if not os.path.exists(directory):\n        return documents\n        \n    for filename in os.listdir(directory):\n        if filename.endswith('.md'):\n            filepath = os.path.join(directory, filename)\n            with open(filepath, 'r', encoding='utf-8') as f:\n                content = f.read()\n                doc = Document(\n                    page_content=content,\n                    metadata={\n                        \"source\": filename,\n                        \"filepath\": filepath\n                    }\n                )\n                documents.append(doc)\n    return documents\n\n# Create output directory for markdown files\noutput_dir = \"markdown_docs\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Convert each URL to markdown and save\nfor i, link_url in enumerate(links_full, 1):\n    try:\n        print(f\"Processing {i}/{len(links_full)}: {link_url}\")\n        html_content = fetch_html(link_url)\n        markdown_content = html_to_markdown(html_content)\n        filename = sanitize_filename(link_url)\n        output_path = os.path.join(output_dir, filename)\n        save_markdown(markdown_content, output_path)\n        print(f\"  ✓ Saved to {output_path}\")\n    except Exception as e:\n        print(f\"  ✗ Error processing {link_url}: {str(e)}\")\n\n# Load markdown documents\ndocuments = load_markdown_files(output_dir)\nprint(f\"\\nLoaded {len(documents)} markdown documents\")\n\n# Split documents into chunks\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=2000,\n    chunk_overlap=200,\n    length_function=len,\n    is_separator_regex=False,\n)\n\nsplits = text_splitter.split_documents(documents)\nprint(f\"Split into {len(splits)} chunks\")\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Remove existing database if it exists\nif os.path.exists(persist_directory):\n    print(f\"Removing existing database at {persist_directory}...\")\n    shutil.rmtree(persist_directory)\n\n# Create new vector store\nvectorstore = Chroma.from_documents(\n    documents=splits,\n    embedding=embeddings,\n    persist_directory=persist_directory\n)\n\nprint(f\"\\n✓ Successfully created ChromaDB with {len(splits)} chunks!\")\nprint(f\"✓ Database saved to: {persist_directory}\")\n```\n:::\n\n\n\n\n:::\n\n\n\n\n\n\n\n\n\n\n# Retrieval\n\n::: {.panel-tabset}\n\n## R\n\nNow that our knowledge base is populated, we can test the retrieval system. We can ask a specific question, like \"What are model variants?\", and query the store to see which text chunks are most relevant. This confirms that our embeddings and search are working correctly.\n\n### Question:What are model variants?\n\nRAG result:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstore_location <- \"openrouter.duckdb\"\nstore <- ragnar_store_connect(store_location)\n\ntext <- \"What are model variants?\"\n\n#' # Retrieving Chunks\n#' Once the store is set up, retrieve the most relevant text chunks like this\n\nrelevant_chunks <- ragnar_retrieve(store, text, top_k = 3)\ncat(\"Retrieved\", nrow(relevant_chunks), \"chunks:\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRetrieved 6 chunks:\n```\n\n\n:::\n\n```{.r .cell-code}\nfor (i in seq_len(nrow(relevant_chunks))) {\n    cat(sprintf(\"--- Chunk %d ---\\n%s\\n\\n\", i, relevant_chunks$text[i]))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n````\n--- Chunk 1 ---\n[Zero Completion Insurance](/docs/features/zero-completion-insurance)\n  + [Provisioning API Keys](/docs/features/provisioning-api-keys)\n  + [App Attribution](/docs/app-attribution)\n* API Reference\n\n  + [Overview](/docs/api-reference/overview)\n  + [Streaming](/docs/api-reference/streaming)\n  + [Embeddings](/docs/api-reference/embeddings)\n  + [Limits](/docs/api-reference/limits)\n  + [Authentication](/docs/api-reference/authentication)\n  + [Parameters](/docs/api-reference/parameters)\n  + [Errors](/docs/api-reference/errors)\n  + Responses API\n  + beta.responses\n  + Analytics\n  + Credits\n  + Embeddings\n  + Generations\n  + Models\n  + Endpoints\n  + Parameters\n  + Providers\n  + API Keys\n  + O Auth\n  + Chat\n  + Completions\n* SDK Reference (BETA)\n\n  + [Python SDK](/docs/sdks/python)\n  + [TypeScript SDK](/docs/sdks/typescript)\n* Use Cases\n\n  + [BYOK](/docs/use-cases/byok)\n  + [Crypto API](/docs/use-cases/crypto-api)\n  + [OAuth PKCE](/docs/use-cases/oauth-pkce)\n  + [MCP Servers](/docs/use-cases/mcp-servers)\n  + [Organization Management](/docs/use-cases/organization-management)\n  + [For Providers](/docs/use-cases/for-providers)\n  + [Reasoning Tokens](/docs/use-cases/reasoning-tokens)\n  + [Usage Accounting](/docs/use-cases/usage-accounting)\n  + [User Tracking](/docs/use-cases/user-tracking)\n* Community\n\n  + [Frameworks and Integrations Overview](/docs/community/frameworks-and-integrations-overview)\n  + [Effect AI SDK](/docs/community/effect-ai-sdk)\n  + [Arize](/docs/community/arize)\n  + [LangChain](/docs/community/lang-chain)\n  + [LiveKit](/docs/community/live-kit)\n  + [Langfuse](/docs/community/langfuse)\n  + [Mastra](/docs/community/mastra)\n  + [OpenAI SDK](/docs/community/open-ai-sdk)\n  + [PydanticAI](/docs/community/pydantic-ai)\n  + [Vercel AI SDK](/docs/community/vercel-ai-sdk)\n  + [Xcode](/docs/community/xcode)\n  + [Zapier](/docs/community/zapier)\n  + [Discord](https://discord.gg/openrouter)\n\nLight\n\nOn this page\n\n* [Requests](#requests)\n* [Completions Request Format](#completions-request-format)\n* [Headers](#headers)\n* [Assistant Prefill](#assistant-prefill)\n* [Responses](#responses)\n* [CompletionsResponse Format](#completionsresponse-format)\n* [Finish Reason](#finish-reason)\n* [Querying Cost and Stats](#querying-cost-and-stats)\n\n[API Reference](/docs/api-reference/overview)\n\n\n\n--- Chunk 2 ---\n###### How frequently are new models added?\n\nWe work on adding models as quickly as we can. We often have partnerships with\nthe labs releasing models and can release models as soon as they are\navailable. If there is a model missing that you’d like OpenRouter to support, feel free to message us on\n[Discord](https://discord.gg/openrouter).\n\n###### What are model variants?\n\nVariants are suffixes that can be added to the model slug to change its behavior.\n\nStatic variants can only be used with specific models and these are listed in our [models api](https://openrouter.ai/api/v1/models).\n\n1. `:free` - The model is always provided for free and has low rate limits.\n2. `:beta` - The model is not moderated by OpenRouter.\n3. `:extended` - The model has longer than usual context length.\n4. `:exacto` - The model only uses OpenRouter-curated high-quality endpoints.\n5. `:thinking` - The model supports reasoning by default.\n\nDynamic variants can be used on all models and they change the behavior of how the request is routed or used.\n\n1. `:online` - All requests will run a query to extract web results that are attached to the prompt.\n2. `:nitro` - Providers will be sorted by throughput rather than the default sort, optimizing for faster response times.\n3. `:floor` - Providers will be sorted by price rather than the default sort, prioritizing the most cost-effective options.\n\n###### I am an inference provider, how can I get listed on OpenRouter?\n\nYou can read our requirements at the [Providers\npage](/docs/use-cases/for-providers). If you would like to contact us, the best\nplace to reach us is over email.\n\n###### What is the expected latency/response time for different models?\n\nFor each model on OpenRouter we show the latency (time to first token) and the token\nthroughput for all providers. You can use this to estimate how long requests\nwill take. If you would like to optimize for throughput you can use the\n`:nitro` variant to route to the fastest provider.\n\n\n\n--- Chunk 3 ---\n###### How frequently are new models added?\n\nWe work on adding models as quickly as we can. We often have partnerships with\nthe labs releasing models and can release models as soon as they are\navailable. If there is a model missing that you’d like OpenRouter to support, feel free to message us on\n[Discord](https://discord.gg/openrouter).\n\n###### What are model variants?\n\nVariants are suffixes that can be added to the model slug to change its behavior.\n\nStatic variants can only be used with specific models and these are listed in our [models api](https://openrouter.ai/api/v1/models).\n\n1. `:free` - The model is always provided for free and has low rate limits.\n2. `:beta` - The model is not moderated by OpenRouter.\n3. `:extended` - The model has longer than usual context length.\n4. `:exacto` - The model only uses OpenRouter-curated high-quality endpoints.\n5. `:thinking` - The model supports reasoning by default.\n\nDynamic variants can be used on all models and they change the behavior of how the request is routed or used.\n\n1. `:online` - All requests will run a query to extract web results that are attached to the prompt.\n2. `:nitro` - Providers will be sorted by throughput rather than the default sort, optimizing for faster response times.\n3. `:floor` - Providers will be sorted by price rather than the default sort, prioritizing the most cost-effective options.\n\n###### I am an inference provider, how can I get listed on OpenRouter?\n\nYou can read our requirements at the [Providers\npage](/docs/use-cases/for-providers). If you would like to contact us, the best\nplace to reach us is over email.\n\n###### What is the expected latency/response time for different models?\n\nFor each model on OpenRouter we show the latency (time to first token) and the token\nthroughput for all providers. You can use this to estimate how long requests\nwill take. If you would like to optimize for throughput you can use the\n`:nitro` variant to route to the fastest provider.\n\n\n\n--- Chunk 4 ---\n## The `models` parameter\n\nThe `models` parameter lets you automatically try other models if the primary model’s providers are down, rate-limited, or refuse to reply due to content moderation.\n\nTypeScript SDKTypeScript (fetch)Python\n\n```code-block-root not-prose rounded-b-[inherit] rounded-t-none\n|  |  |\n| --- | --- |\n| 1 | import { OpenRouter } from '@openrouter/sdk'; |\n| 2 |  |\n| 3 | const openRouter = new OpenRouter({ |\n| 4 | apiKey: '<OPENROUTER_API_KEY>', |\n| 5 | }); |\n| 6 |  |\n| 7 | const completion = await openRouter.chat.send({ |\n| 8 | models: ['anthropic/claude-3.5-sonnet', 'gryphe/mythomax-l2-13b'], |\n| 9 | messages: [ |\n| 10 | { |\n| 11 | role: 'user', |\n| 12 | content: 'What is the meaning of life?', |\n| 13 | }, |\n| 14 | ], |\n| 15 | }); |\n| 16 |  |\n| 17 | console.log(completion.choices[0].message.content); |\n```\n\nIf the model you selected returns an error, OpenRouter will try to use the fallback model instead. If the fallback model is down or returns an error, OpenRouter will return that error.\n\nBy default, any error can trigger the use of a fallback model, including context length validation errors, moderation flags for filtered models, rate-limiting, and downtime.\n\nRequests are priced using the model that was ultimately used, which will be returned in the `model` attribute of the response body.\n\n## Using with OpenAI SDK\n\nTo use the `models` array with the OpenAI SDK, include it in the `extra_body` parameter. In the example below, gpt-4o will be tried first, and the `models` array will be tried in order as fallbacks.\n\nPythonTypeScript\n\n\n\n--- Chunk 5 ---\n[Web Search](/docs/features/web-search)\n  + [Zero Completion Insurance](/docs/features/zero-completion-insurance)\n  + [Provisioning API Keys](/docs/features/provisioning-api-keys)\n  + [App Attribution](/docs/app-attribution)\n* API Reference\n\n  + [Overview](/docs/api-reference/overview)\n  + [Streaming](/docs/api-reference/streaming)\n  + [Embeddings](/docs/api-reference/embeddings)\n  + [Limits](/docs/api-reference/limits)\n  + [Authentication](/docs/api-reference/authentication)\n  + [Parameters](/docs/api-reference/parameters)\n  + [Errors](/docs/api-reference/errors)\n  + Responses API\n  + beta.responses\n  + Analytics\n  + Credits\n  + Embeddings\n  + Generations\n  + Models\n  + Endpoints\n  + Parameters\n  + Providers\n  + API Keys\n  + O Auth\n  + Chat\n  + Completions\n* SDK Reference (BETA)\n\n  + [Python SDK](/docs/sdks/python)\n  + [TypeScript SDK](/docs/sdks/typescript)\n* Use Cases\n\n  + [BYOK](/docs/use-cases/byok)\n  + [Crypto API](/docs/use-cases/crypto-api)\n  + [OAuth PKCE](/docs/use-cases/oauth-pkce)\n  + [MCP Servers](/docs/use-cases/mcp-servers)\n  + [Organization Management](/docs/use-cases/organization-management)\n  + [For Providers](/docs/use-cases/for-providers)\n  + [Reasoning Tokens](/docs/use-cases/reasoning-tokens)\n  + [Usage Accounting](/docs/use-cases/usage-accounting)\n  + [User Tracking](/docs/use-cases/user-tracking)\n* Community\n\n  + [Frameworks and Integrations Overview](/docs/community/frameworks-and-integrations-overview)\n  + [Effect AI SDK](/docs/community/effect-ai-sdk)\n  + [Arize](/docs/community/arize)\n  + [LangChain](/docs/community/lang-chain)\n  + [LiveKit](/docs/community/live-kit)\n  + [Langfuse](/docs/community/langfuse)\n  + [Mastra](/docs/community/mastra)\n  + [OpenAI SDK](/docs/community/open-ai-sdk)\n  + [PydanticAI](/docs/community/pydantic-ai)\n  + [Vercel AI SDK](/docs/community/vercel-ai-sdk)\n  + [Xcode](/docs/community/xcode)\n  + [Zapier](/docs/community/zapier)\n  + [Discord](https://discord.gg/openrouter)\n\nLight\n\nOn this page\n\n* [Within OpenRouter](#within-openrouter)\n* [Provider Policies](#provider-policies)\n* [Training on Prompts](#training-on-prompts)\n* [Data Retention & Logging](#data-retention--logging)\n* [Enterprise EU in-region routing](#enterprise-eu-in-region-routing)\n\n[Features](/docs/features/privacy-and-logging)\n\n\n\n--- Chunk 6 ---\n[Web Search](/docs/features/web-search)\n  + [Zero Completion Insurance](/docs/features/zero-completion-insurance)\n  + [Provisioning API Keys](/docs/features/provisioning-api-keys)\n  + [App Attribution](/docs/app-attribution)\n* API Reference\n\n  + [Overview](/docs/api-reference/overview)\n  + [Streaming](/docs/api-reference/streaming)\n  + [Embeddings](/docs/api-reference/embeddings)\n  + [Limits](/docs/api-reference/limits)\n  + [Authentication](/docs/api-reference/authentication)\n  + [Parameters](/docs/api-reference/parameters)\n  + [Errors](/docs/api-reference/errors)\n  + Responses API\n  + beta.responses\n  + Analytics\n  + Credits\n  + Embeddings\n  + Generations\n  + Models\n  + Endpoints\n  + Parameters\n  + Providers\n  + API Keys\n  + O Auth\n  + Chat\n  + Completions\n* SDK Reference (BETA)\n\n  + [Python SDK](/docs/sdks/python)\n  + [TypeScript SDK](/docs/sdks/typescript)\n* Use Cases\n\n  + [BYOK](/docs/use-cases/byok)\n  + [Crypto API](/docs/use-cases/crypto-api)\n  + [OAuth PKCE](/docs/use-cases/oauth-pkce)\n  + [MCP Servers](/docs/use-cases/mcp-servers)\n  + [Organization Management](/docs/use-cases/organization-management)\n  + [For Providers](/docs/use-cases/for-providers)\n  + [Reasoning Tokens](/docs/use-cases/reasoning-tokens)\n  + [Usage Accounting](/docs/use-cases/usage-accounting)\n  + [User Tracking](/docs/use-cases/user-tracking)\n* Community\n\n  + [Frameworks and Integrations Overview](/docs/community/frameworks-and-integrations-overview)\n  + [Effect AI SDK](/docs/community/effect-ai-sdk)\n  + [Arize](/docs/community/arize)\n  + [LangChain](/docs/community/lang-chain)\n  + [LiveKit](/docs/community/live-kit)\n  + [Langfuse](/docs/community/langfuse)\n  + [Mastra](/docs/community/mastra)\n  + [OpenAI SDK](/docs/community/open-ai-sdk)\n  + [PydanticAI](/docs/community/pydantic-ai)\n  + [Vercel AI SDK](/docs/community/vercel-ai-sdk)\n  + [Xcode](/docs/community/xcode)\n  + [Zapier](/docs/community/zapier)\n  + [Discord](https://discord.gg/openrouter)\n\nLight\n\nOn this page\n\n* [How OpenRouter Manages Data Policies](#how-openrouter-manages-data-policies)\n* [Per-Request ZDR Enforcement](#per-request-zdr-enforcement)\n* [Usage](#usage)\n* [Caching](#caching)\n* [OpenRouter’s Retention Policy](#openrouters-retention-policy)\n* [Zero Retention Endpoints](#zero-retention-endpoints)\n\n[Features](/docs/features/privacy-and-logging)\n````\n\n\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ragnar_store_inspect(store)\n#ragnar_chunks_view(chunks)\n```\n:::\n\n\n## Python DuckDB\n\nIn Python, we can use `LlamaIndex` to interact with our DuckDB vector store. In this step, we'll configure the embedding model and retrieve the top relevant chunks for our query, saving them to a file for inspection. We won't use an LLM for generation yet, focusing solely on verifying the retrieval quality.\n\n### Question:What are model variants?\n\nRAG result:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport os\nimport sys\nprint(f\"Python executable: {sys.executable}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPython executable: /Library/Frameworks/Python.framework/Versions/3.13/bin/python3\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(f\"Python path: {sys.path}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPython path: ['', '/Library/Frameworks/Python.framework/Versions/3.13/bin', '/Library/Frameworks/Python.framework/Versions/3.13/lib/python313.zip', '/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13', '/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/lib-dynload', '/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages', '/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/reticulate/python']\n```\n\n\n:::\n\n```{.python .cell-code}\nfrom typing import Any, List\nfrom openai import OpenAI\nfrom llama_index.core import VectorStoreIndex, Settings\nfrom llama_index.core.embeddings import BaseEmbedding\nfrom llama_index.vector_stores.duckdb import DuckDBVectorStore\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue\n```\n\n\n:::\n\n```{.python .cell-code}\n# Ensure your API key is available\nopenrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n\n# Custom OpenRouter Embedding class for LlamaIndex\nclass OpenRouterEmbedding(BaseEmbedding):\n    \"\"\"Custom embedding class for OpenRouter API compatible with LlamaIndex.\"\"\"\n    \n    def __init__(\n        self,\n        api_key: str,\n        model: str = \"qwen/qwen3-embedding-8b\",\n        **kwargs: Any\n    ):\n        super().__init__(**kwargs)\n        self._client = OpenAI(\n            base_url=\"https://openrouter.ai/api/v1\",\n            api_key=api_key,\n        )\n        self._model = model\n    \n    def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get embedding for a query string.\"\"\"\n        response = self._client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self._model,\n            input=query,\n            encoding_format=\"float\"\n        )\n        return response.data[0].embedding\n    \n    def _get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get embedding for a text string.\"\"\"\n        return self._get_query_embedding(text)\n    \n    async def _aget_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Async version of get_query_embedding.\"\"\"\n        return self._get_query_embedding(query)\n    \n    async def _aget_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Async version of get_text_embedding.\"\"\"\n        return self._get_text_embedding(text)\n\n# 1. Configure Embedding Model using custom OpenRouter class\nembed_model = OpenRouterEmbedding(\n    api_key=openrouter_api_key,\n    model=\"qwen/qwen3-embedding-8b\"\n)\n\n# 2. Apply Settings\nSettings.embed_model = embed_model\n\n# 3. Load and Retrieve\n# Load the existing DuckDB vector store\nprint(\"Loading vector store from openrouter.duckdb...\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLoading vector store from openrouter.duckdb...\n```\n\n\n:::\n\n```{.python .cell-code}\nvector_store = DuckDBVectorStore(database_name=\"openrouter.duckdb\",persist_dir=\"./persist/\")\n\nindex = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n\n# Define query\nquery = \"What are model variants?\"\nprint(f\"\\n{'='*60}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n============================================================\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(f\"Query: '{query}'\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nQuery: 'What are model variants?'\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(f\"{'='*60}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n============================================================\n```\n\n\n:::\n\n```{.python .cell-code}\n# Retrieve top 3 chunks\nretriever = index.as_retriever(similarity_top_k=5)\nnodes = retriever.retrieve(query)\n\n# Print detailed retrieval info\nprint(f\"Retrieved {len(nodes)} chunks from DuckDB:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRetrieved 5 chunks from DuckDB:\n```\n\n\n:::\n\n```{.python .cell-code}\nfor i, node in enumerate(nodes, 1):\n    print(f\"{'─'*60}\")\n    print(f\"Chunk {i}\")\n    print(f\"{'─'*60}\")\n\n    # Print similarity score\n    if hasattr(node, 'score'):\n        print(f\"Similarity Score: {node.score:.4f}\")\n\n    # Print metadata\n    if hasattr(node, 'metadata') and node.metadata:\n        print(f\"Metadata:\")\n        for key, value in node.metadata.items():\n            print(f\"  - {key}: {value}\")\n\n    # Print text content (truncated for display)\n    text_preview = node.text[:500] + \"...\" if len(node.text) > 500 else node.text\n    print(f\"\\nContent:\\n{text_preview}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n────────────────────────────────────────────────────────────\nChunk 1\n────────────────────────────────────────────────────────────\nSimilarity Score: 0.6169\nMetadata:\n  - file_path: /Users/jinchaoduan/Documents/post_project/AI_Blog/posts/RAG/markdown_docs/docs_features_exacto-variant.md\n  - file_name: docs_features_exacto-variant.md\n  - file_type: text/markdown\n  - file_size: 7972\n  - creation_date: 2025-11-21\n  - last_modified_date: 2025-11-21\n\nContent:\nSearch\n\n`/`\n\nAsk AI\n\n[API](/docs/api-reference/overview)[Models](https://openrouter.ai/models)[Chat](https://openrouter.ai/chat)[Ranking](https://openrouter.ai/rankings)\n\n* Overview\n\n  + [Quickstart](/docs/quickstart)\n  + [FAQ](/docs/faq)\n  + [Principles](/docs/overview/principles)\n  + [Models](/docs/overview/models)\n  + [Enterprise](https://openrouter.ai/enterprise)\n* Features\n\n  + [Privacy and Logging](/docs/features/privacy-and-logging)\n  + [Zero Data Retention (ZDR)](/docs/features/zdr)\n  + ...\n\n────────────────────────────────────────────────────────────\nChunk 2\n────────────────────────────────────────────────────────────\nSimilarity Score: 0.6099\nMetadata:\n  - file_path: /Users/jinchaoduan/Documents/post_project/AI_Blog/posts/RAG/markdown_docs/docs_overview_models.md\n  - file_name: docs_overview_models.md\n  - file_type: text/markdown\n  - file_size: 9021\n  - creation_date: 2025-11-21\n  - last_modified_date: 2025-11-21\n\nContent:\nSearch\n\n`/`\n\nAsk AI\n\n[API](/docs/api-reference/overview)[Models](https://openrouter.ai/models)[Chat](https://openrouter.ai/chat)[Ranking](https://openrouter.ai/rankings)\n\n* Overview\n\n  + [Quickstart](/docs/quickstart)\n  + [FAQ](/docs/faq)\n  + [Principles](/docs/overview/principles)\n  + [Models](/docs/overview/models)\n  + [Enterprise](https://openrouter.ai/enterprise)\n* Features\n\n  + [Privacy and Logging](/docs/features/privacy-and-logging)\n  + [Zero Data Retention (ZDR)](/docs/features/zdr)\n  + ...\n\n────────────────────────────────────────────────────────────\nChunk 3\n────────────────────────────────────────────────────────────\nSimilarity Score: 0.5820\nMetadata:\n  - file_path: /Users/jinchaoduan/Documents/post_project/AI_Blog/posts/RAG/markdown_docs/docs_guides_overview_models.md\n  - file_name: docs_guides_overview_models.md\n  - file_type: text/markdown\n  - file_size: 7557\n  - creation_date: 2026-01-06\n  - last_modified_date: 2026-01-06\n\nContent:\nSearch\n\n`/`\n\nAsk AI\n\n[Models](https://openrouter.ai/models)[Chat](https://openrouter.ai/chat)[Ranking](https://openrouter.ai/rankings)[Docs](/docs/api-reference/overview)\n\n[Docs](/docs/quickstart)[API Reference](/docs/api/reference/overview)[SDK Reference](/docs/sdks/call-model/overview)\n\n[Docs](/docs/quickstart)[API Reference](/docs/api/reference/overview)[SDK Reference](/docs/sdks/call-model/overview)\n\n* Overview\n\n  + [Quickstart](/docs/quickstart)\n  + [Principles](/docs/guides/overview/princi...\n\n────────────────────────────────────────────────────────────\nChunk 4\n────────────────────────────────────────────────────────────\nSimilarity Score: 0.5763\nMetadata:\n  - file_path: /Users/jinchaoduan/Documents/post_project/AI_Blog/posts/RAG/markdown_docs/docs_faq_how-are-rate-limits-calculated.md\n  - file_name: docs_faq_how-are-rate-limits-calculated.md\n  - file_type: text/markdown\n  - file_size: 17710\n  - creation_date: 2026-01-06\n  - last_modified_date: 2026-01-06\n\nContent:\nSearch\n\n`/`\n\nAsk AI\n\n[Models](https://openrouter.ai/models)[Chat](https://openrouter.ai/chat)[Ranking](https://openrouter.ai/rankings)[Docs](/docs/api-reference/overview)\n\n[Docs](/docs/quickstart)[API Reference](/docs/api/reference/overview)[SDK Reference](/docs/sdks/call-model/overview)\n\n[Docs](/docs/quickstart)[API Reference](/docs/api/reference/overview)[SDK Reference](/docs/sdks/call-model/overview)\n\n* Overview\n\n  + [Quickstart](/docs/quickstart)\n  + [Principles](/docs/guides/overview/princi...\n\n────────────────────────────────────────────────────────────\nChunk 5\n────────────────────────────────────────────────────────────\nSimilarity Score: 0.5701\nMetadata:\n  - file_path: /Users/jinchaoduan/Documents/post_project/AI_Blog/posts/RAG/markdown_docs/docs_features_model-routing.md\n  - file_name: docs_features_model-routing.md\n  - file_type: text/markdown\n  - file_size: 7024\n  - creation_date: 2025-11-21\n  - last_modified_date: 2025-11-21\n\nContent:\nSearch\n\n`/`\n\nAsk AI\n\n[API](/docs/api-reference/overview)[Models](https://openrouter.ai/models)[Chat](https://openrouter.ai/chat)[Ranking](https://openrouter.ai/rankings)\n\n* Overview\n\n  + [Quickstart](/docs/quickstart)\n  + [FAQ](/docs/faq)\n  + [Principles](/docs/overview/principles)\n  + [Models](/docs/overview/models)\n  + [Enterprise](https://openrouter.ai/enterprise)\n* Features\n\n  + [Privacy and Logging](/docs/features/privacy-and-logging)\n  + [Zero Data Retention (ZDR)](/docs/features/zdr)\n  + ...\n```\n\n\n:::\n\n```{.python .cell-code}\n\n# Save retrieved chunks to a markdown file for easy inspection\n# with open(\"retriever.md\", \"w\", encoding=\"utf-8\") as f:\n#     f.write(f\"# Query: {query}\\n\\n\")\n#     f.write(f\"# Retrieved {len(nodes)} chunks from openrouter.duckdb\\n\\n\")\n#     for i, node in enumerate(nodes, 1):\n#         f.write(f\"{'─'*60}\\n\")\n#         f.write(f\"## Chunk {i}\\n\\n\")\n#         if hasattr(node, 'score'):\n#             f.write(f\"**Similarity Score:** {node.score:.4f}\\n\\n\")\n#         if hasattr(node, 'metadata') and node.metadata:\n#             f.write(f\"**Metadata:**\\n\")\n#             for key, value in node.metadata.items():\n#                 f.write(f\"- {key}: {value}\\n\")\n#             f.write(f\"\\n\")\n#         f.write(f\"{node.text}\\n\\n\")\n```\n:::\n\n\n\n\n\n## Python Chroma\n\nNow that our knowledge base is populated, we can test the retrieval system. We can ask a specific question, like \"What are model variants?\", and query the `Chroma` store to see which text chunks are most relevant. This confirms that our embeddings and search are working correctly.\n\n### Question:What are model variants?\n\nRAG result:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom openai import OpenAI\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_chroma import Chroma\nfrom typing import List\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue\n```\n\n\n:::\n\n```{.python .cell-code}\n# Custom embeddings class for OpenRouter API\nclass OpenRouterEmbeddings(Embeddings):\n    \"\"\"Custom embeddings class for OpenRouter API.\"\"\"\n    \n    def __init__(self, api_key: str, model: str = \"qwen/qwen3-embedding-8b\"):\n        self.client = OpenAI(\n            base_url=\"https://openrouter.ai/api/v1\",\n            api_key=api_key,\n        )\n        self.model = model\n    \n    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"Embed a list of documents.\"\"\"\n        response = self.client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self.model,\n            input=texts,\n            encoding_format=\"float\"\n        )\n        return [item.embedding for item in response.data]\n    \n    def embed_query(self, text: str) -> List[float]:\n        \"\"\"Embed a single query.\"\"\"\n        response = self.client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self.model,\n            input=text,\n            encoding_format=\"float\"\n        )\n        return response.data[0].embedding\n\n# Get OpenRouter API key\nopenrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\nif not openrouter_api_key:\n    raise ValueError(\"OPENROUTER_API_KEY not found in environment variables\")\n\n# Create embeddings instance using OpenRouter\nembeddings = OpenRouterEmbeddings(\n    api_key=openrouter_api_key,\n    model=\"qwen/qwen3-embedding-8b\"\n)\n\n# Define vector store location\npersist_directory = \"chroma_db_data\"\n\n# Load existing vector store\nvectorstore = Chroma(\n    persist_directory=persist_directory,\n    embedding_function=embeddings\n)\n\n# Test query\nquery = \"What are model variants?\"\n\n# Perform similarity search\nresults = vectorstore.similarity_search(query, k=5)\n\nprint(f\"\\nQuery: '{query}'\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nQuery: 'What are model variants?'\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(f\"Found {len(results)} relevant chunks:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFound 5 relevant chunks:\n```\n\n\n:::\n\n```{.python .cell-code}\nfor i, doc in enumerate(results, 1):\n    print(f\"Result {i}:\")\n    print(f\"Source: {doc.metadata.get('source', 'unknown')}\")\n    print(f\"Content preview: {doc.page_content[:800]}...\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResult 1:\nSource: docs_faq_how-are-rate-limits-calculated.md\nContent preview: ###### What are model variants?\n\nVariants are suffixes that can be added to the model slug to change its behavior.\n\nStatic variants can only be used with specific models and these are listed in our [models api](https://openrouter.ai/api/v1/models).\n\n1. `:free` - The model is always provided for free and has low rate limits.\n2. `:beta` - The model is not moderated by OpenRouter.\n3. `:extended` - The model has longer than usual context length.\n4. `:exacto` - The model only uses OpenRouter-curated high-quality endpoints.\n5. `:thinking` - The model supports reasoning by default.\n\nDynamic variants can be used on all models and they change the behavior of how the request is routed or used.\n\n1. `:online` - All requests will run a query to extract web results that are attached to the prompt.\n2. `:...\nResult 2:\nSource: docs_faq.md\nContent preview: ###### What are model variants?\n\nVariants are suffixes that can be added to the model slug to change its behavior.\n\nStatic variants can only be used with specific models and these are listed in our [models api](https://openrouter.ai/api/v1/models).\n\n1. `:free` - The model is always provided for free and has low rate limits.\n2. `:beta` - The model is not moderated by OpenRouter.\n3. `:extended` - The model has longer than usual context length.\n4. `:exacto` - The model only uses OpenRouter-curated high-quality endpoints.\n5. `:thinking` - The model supports reasoning by default.\n\nDynamic variants can be used on all models and they change the behavior of how the request is routed or used.\n\n1. `:online` - All requests will run a query to extract web results that are attached to the prompt.\n2. `:...\nResult 3:\nSource: docs_use-cases_crypto-api.md\nContent preview: [API](/docs/api-reference/overview)[Models](https://openrouter.ai/models)[Chat](https://openrouter.ai/chat)[Ranking](https://openrouter.ai/rankings)...\nResult 4:\nSource: docs_sdks_typescript.md\nContent preview: [API](/docs/api-reference/overview)[Models](https://openrouter.ai/models)[Chat](https://openrouter.ai/chat)[Ranking](https://openrouter.ai/rankings)...\nResult 5:\nSource: docs_features_provider-routing.md\nContent preview: Route requests through OpenRouter-curated providers\n\nNext](/docs/features/exacto-variant)[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=openrouter.ai)\n\n[![Logo](https://files.buildwithfern.com/openrouter.docs.buildwithfern.com/docs/2025-11-21T16:36:36.134Z/content/assets/logo.svg)![Logo](https://files.buildwithfern.com/openrouter.docs.buildwithfern.com/docs/2025-11-21T16:36:36.134Z/content/assets/logo-white.svg)](https://openrouter.ai/)\n\n[API](/docs/api-reference/overview)[Models](https://openrouter.ai/models)[Chat](https://openrouter.ai/chat)[Ranking](https://openrouter.ai/rankings)...\n```\n\n\n:::\n:::\n\n\n:::\n\n\n\n# Chat with RAG\n\n::: {.panel-tabset}\n\n## R\n\nThe final piece is to connect this retrieval capability to a chat interface. We use `ellmer` to create a chat client. Crucially, we register a \"retrieval tool\" using `ragnar_register_tool_retrieve`. This gives the LLM the ability to query our vector store whenever it needs information to answer a user's question.\n\nWe also provide a system prompt that instructs the model to always check the knowledge base and cite its sources.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ellmer)\nlibrary(dotenv)\nload_dot_env(file = \".env\")\n\nchat <- chat_openrouter(\n    api_key = Sys.getenv(\"OPENROUTER_API_KEY\"),\n    model = \"openai/gpt-oss-120b\",\n    system_prompt = glue::trim(\"\n  You are an assistant for question-answering tasks. You are concise.\n\n  Before responding, retrieve relevant material from the knowledge store. Quote or\n  paraphrase passages, clearly marking your own words versus the source. Provide a\n  working link for every source cited, as well as any additional relevant links.\n  Do not answer unless you have retrieved and cited a source.If you do not find\n  relevant information, say 'I could not find anything relevant in the knowledge base\n    \")\n) |>\n    ragnar_register_tool_retrieve(store, top_k = 3)\n```\n:::\n\n\n### Question:What are model variants?\n\n\n```{.r .cell-code}\nchat$chat(\"What are model variants?\")\n```\n\n\n**Model variants** are suffixes you can append to a model’s slug on OpenRouter \nto change how the model behaves or how the request is routed.\n\n### Static variants  \nThese work only with certain models (listed in the **Models API**) and affect \nthe model itself:\n\n| Variant | Meaning |\n|---|---|\n| `:free` | The model is always provided for free and has low rate limits. |\n| `:beta` | The model is not moderated by OpenRouter. |\n| `:extended` | The model has a longer‑than‑usual context length. |\n| `:exacto` | The model uses only OpenRouter‑curated high‑quality endpoints. |\n| `:thinking` | The model supports reasoning by default. |\n\n### Dynamic variants  \nThese can be used with **any** model and modify how the request is handled or \nrouted:\n\n| Variant | Meaning |\n|---|---|\n| `:online` | The request runs a web‑search query and attaches the results to \nthe prompt. |\n| `:nitro` | Providers are sorted by throughput, prioritising faster responses.\n|\n| `:floor` | Providers are sorted by price, prioritising the cheapest options. \n|\n\n*Source:* OpenRouter FAQ – “What are model variants?” – lists static and \ndynamic variants with their descriptions【...】(https://openrouter.ai/docs/faq).\n\n\n## Python chatlas\n\nWe can also use the `chatlas` library to create a chat interface. Here, we define a custom tool `retrieve_trusted_content` that queries our DuckDB index. We then register this tool with the chat model, allowing it to pull in relevant information when answering user questions.\n\n### Question:What are model variants?\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport os\nfrom typing import Any, List\nfrom openai import OpenAI\nimport chatlas as ctl\nfrom llama_index.core import VectorStoreIndex, Settings\nfrom llama_index.core.embeddings import BaseEmbedding\nfrom llama_index.vector_stores.duckdb import DuckDBVectorStore\nfrom dotenv import load_dotenv\nload_dotenv()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue\n```\n\n\n:::\n\n```{.python .cell-code}\n# Ensure API key\nopenrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n\n# Custom OpenRouter Embedding class for LlamaIndex\nclass OpenRouterEmbedding(BaseEmbedding):\n    \"\"\"Custom embedding class for OpenRouter API compatible with LlamaIndex.\"\"\"\n\n    def __init__(\n        self,\n        api_key: str,\n        model: str = \"qwen/qwen3-embedding-8b\",\n        **kwargs: Any\n    ):\n        super().__init__(**kwargs)\n        self._client = OpenAI(\n            base_url=\"https://openrouter.ai/api/v1\",\n            api_key=api_key,\n        )\n        self._model = model\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get embedding for a query string.\"\"\"\n        response = self._client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self._model,\n            input=query,\n            encoding_format=\"float\"\n        )\n        return response.data[0].embedding\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get embedding for a text string.\"\"\"\n        return self._get_query_embedding(text)\n\n    async def _aget_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Async version of get_query_embedding.\"\"\"\n        return self._get_query_embedding(query)\n\n    async def _aget_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Async version of get_text_embedding.\"\"\"\n        return self._get_text_embedding(text)\n\n# 1. Configure Embedding Model using custom OpenRouter class\n# (Note: must use the same embedding model that was used to create the database)\nembed_model = OpenRouterEmbedding(\n    api_key=openrouter_api_key,\n    model=\"qwen/qwen3-embedding-8b\"\n)\nSettings.embed_model = embed_model\n\n# 2. Load Vector Store\nvector_store = DuckDBVectorStore(database_name=\"openrouter.duckdb\",persist_dir=\"./persist/\")\nindex = VectorStoreIndex.from_vector_store(vector_store)\n\n# 3. Define Retrieval Tool\ndef retrieve_trusted_content(query: str, top_k: int = 8):\n    \"\"\"\n    Retrieve relevant content from the knowledge store.\n\n    Parameters\n    ----------\n    query\n        The query used to semantically search the knowledge store.\n    top_k\n        The number of results to retrieve from the knowledge store.\n    \"\"\"\n    #print(f\"Retrieving content for query: '{query}'\")\n    retriever = index.as_retriever(similarity_top_k=top_k)\n    nodes = retriever.retrieve(query)\n    return [f\"<excerpt>{x.text}</excerpt>\" for x in nodes]\n\n# 4. Initialize Chat with System Prompt\nchat = ctl.ChatOpenRouter(\n    model=\"openai/gpt-oss-120b\",\n    api_key=openrouter_api_key,\n    base_url=\"https://openrouter.ai/api/v1\",\n    system_prompt=(\n        \"You are an assistant for question-answering tasks. \"\n        \"Use the retrieve_trusted_content tool to find relevant information from the knowledge store. \"\n        \"Answer questions based on the retrieved content. \"\n        \"If you cannot find relevant information, say so clearly.\"\n    )\n)\n\nchat.register_tool(retrieve_trusted_content)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n#retrieve_trusted_content(\"What are model variants?\",top_k=3)\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nresponse=chat.chat(\"What are model variants?\", echo=\"none\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(response)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n````\n**Model variants** are special suffixes you can attach to a model’s slug to change how that model is used.  \nThey do not create a new model; instead they modify the request‑routing, pricing, context length, tool‑calling behavior, or other characteristics of the underlying model.\n\n---\n\n### Types of variants\n\n| Variant (suffix) | Kind | What it does |\n|------------------|------|--------------|\n| `:free` | **Static** | Forces the model to be served from a free‑tier endpoint (low rate limits, always free). |\n| `:extended` | **Static** | Uses a version of the model with a longer context window than the default. |\n| `:exacto` | **Static** | Routes only through OpenRouter‑curated, high‑quality providers that have superior tool‑calling accuracy. |\n| `:thinking` | **Static** | Enables built‑in reasoning mode for models that support it. |\n| `:online` | **Dynamic** | Adds a web‑search step that fetches results and injects them into the prompt. |\n| `:nitro` | **Dynamic** | Sorts providers by throughput (speed) instead of the default sort, giving faster responses. |\n| `:floor` | **Dynamic** | Sorts providers by price (cost) rather than the default sort, choosing the cheapest option. |\n\n*Static variants* are only available for certain models and are listed in the **Models API**.  \n*Dynamic variants* can be used with **any** model; they affect how the request is routed or processed rather than changing the model itself.\n\n---\n\n### How to use them\n\nJust append the desired suffix to the model identifier when you make an API call, e.g.:\n\n```json\n{\n  \"model\": \"openai/gpt-4o:free\",\n  \"messages\": [...]\n}\n```\n\nYou can combine multiple suffixes (where supported) to get both effects, such as `moonshotai/kimi-k2-0905:exacto` to use the Exacto curated providers for that model.\n\n---\n\n**Source**: OpenRouter FAQ – “What are model variants?” which lists the suffixes and describes static vs. dynamic variants. (see excerpt from the documentation).\n````\n\n\n:::\n:::\n\n\n\n\n## Python LangChain\n\nThe final piece is to connect this retrieval capability to a chat interface. We use `LangChain` to create a RAG chain. We create a `retriever` from our vector store and combine it with a `ChatOpenAI` model (using OpenRouter) and a prompt template. This gives the LLM the ability to query our vector store whenever it needs information to answer a user's question.\n\n\n### Question:What are model variants?\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\nfrom openai import OpenAI\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_chroma import Chroma\nfrom typing import List\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue\n```\n\n\n:::\n\n```{.python .cell-code}\n# Custom embeddings class for OpenRouter API\nclass OpenRouterEmbeddings(Embeddings):\n    \"\"\"Custom embeddings class for OpenRouter API.\"\"\"\n    \n    def __init__(self, api_key: str, model: str = \"qwen/qwen3-embedding-8b\"):\n        self.client = OpenAI(\n            base_url=\"https://openrouter.ai/api/v1\",\n            api_key=api_key,\n        )\n        self.model = model\n    \n    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"Embed a list of documents.\"\"\"\n        response = self.client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self.model,\n            input=texts,\n            encoding_format=\"float\"\n        )\n        return [item.embedding for item in response.data]\n    \n    def embed_query(self, text: str) -> List[float]:\n        \"\"\"Embed a single query.\"\"\"\n        response = self.client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self.model,\n            input=text,\n            encoding_format=\"float\"\n        )\n        return response.data[0].embedding\n\n# Get OpenRouter API key\nopenrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\nif not openrouter_api_key:\n    raise ValueError(\"OPENROUTER_API_KEY not found in environment variables\")\n\n# Create embeddings instance using OpenRouter\nembeddings = OpenRouterEmbeddings(\n    api_key=openrouter_api_key,\n    model=\"qwen/qwen3-embedding-8b\"\n)\n\n# Define vector store location\npersist_directory = \"chroma_db_data\"\n\n# Load existing vector store\nprint(f\"Loading existing vectorstore from {persist_directory}...\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLoading existing vectorstore from chroma_db_data...\n```\n\n\n:::\n\n```{.python .cell-code}\nvectorstore = Chroma(\n    persist_directory=persist_directory,\n    embedding_function=embeddings\n)\nprint(f\"✓ Loaded vectorstore\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n✓ Loaded vectorstore\n```\n\n\n:::\n\n```{.python .cell-code}\n# Initialize LLM using OpenRouter\nllm = ChatOpenAI(\n    model=\"openai/gpt-oss-120b\",\n    openai_api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n    openai_api_base=\"https://openrouter.ai/api/v1\"\n)\n\n# Create prompt template\nsystem_prompt = (\n    \"You are an assistant for question-answering tasks. \"\n    \"Use the following pieces of retrieved context to answer \"\n    \"the question. If you don't know the answer, say that you \"\n    \"don't know. Use three sentences maximum and keep the \"\n    \"answer concise.\"\n    \"\\n\\n\"\n    \"Context: {context}\"\n    \"\\n\\n\"\n    \"Question: {question}\"\n)\n\nprompt = ChatPromptTemplate.from_template(system_prompt)\n\n# Create retriever\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n\n# Helper function to format documents\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n# Create RAG chain using LCEL\nrag_chain = (\n    {\n        \"context\": retriever | format_docs,\n        \"question\": RunnablePassthrough()\n    }\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\nprint(\"✓ RAG chain created successfully!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n✓ RAG chain created successfully!\n```\n\n\n:::\n\n```{.python .cell-code}\n# Test the RAG chain\nquestion = \"What are model variants?\"\n\nprint(f\"\\nQuestion: {question}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nQuestion: What are model variants?\n```\n\n\n:::\n\n```{.python .cell-code}\n# Get context documents separately for display\ncontext_docs = retriever.invoke(question)\n\n# Invoke the RAG chain\nanswer = rag_chain.invoke(question)\nimport textwrap\n\nfor line in answer.split('\\n'):\n    print(textwrap.fill(line, width=80))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel variants are suffixes you append to a model’s slug to modify its\nbehavior. Static variants\n(e.g., `:free`, `:beta`, `:extended`, `:exacto`, `:thinking`) work only with\ncertain models, while dynamic variants (e.g., `:online`, `:nitro`, `:floor`) can\nbe used with any model to affect routing, web‑search, speed, or cost. They let\nyou tailor the model’s pricing, moderation, context length, reasoning support,\nor provider selection.\n```\n\n\n:::\n:::\n\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}