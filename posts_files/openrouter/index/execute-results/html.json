{
  "hash": "f5c49a02185ae7b5896873e38c54fb45",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"OpenRouter: A Unified API for Multiple AI Models\"\nauthor: \"Tony D\"\ndate: \"2025-11-01\"\ncategories: [AI, API, tutorial]\nimage: \"images.png\"\n\nformat:\n  html:\n    code-fold: false\n    code-tools: true\n\nexecute:\n  eval: false\n  warning: false\n---\n\n\n\n\n\n\n\n\n# Introduction to OpenRouter\n\nOpenRouter is a powerful platform that provides a unified API interface for accessing multiple AI models from various providers. Instead of integrating with each AI service separately, developers can use OpenRouter's single endpoint to access models from OpenAI, Anthropic, Google, and many others.\n\n## Getting Started with OpenRouter\n\nGetting started with OpenRouter is straightforward and can be accomplished in just a few steps. The platform is designed to minimize setup time while maintaining security best practices.\n\n## 1. Create Your OpenRouter Account\n\nFirst, visit [openrouter.ai](https://openrouter.ai) and create an account. The registration process is simple:\n\n1. **Sign Up**: Use your email or social login (Google, GitHub)\n2. **Verify Email**: Confirm your email address through the verification link\n3. **Access Dashboard**: Navigate to your dashboard where you'll find your API key\n\n**üí° Pro Tip**: Your dashboard provides valuable insights including:\n- Usage statistics and cost tracking\n- Model performance metrics\n- API key management\n- Billing information\n\n## 2. Secure API Key Management\n\nSecurity is paramount when working with AI APIs. Never hardcode your API keys directly in your code. Instead, use environment variables to keep your credentials safe:\n\n### Why Environment Variables Matter\n- **Security**: Prevents accidental exposure in version control\n- **Flexibility**: Allows different keys for development, staging, and production\n- **Collaboration**: Team members can use their own keys without sharing\n- **Deployment**: Easy management across different hosting environments\n\n### Setting Up Environment Variables\n\nCreate a `.env` file in your project root:\n\nOPENROUTER_API_KEY=your_actual_api_key_here\n\n\n\nInstall the `python-dotenv` library to load environment variables:\n\npip install python-dotenv\n\n\n## 3. Your First API Call\n\nNow that you have your API key set up, let's make your first API call. OpenRouter cleverly uses the same API format as OpenAI, which means you can use the familiar `openai` Python library - just with a different base URL.\n\n### Understanding the Architecture\n\n**üîß How It Works**: OpenRouter acts as a smart proxy that:\n1. Receives your standardized API requests\n2. Routes them to the appropriate AI model provider\n3. Handles provider-specific authentication and formatting\n4. Returns responses in a consistent format\n5. Tracks usage and costs across all models\n\n### Required Dependencies\n\nBefore you start, ensure you have the necessary Python packages:\n\n```bash\npip install openai python-dotenv\n```\n\n- **openai**: The official OpenAI Python client (compatible with OpenRouter)\n- **python-dotenv**: For loading environment variables from `.env` files\n\n### Basic Text Generation Example\n\n\n::: {.panel-tabset}\n\n## python with OpenAI package\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize the OpenAI client with OpenRouter\nclient = OpenAI(\n  base_url=\"https://openrouter.ai/api/v1\",  # OpenRouter's API endpoint\n  api_key=os.getenv(\"OPENROUTER_API_KEY\"),  # Your secure API key\n)\n\n# Create a chat completion with OpenRouter\ncompletion = client.chat.completions.create(\n  extra_headers={\n    \"HTTP-Referer\": \"https://your-site.com\",  # Optional: Helps OpenRouter improve their service\n    \"X-Title\": \"Your Site Name\",             # Optional: Your site name for OpenRouter rankings\n  },\n  model=\"openai/gpt-oss-20b:free\",          # Free model for testing/development\n  messages=[\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello! Can you explain what OpenRouter is in simple terms?\"\n    }\n  ],\n  temperature=0.7  # Controls creativity (0.0 = deterministic, 1.0 = very creative)\n)\n\n# Extract and print the response\nprint(completion.choices[0].message.content)\n```\n:::\n\n\n\n\n## python with chatlas package\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom chatlas import ChatOpenRouter\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize the OpenAI client with OpenRouter\nclient = ChatOpenRouter(api_key=os.getenv(\"OPENROUTER_API_KEY\")\n,base_url='https://openrouter.ai/api/v1'\n ,system_prompt=None\n ,model=\"openai/gpt-oss-20b:free\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nresponse=client.chat(\"What is the capital of France?\")\n#str(response)\n```\n:::\n\n\n\n## R with ellmer package\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ellmer)\nlibrary(dotenv)\nload_dot_env(file = \".env\")\n\nchat <- chat_openrouter(\n  system_prompt = NULL,\n  api_key = Sys.getenv(\"OPENROUTER_API_KEY\"),\n \n  model = \"openai/gpt-oss-20b:free\",\n  echo = \"none\"\n)\n```\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchat$chat(\"Tell me three jokes about statisticians\")\n```\n:::\n\n\n\n:::\n\n\n**Key Components Explained:**\n\n- **`base_url`**: Points to OpenRouter's API instead of OpenAI's\n- **`model`**: Uses OpenRouter's model naming format (`provider/model-name`)\n- **`extra_headers`**: Optional but recommended for OpenRouter's analytics\n- **`temperature`**: Controls response creativity (0.0-2.0 range)\n- **`messages`**: Standard chat format with role-based conversation structure\n\n**üí° Model Selection Tips:**\n- **Free Models**: Great for development (`*free` suffix)\n- **Budget Models**: Cost-effective for production (`*:budget` suffix)\n- **Premium Models**: Best performance (`*`, `*:pro`, `*:latest`)\n- **Specialized**: Task-optimized models (coding, math, creative writing)\n\n\n\n\n\n## 4. Mastering System Prompts\n\nSystem prompts are powerful tools that shape the AI's behavior, personality, and response style. They set the context and rules for the entire conversation, appearing before user messages in the conversation flow.\n\n### What Are System Prompts?\n\nSystem prompts act as **meta-instructions** that guide how the AI should respond throughout the conversation. They're processed first and influence all subsequent interactions.\n\n### Why System Prompts Matter\n\n- **Consistent Behavior**: Ensures AI maintains the desired personality throughout\n- **Output Format**: Dictates response structure (JSON, markdown, code blocks)\n- **Safety Constraints**: Sets boundaries and restrictions on responses\n- **Context Setting**: Provides background information for better responses\n- **Task Specialization**: Optimizes AI for specific use cases\n\n### Effective System Prompt Examples\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Example with system prompt\ncompletion = client.chat.completions.create(\n  model=\"openai/gpt-oss-20b:free\",\n  messages=[\n    {\n      \"role\": \"system\",\n      \"content\": \"You are a helpful AI assistant that explains technical concepts in simple terms. Always be friendly and use analogies when possible and be simple\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"How does temperature in LLM model work?\"\n    }\n  ],\n  temperature=0.7\n)\n\nprint(completion.choices[0].message.content)\n```\n:::\n\n\nCommon system prompt patterns:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Different system prompt examples\nsystem_prompts = {\n    \"coding_assistant\": \"You are an expert programmer. Provide clean, well-commented code solutions and explain your reasoning.\",\n    \"creative_writer\": \"You are a creative storyteller. Write engaging narratives with vivid descriptions and compelling characters.\",\n    \"data_analyst\": \"You are a data analyst. Provide insights based on data, suggest visualizations, and explain statistical concepts clearly.\",\n    \"tutor\": \"You are a patient tutor. Break down complex topics into simple steps and provide encouraging feedback.\"\n}\n\ndef chat_with_persona(persona, user_message):\n    completion = client.chat.completions.create(\n        model=\"openai/gpt-oss-20b:free\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompts[persona]},\n            {\"role\": \"user\", \"content\": user_message}\n        ],\n        temperature=0.7\n    )\n    return completion.choices[0].message.content\n\n# Example usage\n#response = chat_with_persona(\"coding_assistant\", \"How do I reverse a string in Python?\")\n#print(response)\n```\n:::\n\n\n\n## 5. Streaming Responses: Real-Time AI Interaction\n\nStreaming is a game-changer for user experience, especially in chat applications and interactive tools. Instead of waiting for complete responses, streaming delivers content as it's being generated, creating natural and engaging conversations.\n\n### Why Streaming Matters\n\n**üöÄ User Experience Benefits:**\n- **Immediate Feedback**: Users see responses starting immediately\n- **Reduced Perceived Latency**: Content appears as it's generated\n- **Natural Conversation Flow**: Mimics human speech patterns\n- **Progress Indication**: Users know the AI is working\n- **Early Termination**: Users can stop lengthy responses if needed\n\n**‚ö° Technical Advantages:**\n- **Lower Memory Usage**: No need to buffer complete responses\n- **Faster Time-to-First-Byte**: Content starts flowing immediately\n- **Better Error Handling**: Issues detected earlier in process\n- **Resource Efficiency**: Processes data incrementally\n\n### Implementing Streaming with OpenRouter\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Initialize the OpenAI client (if not already done)\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nclient = OpenAI(\n  base_url=\"https://openrouter.ai/api/v1\",\n  api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n)\n\ndef stream_response(model, message):\n    stream = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": message}],\n        stream=True\n    )\n\n    for chunk in stream:\n        if chunk.choices[0].delta.content is not None:\n            print(chunk.choices[0].delta.content, end='', flush=True)\n\n# Stream example\nprint(\"Streaming response:\")\nstream_response(\"openai/gpt-oss-20b:free\", \"Tell me a short story about AI\")\nprint()  # Add newline after streaming\n```\n:::\n\n\n\n\n\n### 6. text to image\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom openai import OpenAI\nimport base64\nimport datetime\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# | eval: false\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize the OpenAI client with OpenRouter\nclient = OpenAI(\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n)\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Image generation request\nresponse = client.chat.completions.create(\n    extra_headers={\n        \"HTTP-Referer\": \"www.tonydotdev.com\",\n        \"X-Title\": \"TT_AI_blog\",\n    },\n    model=\"google/gemini-2.5-flash-image\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Generate a serene and realistic snowy mountain landscape at sunrise.\",\n        }\n    ],\n    modalities=[\"image\", \"text\"],\n    #image_size=\"1024x1024\",\n)\n```\n:::\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport base64\nimport datetime\n# Extract message\nmessage = response.choices[0].message\n\n# Handle image output (base64 string inside \"data:image/png;base64,...\")\nif message.images:\n    data_url = message.images[0][\"image_url\"][\"url\"]\n\n    # Strip prefix if present\n    if data_url.startswith(\"data:image\"):\n        _, base64_data = data_url.split(\",\", 1)\n    else:\n        base64_data = data_url\n\n    # Decode and save as PNG\n    image_bytes = base64.b64decode(base64_data)\n    current_time = datetime.datetime.now().strftime(\"%H%M\")\n    output_file = f\"text_image_gemini_25_openrouter_{current_time}.png\"\n    with open(output_file, \"wb\") as f:\n        f.write(image_bytes)\n\n    print(f\"‚úÖ Image saved as {output_file}\")\nelse:\n    print(\"‚ùå No image returned in response\")\n```\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"text_image_gemini_25_openrouter_1352.png\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<IPython.core.display.Image object>\n```\n\n\n:::\n:::\n\n\n\n### 6 image to text\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport base64\n\n# Convert local image to data URL\nwith open(\"text_image_gemini_25_openrouter_1352.png\", \"rb\") as image_file:\n    base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n    data_url = f\"data:image/png;base64,{base64_image}\"\n\ncompletion = client.chat.completions.create(\n  extra_headers={\n    \"HTTP-Referer\": \"<YOUR_SITE_URL>\", # Optional. Site URL for rankings on openrouter.ai.\n    \"X-Title\": \"<YOUR_SITE_NAME>\", # Optional. Site title for rankings on openrouter.ai.\n  },\n  extra_body={},\n  model=\"nvidia/nemotron-nano-12b-v2-vl:free\",\n  messages=[\n              {\n                \"role\": \"user\",\n                \"content\": [\n                  {\n                    \"type\": \"text\",\n                    \"text\": \"What is in this image?\"\n                  },\n                  {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                      \"url\": data_url\n                    }\n                  }\n                ]\n              }\n            ]\n)\nprint(completion.choices[0].message.content)\n```\n:::\n\n\n\n\n### 7. text,image to image\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# | eval: false\nimport base64\n\n# Convert the previously generated image to base64\nwith open(\"text_image_gemini_25_openrouter_1352.png\", \"rb\") as image_file:\n    base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n\nresponse = client.chat.completions.create(\n    extra_headers={\n        \"HTTP-Referer\": \"www.tonydotdev.com\",\n        \"X-Title\": \"TT_AI_blog\",\n    },\n    model=\"google/gemini-2.5-flash-image\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Transform this image into a sunset version with warmer colors and golden light.add one person skiing in the foreground.\",\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"},\n                },\n            ],\n        }\n    ],\n    modalities=[\"image\", \"text\"],\n    # image_size=\"1024x1024\",\n)\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport base64\nimport datetime\n# Extract message\nmessage = response.choices[0].message\n\n# Handle image output (base64 string inside \"data:image/png;base64,...\")\nif message.images:\n    data_url = message.images[0][\"image_url\"][\"url\"]\n\n    # Strip prefix if present\n    if data_url.startswith(\"data:image\"):\n        _, base64_data = data_url.split(\",\", 1)\n    else:\n        base64_data = data_url\n\n    # Decode and save as PNG\n    image_bytes = base64.b64decode(base64_data)\n    current_time = datetime.datetime.now().strftime(\"%H%M\")\n    output_file = f\"text_image_gemini_25_openrouter_{current_time}.png\"\n    with open(output_file, \"wb\") as f:\n        f.write(image_bytes)\n\n    print(f\"‚úÖ Image saved as {output_file}\")\nelse:\n    print(\"‚ùå No image returned in response\")\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"text_image_gemini_25_openrouter_1405.png\"))\n```\n:::\n\n\n### 8 Embedding\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize the OpenAI client with OpenRouter\nclient = OpenAI(\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n)\n\n\nembedding = client.embeddings.create(\n  extra_headers={\n    \"HTTP-Referer\": \"<YOUR_SITE_URL>\", # Optional. Site URL for rankings on openrouter.ai.\n    \"X-Title\": \"<YOUR_SITE_NAME>\", # Optional. Site title for rankings on openrouter.ai.\n  },\n  model=\"thenlper/gte-base\",\n  input=\"I can\",\n  encoding_format=\"float\"\n)\n\n#print(embedding.data[0].embedding) \n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlen(embedding.data[0].embedding)\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(embedding.data[0].embedding[:5])  # Print first 5 dimensions\n```\n:::\n\n\n\n\n## Cost Management: Optimizing Your AI Spending\n\nOne of OpenRouter's most powerful features is its transparent pricing model and cost management capabilities. Understanding and managing costs is crucial for production AI applications.\n\n## Why Cost Management Matters\n\n**üí∞ Financial Planning:**\n- Predictable monthly expenses\n- Budget allocation across different use cases\n- ROI analysis for AI features\n- Cost per user tracking\n\n**üîç Technical Optimization:**\n- Model selection based on cost/performance ratio\n- Prompt engineering to reduce token usage\n- Caching strategies for repeated requests\n- Batch processing for efficiency\n\n## Real-Time Cost Tracking\n\nOpenRouter provides programmatic access to current pricing for all models:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Initialize the OpenAI client (if not already done)\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\nimport pandas as pd\n\nload_dotenv()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue\n```\n\n\n:::\n\n```{.python .cell-code}\nclient = OpenAI(\n  base_url=\"https://openrouter.ai/api/v1\",\n  api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n)\n\ndef get_model_pricing():\n    models_list = client.models.list()\n    pricing_data = []\n\n    for model in models_list.data:\n        name = model.id\n        pricing = model.pricing\n\n        # Get model metadata\n        context_length = getattr(model, 'context_length', None)\n        description = getattr(model, 'description', '')\n\n        # Get creation date and convert to readable format\n        created_timestamp = getattr(model, 'created', None)\n        if created_timestamp:\n            import datetime\n            created_date = datetime.datetime.fromtimestamp(created_timestamp).strftime('%Y-%m-%d')\n        else:\n            created_date = None\n\n        # Extract company from model name (first part before '/')\n        company = name.split('/')[0] if '/' in name else 'Unknown'\n\n        # Convert per-token prices to cost per 1M tokens\n        prompt_cost = float(pricing.get('prompt', 0)) * 1000000 if pricing and pricing.get('prompt') else 0\n        completion_cost = float(pricing.get('completion', 0)) * 1000000 if pricing and pricing.get('completion') else 0\n        request_cost = float(pricing.get('request', 0)) * 1000000 if pricing and pricing.get('request') else 0\n        image_cost = float(pricing.get('image', 0)) * 1000000 if pricing and pricing.get('image') else 0\n\n        pricing_data.append({\n            'Model': name,\n            'Company': company,\n            'Description': description,\n            'Context_Length': context_length,\n            'Created_Date': created_date,\n            'Prompt_Cost_per_1M': prompt_cost,\n            'Completion_Cost_per_1M': completion_cost,\n            'Request_Cost_per_1M': request_cost,\n            'Image_Cost_per_1M': image_cost\n        })\n\n    # Create pandas DataFrame\n    df = pd.DataFrame(pricing_data)\n\n    # Sort by company then by model name for better organization\n    df = df.sort_values(['Company', 'Model']).reset_index(drop=True)\n\n    return df\n\n# Get pricing DataFrames (all models and paid models only)\nall_models_df = get_model_pricing()\n```\n:::\n\n\n\n\n\n#### Most Expensive Models\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport panel as pn\n\n\ndf = all_models_df[\n    [\n        \"Model\",\n        \"Context_Length\",\n        \"Created_Date\",\n        \"Prompt_Cost_per_1M\",\n        \"Completion_Cost_per_1M\",\n    ]\n].sort_values(\"Prompt_Cost_per_1M\", ascending=False)\n\n# Create a paginated table\npn.extension(\"tabulator\")\ntable = pn.widgets.Tabulator(df, pagination=\"local\", page_size=10, show_index=False)\ntable\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTabulator(page_size=10, pagination='local', show_index=False, value=              ...)\n```\n\n\n:::\n:::\n\n\n\n## Best Practices: Production-Ready AI Development\n\nFollowing these best practices will help you build robust, secure, and efficient AI applications with OpenRouter.\n\n## üîí Security Best Practices\n\n### 1. API Key Management\n- **Never hardcode API keys** in source code or configuration files\n- **Use environment variables** or secret management systems (AWS Secrets Manager, Azure Key Vault)\n- **Rotate API keys regularly** and implement key rotation policies\n- **Use different keys** for development, staging, and production environments\n- **Add .env to .gitignore** - Never commit credentials to version control\n\n### 2. Input Validation and Sanitization\n- **Validate user inputs** before sending to AI models\n- **Sanitize prompts** to prevent prompt injection attacks\n- **Implement rate limiting** to prevent abuse\n- **Log and monitor** for suspicious activity patterns\n\n## ‚ö° Performance Best Practices\n\n### 3. Model Selection Strategy\n- **Choose the right model for your use case** - Not all tasks need the most expensive model\n- **Benchmark models** for your specific use cases\n- **Use free models** for development and testing\n- **Consider specialized models** for specific tasks (coding, math, creative writing)\n\n### 4. Optimization Techniques\n- **Implement caching** for repeated requests to reduce costs\n- **Use streaming** for better user experience\n- **Batch requests** when appropriate for efficiency\n- **Optimize prompts** - Well-crafted prompts reduce token usage and improve results\n\n## üõ°Ô∏è Reliability Best Practices\n\n### 5. Error Handling and Resilience\n- **Implement comprehensive error handling** - Models can be unavailable or rate-limited\n- **Use fallback models** - Ensure your application remains functional even if one model is down\n- **Implement retry logic** with exponential backoff\n- **Monitor response times** and set appropriate timeouts\n\n### 6. Monitoring and Analytics\n- **Monitor usage** - Keep track of costs and set limits\n- **Track performance metrics** (latency, success rates, error rates)\n- **Set up alerts** for unusual activity patterns\n- **Create dashboards** for real-time monitoring\n\n## üìä Cost Management Best Practices\n\n### 7. Budget Control\n- **Set spending limits** and alerts in your OpenRouter dashboard\n- **Use cost-effective models** for non-critical tasks\n- **Implement token counting** to estimate costs before requests\n- **Review usage reports** regularly to identify optimization opportunities\n\n## Conclusion: Building the Future of AI Applications\n\nOpenRouter represents a paradigm shift in how developers interact with AI models. By providing a unified, reliable, and cost-effective gateway to the world's most advanced AI models, OpenRouter enables developers to focus on creating value rather than managing infrastructure complexities.\n\n\n\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}